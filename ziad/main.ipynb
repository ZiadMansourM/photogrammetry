{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import uuid\n",
    "from typing import Final, List, Tuple\n",
    "\n",
    "import cv2 as OpenCV\n",
    "# import joblib\n",
    "import numpy as np\n",
    "# import trimesh\n",
    "from numpy.linalg import norm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial import Delaunay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalibrationError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "class IntrinsicParametersNotFoundError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, keypoints, descriptors, path):\n",
    "        self.img_id: int = img_id\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "        self.similar_images: list[tuple[Image, float]] = []\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        # Replace cv2.KeyPoint objects with tuples\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        # Convert tuples back to cv2.KeyPoint objects\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        # Replace OpenCV.DMatch objects with tuples\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        # Convert tuples back to OpenCV.DMatch objects\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step One: Read and Load Images\n",
    "Inputs: \n",
    "- images_set_name: str\n",
    "\n",
    "Outputs:\n",
    "- gray_images: list[np.ndarray]\n",
    "\n",
    "Main Functions:\n",
    "1. read_images: read images from a folder\n",
    "2. rgb_to_gray: convert a list of RGB images to grayscale\n",
    "\n",
    "Utils Functions:\n",
    "1. read_images_rbg: read an image from a path and convert it to RGB format\n",
    "\"\"\"\n",
    "\n",
    "def prepare_images(folder_path: str) -> Images:\n",
    "    \"\"\" Read and load images \"\"\"\n",
    "    images: Images = Images([], folder_path.split(\"/\")[-1])\n",
    "    files: list[str] = filter(lambda file: \".jpg\" in file, os.listdir(folder_path))\n",
    "    for i, file in enumerate(files):\n",
    "        image_path = f\"{folder_path}/{file}\"\n",
    "        rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "        gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "        images.images.append(Image(i, rgb_image, gray_image, [], [], image_path))\n",
    "    return images\n",
    "\n",
    "def dump_images_bak(images_file_path: str, images: Images) -> None:\n",
    "    \"\"\" Dump images to a file \"\"\"\n",
    "    with open(images_file_path, \"wb\") as file:\n",
    "        pickle.dump(images, file)\n",
    "\n",
    "def load_images_bak(images_file_path: str) -> Images:\n",
    "    \"\"\" Load images from a file \"\"\"\n",
    "    with open(images_file_path, \"rb\") as file:\n",
    "        images = pickle.load(file)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Two: Feature Extraction\n",
    "Inputs:\n",
    "- gray_images: list[Image]\n",
    "- SIFT: cv2.SIFT\n",
    "\n",
    "Outputs:\n",
    "- keypoints: list[np.ndarray]\n",
    "- descriptors: list[np.ndarray]\n",
    "\n",
    "Main Functions:\n",
    "1. get_images_keypoints\n",
    "\n",
    "Utils functions:\n",
    "1. load_sift_features\n",
    "2. dump_sift_features\n",
    "3. convert_keypoints_to_tuples\n",
    "4. convert_tuples_to_keypoints\n",
    "\"\"\"\n",
    "\n",
    "def compute_keypoints_descriptors(images: Image, SIFT: OpenCV.SIFT) -> None:\n",
    "    \"\"\" Get keypoints and descriptors for each image in the list of images\"\"\"\n",
    "    for img in images.images:\n",
    "        keypoints: list[OpenCV.KeyPoint]\n",
    "        descriptors: np.ndarray\n",
    "        keypoints, descriptors = SIFT.detectAndCompute(img.gray_image, None)\n",
    "        img.keypoints = keypoints\n",
    "        img.descriptors = descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step Three: Image Matching\n",
    "Inputs:\n",
    "- descriptors: list[np.ndarray]\n",
    "\n",
    "Outputs:\n",
    "- centroids: np.ndarray\n",
    "- variance: np.ndarray\n",
    "- CLUSTER_COUNT: int\n",
    "- matches_ids: list[list[tuple[int, float]]]\n",
    "\n",
    "Main Functions:\n",
    "1. match_images\n",
    "2. get_matches_ids\n",
    "\n",
    "Utils Functions:\n",
    "1. load_image_matching\n",
    "2. dump_image_matching\n",
    "3. get_visual_words\n",
    "4. get_frequency_vectors\n",
    "5. get_tf_idf\n",
    "6. search_matches\n",
    "\"\"\"\n",
    "\n",
    "def get_matches(images: Images) -> None:\n",
    "    \"\"\" Match images using k-means clustering \"\"\"\n",
    "    all_descriptors = np.concatenate([image.descriptors for image in images.images])\n",
    "    CLUSTER_COUNT: Final = 400\n",
    "    ITER: Final = 2\n",
    "    centroids, _ = kmeans(all_descriptors, CLUSTER_COUNT, ITER)\n",
    "    matches_ids: list[list[tuple[int, float]]] =  get_matches_ids([image.descriptors for image in images.images], centroids, images.images)\n",
    "    for i, image in enumerate(images.images):\n",
    "        inner_list: list[Image, float] = [\n",
    "            (images.images[match[0]], match[1]) for match in matches_ids[i]\n",
    "        ]\n",
    "        image.similar_images = inner_list\n",
    "\n",
    "\n",
    "def get_visual_words(descriptors: list[np.ndarray], centroids: np.ndarray) -> list[np.ndarray]:\n",
    "    visual_words = []\n",
    "    for descriptor in descriptors:\n",
    "        words, _ = vq(descriptor, centroids)\n",
    "        visual_words.append(words)\n",
    "    return visual_words\n",
    "\n",
    "\n",
    "def get_frequency_vectors(visual_words: list[np.ndarray], CLUSTER_COUNT: int) -> np.ndarray:\n",
    "    frequency_vectors = []\n",
    "    for img_words in visual_words:\n",
    "        histogram = np.zeros(CLUSTER_COUNT)\n",
    "        for word in img_words:\n",
    "            histogram[word] += 1\n",
    "        frequency_vectors.append(histogram)\n",
    "    return np.stack(frequency_vectors)\n",
    "\n",
    "\n",
    "def get_tf_idf(frequency_vectors, IMAGES_COUNT) -> np.ndarray:\n",
    "    df = np.sum(frequency_vectors > 0, axis = 0)\n",
    "    idf = np.log(IMAGES_COUNT/df)\n",
    "    return frequency_vectors * idf\n",
    "\n",
    "\n",
    "def search_matches(i, top_clusters, tf_idf) -> list[tuple[int, float]]:\n",
    "    \"\"\" Search for the top_clusters most similar images to the i-th image\n",
    "    Args:\n",
    "        i: the index of the image to search for similar images\n",
    "        top_clusters: the number of similar images to return\n",
    "        tf_idf: Term Frequency-Inverse Document Frequency\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains the index of a similar image and the cosine similarity \n",
    "        between the i-th image and the similar image. The list is sorted by the cosine similarity in \n",
    "        descending order.\n",
    "    \"\"\"\n",
    "    b = tf_idf\n",
    "    a = tf_idf[i]\n",
    "    b_subset = b[:tf_idf.shape[0]]\n",
    "    cosine_similarity = np.dot(a, b_subset.T)/(norm(a) * norm(b_subset, axis=1))\n",
    "    idx = np.argsort(-cosine_similarity)[:top_clusters]\n",
    "    return list(zip(idx, cosine_similarity[idx]))\n",
    "\n",
    "\n",
    "def get_matches_ids(descriptors, centroids, images_list) -> list[list[tuple[int, float]]]:\n",
    "    \"\"\"Returns: a list of lists, where each list contains the top 10 most similar images to the i-th image.\"\"\"\n",
    "    visual_words = get_visual_words(descriptors, centroids)\n",
    "    frequency_vectors = get_frequency_vectors(visual_words, centroids.shape[0])\n",
    "    \"\"\" tf_idf: Term Frequency-Inverse Document Frequency \"\"\"\n",
    "    tf_idf = get_tf_idf(frequency_vectors, len(images_list))\n",
    "    return [\n",
    "        search_matches(i, 10, tf_idf)\n",
    "        for i in range(len(images_list))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Four: Feature Matching\n",
    "Inputs:\n",
    "- matches_ids: list[list[tuple[int, float]]]\n",
    "- descriptors: list[np.ndarray]\n",
    "\n",
    "Outputs:\n",
    "- feature_matches: list[list[int, int, list[OpenCV.DMatch]]]\n",
    "\n",
    "Main Functions:\n",
    "1. data_feature_matching\n",
    "\n",
    "Utils Functions:\n",
    "1. feature_matching\n",
    "2. dump_feature_matching\n",
    "3. load_feature_matching\n",
    "\"\"\"\n",
    "\n",
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    \"\"\" Match features between two images using Brute Force Matcher\n",
    "    Args:\n",
    "        img_id_one: the index of the first image\n",
    "        img_id_two: the index of the second image\n",
    "        descriptors: a list of descriptors of the images\n",
    "    Returns:\n",
    "        A list of OpenCV.DMatch objects.\n",
    "    \"\"\"\n",
    "    matcher = OpenCV.BFMatcher()\n",
    "    return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "def data_feature_matching(images: Images) -> None:\n",
    "    \"\"\" Match features between images using Brute Force Matcher\n",
    "    Args:\n",
    "        matchesIDs: a list of lists of tuples, where each tuple contains the index of a similar image and the cosine similarity \n",
    "            between the i-th image and the similar image. The list is sorted by the cosine similarity in \n",
    "            descending order.\n",
    "        descriptors: a list of descriptors of the images\n",
    "    Returns:\n",
    "        A list of lists, where each list contains \n",
    "        the index of the first image, the index of the second image, \n",
    "        and a list of OpenCV.DMatch objects.\n",
    "    \"\"\"\n",
    "    num_images: int = len(images.images)\n",
    "    checked = np.zeros((num_images, num_images), dtype=int)\n",
    "    # feature_matches_list: list[list[int, int, list[OpenCV.DMatch]]] = []\n",
    "    for image in images.images:\n",
    "        # logging.info(f\"---------- START Matches for: {str(imageID)}\")\n",
    "        for matched_image, probability in image.similar_images:\n",
    "            if ((checked[image.img_id][matched_image.img_id] == 0 or checked[matched_image.img_id][image.img_id] == 0) and image.img_id != matched_image.img_id and probability > 0.93):\n",
    "                # start_time = time.time()\n",
    "                images.feature_matches.append(FeatureMatches(image, matched_image, feature_matching(image.descriptors, matched_image.descriptors)))\n",
    "                checked[image.img_id][matched_image.img_id], checked[matched_image.img_id][image.img_id] = 1, 1\n",
    "                # logging.info(f\"done [{i}/{len(matchesIDs[imageID])}] in {(time.time() - start_time):.4f}: {str(imageID)} - {str(matchedID)}\")\n",
    "        # Flush the log file force write to disk\n",
    "        # logging.shutdown()\n",
    "\n",
    "\n",
    "# def convert_matches_to_dicts(matches: list[OpenCV.DMatch]) -> list[dict[str, int, int, float]]:\n",
    "#     \"\"\"Convert a list of OpenCV.DMatch objects to a list of dictionaries to serialize.\"\"\"\n",
    "#     match_dicts = []\n",
    "#     for match in matches:\n",
    "#         match_dict = {'queryIdx': match.queryIdx, 'trainIdx': match.trainIdx, 'distance': match.distance}\n",
    "#         match_dicts.append(match_dict)\n",
    "#     return match_dicts\n",
    "\n",
    "\n",
    "# def load_feature_matching(path: str) -> list[list[int, int, list[OpenCV.DMatch]]]:\n",
    "#     \"\"\"Load a list of lists of OpenCV.DMatch objects from a pickle file.\"\"\"\n",
    "#     with open(path, 'rb') as f:\n",
    "#         feature_matches_dicts = pickle.load(f)\n",
    "#     feature_matches = []\n",
    "#     for match_dict in feature_matches_dicts:\n",
    "#         matches = [\n",
    "#             OpenCV.DMatch(\n",
    "#                 match['queryIdx'], \n",
    "#                 match['trainIdx'], \n",
    "#                 match['distance']\n",
    "#             ) for match in match_dict[2]\n",
    "#         ]\n",
    "#         feature_matches.append([match_dict[0], match_dict[1], matches])\n",
    "#     return feature_matches\n",
    "\n",
    "\n",
    "# def dump_feature_matching(path: str, feature_matches: list[list[int, int, list[OpenCV.DMatch]]]) -> None:\n",
    "#     \"\"\"Dump a list of lists of OpenCV.DMatch objects to a pickle file.\"\"\"\n",
    "#     matches_dicts = [\n",
    "#         [\n",
    "#             match[0],\n",
    "#             match[1], \n",
    "#             convert_matches_to_dicts(match[2])\n",
    "#         ] for match in feature_matches\n",
    "#     ]\n",
    "#     with open(path, 'wb') as f:\n",
    "#         pickle.dump(matches_dicts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Five: Camera Calibration\n",
    "Inputs:\n",
    "- None.\n",
    "\n",
    "Outputs:\n",
    "- k_matrix: np.ndarray\n",
    "\n",
    "ToDo:\n",
    "1- generate K_matrix.pickle for each camera using Chess board pattern.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Six: Triangulation (3D Reconstruction)\n",
    "Inputs:\n",
    "- feature_matches_list: list[list[int, int, list[OpenCV.DMatch]]]\n",
    "    -> A list of lists, where each list contains \n",
    "        the index of the first image, the index of the second image, \n",
    "        and a list of OpenCV.DMatch objects.\n",
    "- K_matrix: np.ndarray\n",
    "    -> The camera matrix of the camera used to take the images.\n",
    "\n",
    "Outputs:\n",
    "- point_cloud: list[np.ndarray]; each element is a 3D point.\n",
    "\n",
    "Main Functions:\n",
    "1. generate_point_cloud\n",
    "\n",
    "Utils Functions:\n",
    "1. triangulatePoints\n",
    "\"\"\"\n",
    "\n",
    "def triangulatePoints(P1, P2, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Triangulates the given matching points from two images using the given camera matrices.\n",
    "\n",
    "    Parameters:\n",
    "    P1 (numpy.ndarray): 3x4 camera matrix of the first image.\n",
    "    P2 (numpy.ndarray): 3x4 camera matrix of the second image.\n",
    "    pts1 (numpy.ndarray): Nx2 matrix containing the coordinates of matching points in the first image.\n",
    "    pts2 (numpy.ndarray): Nx2 matrix containing the coordinates of matching points in the second image.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Nx3 matrix containing the triangulated 3D points.\n",
    "    \"\"\"\n",
    "    pts4D = OpenCV.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    pts4D /= pts4D[3]\n",
    "    return pts4D[:3].T\n",
    "\n",
    "\n",
    "def generate_point_cloud(images: Images, K_matrix):\n",
    "    \"\"\"\n",
    "    Generates a cloud of 3D points using triangulation from feature matches and camera calibration matrix.\n",
    "\n",
    "    Parameters:\n",
    "    feature_matches_list (list): List of feature matches between images.\n",
    "    K_matrix (numpy.ndarray): 3x3 camera calibration matrix.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Nx3 matrix containing the cloud of 3D points.\n",
    "    \"\"\"\n",
    "    point_cloud = []\n",
    "    feature_matches_list = images.feature_matches\n",
    "    for match in feature_matches_list:\n",
    "        img_one = match.image_one\n",
    "        img_two = match.image_two\n",
    "        matches = match.matches\n",
    "        pts1 = np.float32([img_one.keypoints[m.queryIdx].pt for m in matches])\n",
    "        pts2 = np.float32([img_two.keypoints[m.trainIdx].pt for m in matches])\n",
    "        E, _ = OpenCV.findEssentialMat(pts1, pts2, K_matrix)\n",
    "        R1, R2, t = OpenCV.decomposeEssentialMat(E)\n",
    "\n",
    "        for i in range(len(R1)):\n",
    "            P1 = np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "            P2 = np.hstack((R1[i], t.reshape(3, 1)))\n",
    "            # P2 = np.hstack((R1[i], t))\n",
    "            pts_3d = triangulatePoints(K_matrix.dot(P1), K_matrix.dot(P2), pts1, pts2)\n",
    "            point_cloud.append(pts_3d)\n",
    "    return np.concatenate(point_cloud, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Welcome ScanMate...\")\n",
    "image_set_name = \"snow-man\"\n",
    "# 1. Load and prepare Images\n",
    "if os.path.isfile(f\"bak/{image_set_name}/images.pkl\"):\n",
    "    print(f\"File [bak/{image_set_name}/sift-images.pkl] exists\")\n",
    "    print(\"Loading images from pickle file...\")\n",
    "    images: Images = load_images_bak(f\"bak/{image_set_name}/images.pkl\")\n",
    "else:\n",
    "    print(f\"File [bak/{image_set_name}/images.pkl] does not exist\")\n",
    "    print(\"Loading images from images directory...\")\n",
    "    images: Images = prepare_images(f\"images/{image_set_name}\")\n",
    "    print(\"Saving images to pickle file...\")\n",
    "    dump_images_bak(f\"bak/{image_set_name}/images.pkl\", images)\n",
    "print(\"Images loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Extraction: SIFT\n",
    "if os.path.isfile(f\"bak/{image_set_name}/sift-features.pkl\"):\n",
    "    print(f\"File [bak/{image_set_name}/sift-features.pkl] exists\")\n",
    "    images: Images = load_images_bak(f\"bak/{image_set_name}/sift-features.pkl\")\n",
    "else:\n",
    "    print(\"File [bak/{image_set_name}/sift-features.pkl] DO NOT exists\")\n",
    "    print(\"Extracting SIFT features...\")\n",
    "    sift = OpenCV.SIFT_create()\n",
    "    compute_keypoints_descriptors(images, sift)\n",
    "    dump_images_bak(f\"bak/{image_set_name}/sift-features.pkl\", images)\n",
    "print(\"Feature Extraction: SIFT DONE...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(images.images))\n",
    "for i, image in enumerate(images.images):\n",
    "    print(f\"Image {i} has {len(image.keypoints)} keypoints and {len(image.descriptors)} descriptors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Image Matching\n",
    "if os.path.isfile(f\"bak/{image_set_name}/images-matched.pkl\"):\n",
    "    print(f\"File [bak/{image_set_name}/images-matched.pkl] exists\")\n",
    "    images: Images = load_images_bak(f\"bak/{image_set_name}/images-matched.pkl\")\n",
    "else:\n",
    "    print(f\"File [bak/{image_set_name}/images-matched.pkl] DO NOT exists\")\n",
    "    get_matches(images)\n",
    "    dump_images_bak(f\"bak/{image_set_name}/images-matched.pkl\", images)\n",
    "print(\"Done Image Matching Step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Matching\n",
    "if os.path.isfile(f\"bak/{image_set_name}/feature-matching-output.pkl\"):\n",
    "    print(f\"File [bak/{image_set_name}/feature-matching-output.pkl] exists\")\n",
    "    images: Images = load_images_bak(f\"bak/{image_set_name}/feature-matching-output.pkl\")\n",
    "else:\n",
    "    print(\"File [bak/{image_set_name}/feature-matching-output.pkl] Do NOT exists\")\n",
    "    # logging.info('----> Processing {image_set_name}...')\n",
    "    data_feature_matching(images)\n",
    "    dump_images_bak(f\"bak/{image_set_name}/feature-matching-output.pkl\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Camera Calibration\n",
    "print(\"Camera Calibration starts ....\")\n",
    "if not os.path.isfile(\"bak/snow-man/checker/K_matrix.pickle\"):\n",
    "    raise IntrinsicParametersNotFoundError(\"Intrinsic parameters not found\")\n",
    "print(\"File bak/snow-man/checker/K_matrix.pickle exists\")\n",
    "with open('bak/snow-man/checker/K_matrix.pickle', 'rb') as f:\n",
    "    K_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Triangulation (3D reconstruction)\n",
    "print(\"Triangulation starts ....\")\n",
    "points_cloud = generate_point_cloud(images, K_matrix)\n",
    "np.savetxt(\"points_cloud.txt\", points_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. generate mesh\n",
    "print(\"Generate mesh ....\")\n",
    "tri = Delaunay(points_cloud)\n",
    "mesh = trimesh.Trimesh(points_cloud, tri.simplices)\n",
    "mesh = mesh.simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. output .obj, .stl and .ply files\n",
    "print(\"Generate mesh ....\")\n",
    "mesh.export(f\"output/{image_set_name}/snow_man.obj\")\n",
    "mesh.export(f\"output/{image_set_name}/snow_man.stl\")\n",
    "mesh.export(f\"output/{image_set_name}/snow_man.ply\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
