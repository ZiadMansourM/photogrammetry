{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import uuid\n",
    "from typing import Final, List, Tuple\n",
    "\n",
    "import cv2 as OpenCV\n",
    "# import joblib\n",
    "import numpy as np\n",
    "# import trimesh\n",
    "from numpy.linalg import norm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial import Delaunay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalibrationError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "class IntrinsicParametersNotFoundError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, keypoints, descriptors, path):\n",
    "        self.img_id: int = img_id\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "        self.similar_images: list[tuple[Image, float]] = []\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        # Replace cv2.KeyPoint objects with tuples\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        # Convert tuples back to cv2.KeyPoint objects\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        # Replace OpenCV.DMatch objects with tuples\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        # Convert tuples back to OpenCV.DMatch objects\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step One: Read and Load Images\n",
    "Inputs: \n",
    "- images_set_name: str\n",
    "\n",
    "Outputs:\n",
    "- gray_images: list[np.ndarray]\n",
    "\n",
    "Main Functions:\n",
    "1. read_images: read images from a folder\n",
    "2. rgb_to_gray: convert a list of RGB images to grayscale\n",
    "\n",
    "Utils Functions:\n",
    "1. read_images_rbg: read an image from a path and convert it to RGB format\n",
    "\"\"\"\n",
    "\n",
    "def prepare_images(folder_path: str) -> Images:\n",
    "    \"\"\" Read and load images \"\"\"\n",
    "    images: Images = Images([], folder_path.split(\"/\")[-1])\n",
    "    files: list[str] = filter(lambda file: \".jpg\" in file, os.listdir(folder_path))\n",
    "    for i, file in enumerate(files):\n",
    "        image_path = f\"{folder_path}/{file}\"\n",
    "        rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "        gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "        images.images.append(Image(i, rgb_image, gray_image, [], [], image_path))\n",
    "    return images\n",
    "\n",
    "def dump_images_bak(images_file_path: str, images: Images) -> None:\n",
    "    \"\"\" Dump images to a file \"\"\"\n",
    "    with open(images_file_path, \"wb\") as file:\n",
    "        pickle.dump(images, file)\n",
    "\n",
    "def load_images_bak(images_file_path: str) -> Images:\n",
    "    \"\"\" Load images from a file \"\"\"\n",
    "    with open(images_file_path, \"rb\") as file:\n",
    "        images = pickle.load(file)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Two: Feature Extraction\n",
    "Inputs:\n",
    "- gray_images: list[Image]\n",
    "- SIFT: cv2.SIFT\n",
    "\n",
    "Outputs:\n",
    "- keypoints: list[np.ndarray]\n",
    "- descriptors: list[np.ndarray]\n",
    "\n",
    "Main Functions:\n",
    "1. get_images_keypoints\n",
    "\n",
    "Utils functions:\n",
    "1. load_sift_features\n",
    "2. dump_sift_features\n",
    "3. convert_keypoints_to_tuples\n",
    "4. convert_tuples_to_keypoints\n",
    "\"\"\"\n",
    "\n",
    "def compute_keypoints_descriptors(images: Image, SIFT: OpenCV.SIFT) -> None:\n",
    "    \"\"\" Get keypoints and descriptors for each image in the list of images\"\"\"\n",
    "    for img in images.images:\n",
    "        keypoints: list[OpenCV.KeyPoint]\n",
    "        descriptors: np.ndarray\n",
    "        keypoints, descriptors = SIFT.detectAndCompute(img.gray_image, None)\n",
    "        img.keypoints = keypoints\n",
    "        img.descriptors = descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step Three: Image Matching\n",
    "Inputs:\n",
    "- descriptors: list[np.ndarray]\n",
    "\n",
    "Outputs:\n",
    "- centroids: np.ndarray\n",
    "- variance: np.ndarray\n",
    "- CLUSTER_COUNT: int\n",
    "- matches_ids: list[list[tuple[int, float]]]\n",
    "\n",
    "Main Functions:\n",
    "1. match_images\n",
    "2. get_matches_ids\n",
    "\n",
    "Utils Functions:\n",
    "1. load_image_matching\n",
    "2. dump_image_matching\n",
    "3. get_visual_words\n",
    "4. get_frequency_vectors\n",
    "5. get_tf_idf\n",
    "6. search_matches\n",
    "\"\"\"\n",
    "\n",
    "def get_matches(images: Images) -> None:\n",
    "    \"\"\" Match images using k-means clustering \"\"\"\n",
    "    all_descriptors = np.concatenate([image.descriptors for image in images.images])\n",
    "    CLUSTER_COUNT: Final = 400\n",
    "    ITER: Final = 2\n",
    "    centroids, _ = kmeans(all_descriptors, CLUSTER_COUNT, ITER)\n",
    "    matches_ids: list[list[tuple[int, float]]] =  get_matches_ids([image.descriptors for image in images.images], centroids, images.images)\n",
    "    for i, image in enumerate(images.images):\n",
    "        inner_list: list[Image, float] = [\n",
    "            (images.images[match[0]], match[1]) for match in matches_ids[i]\n",
    "        ]\n",
    "        image.similar_images = inner_list\n",
    "\n",
    "\n",
    "def get_visual_words(descriptors: list[np.ndarray], centroids: np.ndarray) -> list[np.ndarray]:\n",
    "    visual_words = []\n",
    "    for descriptor in descriptors:\n",
    "        words, _ = vq(descriptor, centroids)\n",
    "        visual_words.append(words)\n",
    "    return visual_words\n",
    "\n",
    "\n",
    "def get_frequency_vectors(visual_words: list[np.ndarray], CLUSTER_COUNT: int) -> np.ndarray:\n",
    "    frequency_vectors = []\n",
    "    for img_words in visual_words:\n",
    "        histogram = np.zeros(CLUSTER_COUNT)\n",
    "        for word in img_words:\n",
    "            histogram[word] += 1\n",
    "        frequency_vectors.append(histogram)\n",
    "    return np.stack(frequency_vectors)\n",
    "\n",
    "\n",
    "def get_tf_idf(frequency_vectors, IMAGES_COUNT) -> np.ndarray:\n",
    "    df = np.sum(frequency_vectors > 0, axis = 0)\n",
    "    idf = np.log(IMAGES_COUNT/df)\n",
    "    return frequency_vectors * idf\n",
    "\n",
    "\n",
    "def search_matches(i, top_clusters, tf_idf) -> list[tuple[int, float]]:\n",
    "    \"\"\" Search for the top_clusters most similar images to the i-th image\n",
    "    Args:\n",
    "        i: the index of the image to search for similar images\n",
    "        top_clusters: the number of similar images to return\n",
    "        tf_idf: Term Frequency-Inverse Document Frequency\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains the index of a similar image and the cosine similarity \n",
    "        between the i-th image and the similar image. The list is sorted by the cosine similarity in \n",
    "        descending order.\n",
    "    \"\"\"\n",
    "    b = tf_idf\n",
    "    a = tf_idf[i]\n",
    "    b_subset = b[:tf_idf.shape[0]]\n",
    "    cosine_similarity = np.dot(a, b_subset.T)/(norm(a) * norm(b_subset, axis=1))\n",
    "    idx = np.argsort(-cosine_similarity)[:top_clusters]\n",
    "    return list(zip(idx, cosine_similarity[idx]))\n",
    "\n",
    "\n",
    "def get_matches_ids(descriptors, centroids, images_list) -> list[list[tuple[int, float]]]:\n",
    "    \"\"\"Returns: a list of lists, where each list contains the top 10 most similar images to the i-th image.\"\"\"\n",
    "    visual_words = get_visual_words(descriptors, centroids)\n",
    "    frequency_vectors = get_frequency_vectors(visual_words, centroids.shape[0])\n",
    "    \"\"\" tf_idf: Term Frequency-Inverse Document Frequency \"\"\"\n",
    "    tf_idf = get_tf_idf(frequency_vectors, len(images_list))\n",
    "    return [\n",
    "        search_matches(i, 10, tf_idf)\n",
    "        for i in range(len(images_list))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Four: Feature Matching\n",
    "Inputs:\n",
    "- matches_ids: list[list[tuple[int, float]]]\n",
    "- descriptors: list[np.ndarray]\n",
    "\n",
    "Outputs:\n",
    "- feature_matches: list[list[int, int, list[OpenCV.DMatch]]]\n",
    "\n",
    "Main Functions:\n",
    "1. data_feature_matching\n",
    "\n",
    "Utils Functions:\n",
    "1. feature_matching\n",
    "2. dump_feature_matching\n",
    "3. load_feature_matching\n",
    "\"\"\"\n",
    "\n",
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    \"\"\" Match features between two images using Brute Force Matcher\n",
    "    Args:\n",
    "        img_id_one: the index of the first image\n",
    "        img_id_two: the index of the second image\n",
    "        descriptors: a list of descriptors of the images\n",
    "    Returns:\n",
    "        A list of OpenCV.DMatch objects.\n",
    "    \"\"\"\n",
    "    matcher = OpenCV.BFMatcher()\n",
    "    return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "def data_feature_matching(images: Images) -> None:\n",
    "    \"\"\" Match features between images using Brute Force Matcher\n",
    "    Args:\n",
    "        matchesIDs: a list of lists of tuples, where each tuple contains the index of a similar image and the cosine similarity \n",
    "            between the i-th image and the similar image. The list is sorted by the cosine similarity in \n",
    "            descending order.\n",
    "        descriptors: a list of descriptors of the images\n",
    "    Returns:\n",
    "        A list of lists, where each list contains \n",
    "        the index of the first image, the index of the second image, \n",
    "        and a list of OpenCV.DMatch objects.\n",
    "    \"\"\"\n",
    "    num_images: int = len(images.images)\n",
    "    checked = np.zeros((num_images, num_images), dtype=int)\n",
    "    # feature_matches_list: list[list[int, int, list[OpenCV.DMatch]]] = []\n",
    "    for image in images.images:\n",
    "        # logging.info(f\"---------- START Matches for: {str(imageID)}\")\n",
    "        for matched_image, probability in image.similar_images:\n",
    "            if ((checked[image.img_id][matched_image.img_id] == 0 or checked[matched_image.img_id][image.img_id] == 0) and image.img_id != matched_image.img_id and probability > 0.93):\n",
    "                # start_time = time.time()\n",
    "                images.feature_matches.append(FeatureMatches(image, matched_image, feature_matching(image.descriptors, matched_image.descriptors)))\n",
    "                checked[image.img_id][matched_image.img_id], checked[matched_image.img_id][image.img_id] = 1, 1\n",
    "                # logging.info(f\"done [{i}/{len(matchesIDs[imageID])}] in {(time.time() - start_time):.4f}: {str(imageID)} - {str(matchedID)}\")\n",
    "        # Flush the log file force write to disk\n",
    "        # logging.shutdown()\n",
    "\n",
    "\n",
    "# def convert_matches_to_dicts(matches: list[OpenCV.DMatch]) -> list[dict[str, int, int, float]]:\n",
    "#     \"\"\"Convert a list of OpenCV.DMatch objects to a list of dictionaries to serialize.\"\"\"\n",
    "#     match_dicts = []\n",
    "#     for match in matches:\n",
    "#         match_dict = {'queryIdx': match.queryIdx, 'trainIdx': match.trainIdx, 'distance': match.distance}\n",
    "#         match_dicts.append(match_dict)\n",
    "#     return match_dicts\n",
    "\n",
    "\n",
    "# def load_feature_matching(path: str) -> list[list[int, int, list[OpenCV.DMatch]]]:\n",
    "#     \"\"\"Load a list of lists of OpenCV.DMatch objects from a pickle file.\"\"\"\n",
    "#     with open(path, 'rb') as f:\n",
    "#         feature_matches_dicts = pickle.load(f)\n",
    "#     feature_matches = []\n",
    "#     for match_dict in feature_matches_dicts:\n",
    "#         matches = [\n",
    "#             OpenCV.DMatch(\n",
    "#                 match['queryIdx'], \n",
    "#                 match['trainIdx'], \n",
    "#                 match['distance']\n",
    "#             ) for match in match_dict[2]\n",
    "#         ]\n",
    "#         feature_matches.append([match_dict[0], match_dict[1], matches])\n",
    "#     return feature_matches\n",
    "\n",
    "\n",
    "# def dump_feature_matching(path: str, feature_matches: list[list[int, int, list[OpenCV.DMatch]]]) -> None:\n",
    "#     \"\"\"Dump a list of lists of OpenCV.DMatch objects to a pickle file.\"\"\"\n",
    "#     matches_dicts = [\n",
    "#         [\n",
    "#             match[0],\n",
    "#             match[1], \n",
    "#             convert_matches_to_dicts(match[2])\n",
    "#         ] for match in feature_matches\n",
    "#     ]\n",
    "#     with open(path, 'wb') as f:\n",
    "#         pickle.dump(matches_dicts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Step Five: Camera Calibration\\nInputs:\\n- None.\\n\\nOutputs:\\n- k_matrix: np.ndarray\\n\\nToDo:\\n1- generate K_matrix.pickle for each camera using Chess board pattern.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Step Five: Camera Calibration\n",
    "Inputs:\n",
    "- None.\n",
    "\n",
    "Outputs:\n",
    "- k_matrix: np.ndarray\n",
    "\n",
    "ToDo:\n",
    "1- generate K_matrix.pickle for each camera using Chess board pattern.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Six: Triangulation (3D Reconstruction)\n",
    "Inputs:\n",
    "- feature_matches_list: list[list[int, int, list[OpenCV.DMatch]]]\n",
    "    -> A list of lists, where each list contains \n",
    "        the index of the first image, the index of the second image, \n",
    "        and a list of OpenCV.DMatch objects.\n",
    "- K_matrix: np.ndarray\n",
    "    -> The camera matrix of the camera used to take the images.\n",
    "\n",
    "Outputs:\n",
    "- point_cloud: list[np.ndarray]; each element is a 3D point.\n",
    "\n",
    "Main Functions:\n",
    "1. generate_point_cloud\n",
    "\n",
    "Utils Functions:\n",
    "1. triangulatePoints\n",
    "\"\"\"\n",
    "\n",
    "def triangulatePoints(P1, P2, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Triangulates the given matching points from two images using the given camera matrices.\n",
    "\n",
    "    Parameters:\n",
    "    P1 (numpy.ndarray): 3x4 camera matrix of the first image.\n",
    "    P2 (numpy.ndarray): 3x4 camera matrix of the second image.\n",
    "    pts1 (numpy.ndarray): Nx2 matrix containing the coordinates of matching points in the first image.\n",
    "    pts2 (numpy.ndarray): Nx2 matrix containing the coordinates of matching points in the second image.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Nx3 matrix containing the triangulated 3D points.\n",
    "    \"\"\"\n",
    "    pts4D = OpenCV.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    pts4D /= pts4D[3]\n",
    "    return pts4D[:3].T\n",
    "\n",
    "\n",
    "def generate_point_cloud(images: Images, K_matrix):\n",
    "    \"\"\"\n",
    "    Generates a cloud of 3D points using triangulation from feature matches and camera calibration matrix.\n",
    "\n",
    "    Parameters:\n",
    "    feature_matches_list (list): List of feature matches between images.\n",
    "    K_matrix (numpy.ndarray): 3x3 camera calibration matrix.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Nx3 matrix containing the cloud of 3D points.\n",
    "    \"\"\"\n",
    "    point_cloud = []\n",
    "    feature_matches_list = images.feature_matches\n",
    "    for match in feature_matches_list:\n",
    "        img_one = match.image_one\n",
    "        img_two = match.image_two\n",
    "        matches = match.matches\n",
    "        pts1 = np.float32([img_one.keypoints[m.queryIdx].pt for m in matches])\n",
    "        pts2 = np.float32([img_two.keypoints[m.trainIdx].pt for m in matches])\n",
    "        E, _ = OpenCV.findEssentialMat(pts1, pts2, K_matrix)\n",
    "        R1, R2, t = OpenCV.decomposeEssentialMat(E)\n",
    "\n",
    "        for i in range(len(R1)):\n",
    "            P1 = np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "            P2 = np.hstack((R1[i], t.reshape(3, 1)))\n",
    "            # P2 = np.hstack((R1[i], t))\n",
    "            pts_3d = triangulatePoints(K_matrix.dot(P1), K_matrix.dot(P2), pts1, pts2)\n",
    "            point_cloud.append(pts_3d)\n",
    "    return np.concatenate(point_cloud, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome ScanMate...\n",
      "File [bak/snow-man/sift-images.pkl] exists\n",
      "Loading images from pickle file...\n",
      "Images loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome ScanMate...\")\n",
    "image_set_name = \"snow-man\"\n",
    "# 1. Load and prepare Images\n",
    "if os.path.isfile(f\"bak/{image_set_name}/images.pkl\"):\n",
    "    print(f\"File [bak/{image_set_name}/sift-images.pkl] exists\")\n",
    "    print(\"Loading images from pickle file...\")\n",
    "    images: Images = load_images_bak(f\"bak/{image_set_name}/images.pkl\")\n",
    "else:\n",
    "    print(f\"File [bak/{image_set_name}/images.pkl] does not exist\")\n",
    "    print(\"Loading images from images directory...\")\n",
    "    images: Images = prepare_images(f\"images/{image_set_name}\")\n",
    "    print(\"Saving images to pickle file...\")\n",
    "    dump_images_bak(f\"bak/{image_set_name}/images.pkl\", images)\n",
    "print(\"Images loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File [bak/snow-man/sift-features.pkl] exists\n",
      "Feature Extraction: SIFT DONE...\n"
     ]
    }
   ],
   "source": [
    "# 2. Feature Extraction: SIFT\n",
    "if os.path.isfile(f\"bak/{image_set_name}/sift-features.pkl\"):\n",
    "    print(f\"File [bak/{image_set_name}/sift-features.pkl] exists\")\n",
    "    images: Images = load_images_bak(f\"bak/{image_set_name}/sift-features.pkl\")\n",
    "else:\n",
    "    print(\"File [bak/{image_set_name}/sift-features.pkl] DO NOT exists\")\n",
    "    print(\"Extracting SIFT features...\")\n",
    "    sift = OpenCV.SIFT_create()\n",
    "    compute_keypoints_descriptors(images, sift)\n",
    "    dump_images_bak(f\"bak/{image_set_name}/sift-features.pkl\", images)\n",
    "print(\"Feature Extraction: SIFT DONE...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n",
      "Image 0 has 30209 keypoints and 30209 descriptors\n",
      "Image 1 has 24060 keypoints and 24060 descriptors\n",
      "Image 2 has 15784 keypoints and 15784 descriptors\n",
      "Image 3 has 20075 keypoints and 20075 descriptors\n",
      "Image 4 has 9188 keypoints and 9188 descriptors\n",
      "Image 5 has 8359 keypoints and 8359 descriptors\n",
      "Image 6 has 7740 keypoints and 7740 descriptors\n",
      "Image 7 has 5260 keypoints and 5260 descriptors\n",
      "Image 8 has 6567 keypoints and 6567 descriptors\n",
      "Image 9 has 5377 keypoints and 5377 descriptors\n",
      "Image 10 has 7142 keypoints and 7142 descriptors\n",
      "Image 11 has 1003 keypoints and 1003 descriptors\n",
      "Image 12 has 12652 keypoints and 12652 descriptors\n",
      "Image 13 has 2439 keypoints and 2439 descriptors\n",
      "Image 14 has 3968 keypoints and 3968 descriptors\n",
      "Image 15 has 1067 keypoints and 1067 descriptors\n",
      "Image 16 has 2312 keypoints and 2312 descriptors\n",
      "Image 17 has 3024 keypoints and 3024 descriptors\n",
      "Image 18 has 4884 keypoints and 4884 descriptors\n",
      "Image 19 has 4276 keypoints and 4276 descriptors\n",
      "Image 20 has 6266 keypoints and 6266 descriptors\n",
      "Image 21 has 14432 keypoints and 14432 descriptors\n",
      "Image 22 has 10849 keypoints and 10849 descriptors\n",
      "Image 23 has 4665 keypoints and 4665 descriptors\n",
      "Image 24 has 3486 keypoints and 3486 descriptors\n",
      "Image 25 has 18863 keypoints and 18863 descriptors\n",
      "Image 26 has 10004 keypoints and 10004 descriptors\n",
      "Image 27 has 3094 keypoints and 3094 descriptors\n",
      "Image 28 has 11132 keypoints and 11132 descriptors\n",
      "Image 29 has 16107 keypoints and 16107 descriptors\n",
      "Image 30 has 7090 keypoints and 7090 descriptors\n",
      "Image 31 has 3369 keypoints and 3369 descriptors\n",
      "Image 32 has 2016 keypoints and 2016 descriptors\n",
      "Image 33 has 2037 keypoints and 2037 descriptors\n",
      "Image 34 has 1066 keypoints and 1066 descriptors\n",
      "Image 35 has 2440 keypoints and 2440 descriptors\n",
      "Image 36 has 809 keypoints and 809 descriptors\n",
      "Image 37 has 2175 keypoints and 2175 descriptors\n",
      "Image 38 has 957 keypoints and 957 descriptors\n",
      "Image 39 has 3030 keypoints and 3030 descriptors\n",
      "Image 40 has 10364 keypoints and 10364 descriptors\n",
      "Image 41 has 4724 keypoints and 4724 descriptors\n",
      "Image 42 has 10722 keypoints and 10722 descriptors\n",
      "Image 43 has 9490 keypoints and 9490 descriptors\n",
      "Image 44 has 17010 keypoints and 17010 descriptors\n",
      "Image 45 has 4034 keypoints and 4034 descriptors\n",
      "Image 46 has 19569 keypoints and 19569 descriptors\n",
      "Image 47 has 43910 keypoints and 43910 descriptors\n",
      "Image 48 has 5187 keypoints and 5187 descriptors\n",
      "Image 49 has 2378 keypoints and 2378 descriptors\n",
      "Image 50 has 24528 keypoints and 24528 descriptors\n",
      "Image 51 has 16356 keypoints and 16356 descriptors\n",
      "Image 52 has 75803 keypoints and 75803 descriptors\n",
      "Image 53 has 91296 keypoints and 91296 descriptors\n",
      "Image 54 has 35795 keypoints and 35795 descriptors\n",
      "Image 55 has 56258 keypoints and 56258 descriptors\n",
      "Image 56 has 1411 keypoints and 1411 descriptors\n",
      "Image 57 has 27972 keypoints and 27972 descriptors\n",
      "Image 58 has 10626 keypoints and 10626 descriptors\n",
      "Image 59 has 11699 keypoints and 11699 descriptors\n",
      "Image 60 has 6130 keypoints and 6130 descriptors\n",
      "Image 61 has 7888 keypoints and 7888 descriptors\n",
      "Image 62 has 275 keypoints and 275 descriptors\n",
      "Image 63 has 6143 keypoints and 6143 descriptors\n",
      "Image 64 has 5031 keypoints and 5031 descriptors\n",
      "Image 65 has 2885 keypoints and 2885 descriptors\n",
      "Image 66 has 3741 keypoints and 3741 descriptors\n",
      "Image 67 has 8548 keypoints and 8548 descriptors\n",
      "Image 68 has 3902 keypoints and 3902 descriptors\n",
      "Image 69 has 517 keypoints and 517 descriptors\n",
      "Image 70 has 10169 keypoints and 10169 descriptors\n",
      "Image 71 has 5660 keypoints and 5660 descriptors\n",
      "Image 72 has 9519 keypoints and 9519 descriptors\n",
      "Image 73 has 14547 keypoints and 14547 descriptors\n",
      "Image 74 has 8674 keypoints and 8674 descriptors\n",
      "Image 75 has 9612 keypoints and 9612 descriptors\n",
      "Image 76 has 3574 keypoints and 3574 descriptors\n",
      "Image 77 has 5413 keypoints and 5413 descriptors\n",
      "Image 78 has 6014 keypoints and 6014 descriptors\n",
      "Image 79 has 19098 keypoints and 19098 descriptors\n",
      "Image 80 has 30201 keypoints and 30201 descriptors\n",
      "Image 81 has 1464 keypoints and 1464 descriptors\n",
      "Image 82 has 9389 keypoints and 9389 descriptors\n",
      "Image 83 has 23071 keypoints and 23071 descriptors\n",
      "Image 84 has 20041 keypoints and 20041 descriptors\n",
      "Image 85 has 19830 keypoints and 19830 descriptors\n",
      "Image 86 has 6859 keypoints and 6859 descriptors\n",
      "Image 87 has 3894 keypoints and 3894 descriptors\n",
      "Image 88 has 12667 keypoints and 12667 descriptors\n",
      "Image 89 has 6466 keypoints and 6466 descriptors\n",
      "Image 90 has 3149 keypoints and 3149 descriptors\n",
      "Image 91 has 5084 keypoints and 5084 descriptors\n",
      "Image 92 has 52593 keypoints and 52593 descriptors\n",
      "Image 93 has 102355 keypoints and 102355 descriptors\n",
      "Image 94 has 12823 keypoints and 12823 descriptors\n",
      "Image 95 has 32483 keypoints and 32483 descriptors\n",
      "Image 96 has 23916 keypoints and 23916 descriptors\n",
      "Image 97 has 5067 keypoints and 5067 descriptors\n",
      "Image 98 has 42670 keypoints and 42670 descriptors\n",
      "Image 99 has 30971 keypoints and 30971 descriptors\n",
      "Image 100 has 26247 keypoints and 26247 descriptors\n",
      "Image 101 has 37630 keypoints and 37630 descriptors\n",
      "Image 102 has 42297 keypoints and 42297 descriptors\n",
      "Image 103 has 50236 keypoints and 50236 descriptors\n",
      "Image 104 has 16449 keypoints and 16449 descriptors\n",
      "Image 105 has 7230 keypoints and 7230 descriptors\n",
      "Image 106 has 12537 keypoints and 12537 descriptors\n",
      "Image 107 has 6261 keypoints and 6261 descriptors\n",
      "Image 108 has 4626 keypoints and 4626 descriptors\n",
      "Image 109 has 1141 keypoints and 1141 descriptors\n",
      "Image 110 has 1508 keypoints and 1508 descriptors\n",
      "Image 111 has 11611 keypoints and 11611 descriptors\n",
      "Image 112 has 72755 keypoints and 72755 descriptors\n",
      "Image 113 has 137129 keypoints and 137129 descriptors\n",
      "Image 114 has 110434 keypoints and 110434 descriptors\n",
      "Image 115 has 18104 keypoints and 18104 descriptors\n",
      "Image 116 has 22413 keypoints and 22413 descriptors\n",
      "Image 117 has 60194 keypoints and 60194 descriptors\n",
      "Image 118 has 28468 keypoints and 28468 descriptors\n",
      "Image 119 has 7706 keypoints and 7706 descriptors\n",
      "Image 120 has 7005 keypoints and 7005 descriptors\n",
      "Image 121 has 12665 keypoints and 12665 descriptors\n",
      "Image 122 has 8682 keypoints and 8682 descriptors\n",
      "Image 123 has 8702 keypoints and 8702 descriptors\n",
      "Image 124 has 33066 keypoints and 33066 descriptors\n",
      "Image 125 has 25576 keypoints and 25576 descriptors\n",
      "Image 126 has 75419 keypoints and 75419 descriptors\n",
      "Image 127 has 3357 keypoints and 3357 descriptors\n",
      "Image 128 has 19479 keypoints and 19479 descriptors\n",
      "Image 129 has 29750 keypoints and 29750 descriptors\n",
      "Image 130 has 34953 keypoints and 34953 descriptors\n",
      "Image 131 has 4640 keypoints and 4640 descriptors\n",
      "Image 132 has 57841 keypoints and 57841 descriptors\n",
      "Image 133 has 109502 keypoints and 109502 descriptors\n",
      "Image 134 has 29821 keypoints and 29821 descriptors\n",
      "Image 135 has 4255 keypoints and 4255 descriptors\n",
      "Image 136 has 7733 keypoints and 7733 descriptors\n",
      "Image 137 has 2413 keypoints and 2413 descriptors\n",
      "Image 138 has 8848 keypoints and 8848 descriptors\n",
      "Image 139 has 12692 keypoints and 12692 descriptors\n",
      "Image 140 has 23020 keypoints and 23020 descriptors\n",
      "Image 141 has 77360 keypoints and 77360 descriptors\n",
      "Image 142 has 97377 keypoints and 97377 descriptors\n",
      "Image 143 has 88101 keypoints and 88101 descriptors\n",
      "Image 144 has 35749 keypoints and 35749 descriptors\n",
      "Image 145 has 9190 keypoints and 9190 descriptors\n",
      "Image 146 has 49983 keypoints and 49983 descriptors\n",
      "Image 147 has 27149 keypoints and 27149 descriptors\n",
      "Image 148 has 13494 keypoints and 13494 descriptors\n",
      "Image 149 has 3925 keypoints and 3925 descriptors\n",
      "Image 150 has 5050 keypoints and 5050 descriptors\n",
      "Image 151 has 15210 keypoints and 15210 descriptors\n",
      "Image 152 has 5178 keypoints and 5178 descriptors\n",
      "Image 153 has 5287 keypoints and 5287 descriptors\n",
      "Image 154 has 3936 keypoints and 3936 descriptors\n",
      "Image 155 has 15374 keypoints and 15374 descriptors\n",
      "Image 156 has 25733 keypoints and 25733 descriptors\n",
      "Image 157 has 2551 keypoints and 2551 descriptors\n",
      "Image 158 has 4290 keypoints and 4290 descriptors\n",
      "Image 159 has 6748 keypoints and 6748 descriptors\n",
      "Image 160 has 42699 keypoints and 42699 descriptors\n",
      "Image 161 has 4524 keypoints and 4524 descriptors\n",
      "Image 162 has 3766 keypoints and 3766 descriptors\n",
      "Image 163 has 12856 keypoints and 12856 descriptors\n",
      "Image 164 has 5138 keypoints and 5138 descriptors\n",
      "Image 165 has 4136 keypoints and 4136 descriptors\n",
      "Image 166 has 2854 keypoints and 2854 descriptors\n",
      "Image 167 has 5692 keypoints and 5692 descriptors\n",
      "Image 168 has 4525 keypoints and 4525 descriptors\n",
      "Image 169 has 8908 keypoints and 8908 descriptors\n",
      "Image 170 has 10857 keypoints and 10857 descriptors\n"
     ]
    }
   ],
   "source": [
    "print(len(images.images))\n",
    "for i, image in enumerate(images.images):\n",
    "    print(f\"Image {i} has {len(image.keypoints)} keypoints and {len(image.descriptors)} descriptors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File [bak/snow-man/images-matched.pkl] exists\n",
      "Done Image Matching Step...\n"
     ]
    }
   ],
   "source": [
    "# 3. Image Matching\n",
    "if os.path.isfile(f\"bak/{image_set_name}/images-matched.pkl\"):\n",
    "    print(f\"File [bak/{image_set_name}/images-matched.pkl] exists\")\n",
    "    images: Images = load_images_bak(f\"bak/{image_set_name}/images-matched.pkl\")\n",
    "else:\n",
    "    print(f\"File [bak/{image_set_name}/images-matched.pkl] DO NOT exists\")\n",
    "    get_matches(images)\n",
    "    dump_images_bak(f\"bak/{image_set_name}/images-matched.pkl\", images)\n",
    "print(\"Done Image Matching Step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File [bak/snow-man/feature-matching-output.pkl] exists\n"
     ]
    }
   ],
   "source": [
    "# 4. Feature Matching\n",
    "if os.path.isfile(f\"bak/{image_set_name}/feature-matching-output.pkl\"):\n",
    "    print(f\"File [bak/{image_set_name}/feature-matching-output.pkl] exists\")\n",
    "    images: Images = load_images_bak(f\"bak/{image_set_name}/feature-matching-output.pkl\")\n",
    "else:\n",
    "    print(\"File [bak/{image_set_name}/feature-matching-output.pkl] Do NOT exists\")\n",
    "    # logging.info('----> Processing {image_set_name}...')\n",
    "    data_feature_matching(images)\n",
    "    dump_images_bak(f\"bak/{image_set_name}/feature-matching-output.pkl\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Calibration starts ....\n",
      "File bak/snow-man/checker/K_matrix.pickle exists\n"
     ]
    }
   ],
   "source": [
    "# 5. Camera Calibration\n",
    "print(\"Camera Calibration starts ....\")\n",
    "if not os.path.isfile(\"bak/snow-man/checker/K_matrix.pickle\"):\n",
    "    raise IntrinsicParametersNotFoundError(\"Intrinsic parameters not found\")\n",
    "print(\"File bak/snow-man/checker/K_matrix.pickle exists\")\n",
    "with open('bak/snow-man/checker/K_matrix.pickle', 'rb') as f:\n",
    "    K_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triangulation starts ....\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 6. Triangulation (3D reconstruction)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTriangulation starts ....\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m points_cloud \u001b[39m=\u001b[39m generate_point_cloud(images, K_matrix)\n\u001b[0;32m      4\u001b[0m np\u001b[39m.\u001b[39msavetxt(\u001b[39m\"\u001b[39m\u001b[39mpoints_cloud.txt\u001b[39m\u001b[39m\"\u001b[39m, points_cloud)\n",
      "Cell \u001b[1;32mIn[21], line 89\u001b[0m, in \u001b[0;36mgenerate_point_cloud\u001b[1;34m(images, K_matrix)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(R1)):\n\u001b[0;32m     88\u001b[0m     P1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((np\u001b[39m.\u001b[39meye(\u001b[39m3\u001b[39m), np\u001b[39m.\u001b[39mzeros((\u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m))))\n\u001b[1;32m---> 89\u001b[0m     P2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mhstack((R1[i], t))\n\u001b[0;32m     90\u001b[0m     pts_3d \u001b[39m=\u001b[39m triangulatePoints(K_matrix\u001b[39m.\u001b[39mdot(P1), K_matrix\u001b[39m.\u001b[39mdot(P2), pts1, pts2)\n\u001b[0;32m     91\u001b[0m     point_cloud\u001b[39m.\u001b[39mappend(pts_3d)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\CUFE\\Year 4\\Semester 2\\GP\\Project\\photogrammetry\\myvenv\\Lib\\site-packages\\numpy\\core\\shape_base.py:368\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39mif\u001b[39;00m arrs \u001b[39mand\u001b[39;00m arrs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 368\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, casting\u001b[39m=\u001b[39;49mcasting)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39mconcatenate(arrs, \u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mdtype, casting\u001b[39m=\u001b[39mcasting)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "# 6. Triangulation (3D reconstruction)\n",
    "print(\"Triangulation starts ....\")\n",
    "points_cloud = generate_point_cloud(images, K_matrix)\n",
    "np.savetxt(\"points_cloud.txt\", points_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/moham/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 7. generate mesh\n",
    "print(\"Generate mesh ....\")\n",
    "tri = Delaunay(points_cloud)\n",
    "mesh = trimesh.Trimesh(points_cloud, tri.simplices)\n",
    "mesh = mesh.simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/moham/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 8. output .obj, .stl and .ply files\n",
    "print(\"Generate mesh ....\")\n",
    "mesh.export(f\"output/{image_set_name}/snow_man.obj\")\n",
    "mesh.export(f\"output/{image_set_name}/snow_man.stl\")\n",
    "mesh.export(f\"output/{image_set_name}/snow_man.ply\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
