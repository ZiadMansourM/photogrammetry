"use strict";(self.webpackChunkscanmate=self.webpackChunkscanmate||[]).push([[81],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>d});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var _=a.createContext({}),m=function(e){var t=a.useContext(_),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=m(e.components);return a.createElement(_.Provider,{value:t},e.children)},l="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,_=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),l=m(n),g=i,d=l["".concat(_,".").concat(g)]||l[g]||p[g]||r;return n?a.createElement(d,o(o({ref:t},c),{},{components:n})):a.createElement(d,o({ref:t},c))}));function d(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=g;var s={};for(var _ in t)hasOwnProperty.call(t,_)&&(s[_]=t[_]);s.originalType=e,s[l]="string"==typeof e?e:i,o[1]=s;for(var m=2;m<r;m++)o[m]=n[m];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},3992:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>_,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>m});var a=n(7462),i=(n(7294),n(3905));const r={sidebar_position:6,id:"Trangulation",description:"Generate 3D points from feature matches.",slug:"/under-the-hood/trangulation"},o=void 0,s={unversionedId:"under-the-hood/Trangulation",id:"under-the-hood/Trangulation",title:"Trangulation",description:"Generate 3D points from feature matches.",source:"@site/docs/under-the-hood/genrate_points_cloud.md",sourceDirName:"under-the-hood",slug:"/under-the-hood/trangulation",permalink:"/under-the-hood/trangulation",draft:!1,editUrl:"https://github.com/ZiadMansourM/photogrammetry/tree/main/docs/under-the-hood/genrate_points_cloud.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6,id:"Trangulation",description:"Generate 3D points from feature matches.",slug:"/under-the-hood/trangulation"},sidebar:"tutorialSidebar",previous:{title:"Feature Matching",permalink:"/under-the-hood/feature-matching"},next:{title:"Our Mobile App",permalink:"/category/our-mobile-app"}},_={},m=[{value:"\ud83d\udcdd Generate Point Cloud",id:"-generate-point-cloud",level:2}],c={toc:m},l="wrapper";function p(e){let{components:t,...n}=e;return(0,i.kt)(l,(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"-generate-point-cloud"},"\ud83d\udcdd Generate Point Cloud"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},'@timeit\ndef find_3D_2D_correspondences(\n        image_two: Image,\n        feature_matches: list[FeatureMatches], \n        global_dict: dict[np.ndarray, set[tuple[int]]],\n        **kwargs\n    ) -> dict[np.ndarray, np.ndarray]:\n    local_dict: dict[np.ndarray, np.ndarray] = {}\n    for feature_match in feature_matches: # 1, 2, 3, 4 -> [(1, 2), "(1, 3)", (1, 4), "(2, 3)", (2, 4), (3, 4)]\n        if feature_match.image_two != image_two:\n            continue\n        for match in feature_match.matches:\n            search_keypoint_one = feature_match.image_one.keypoints[match.queryIdx].pt\n            search_img_id = feature_match.image_one.img_id\n            search_tuple = (search_img_id, search_keypoint_one)\n            for key, values in global_dict.items():\n                if search_tuple in values and key not in local_dict:\n                    local_dict[key] = image_two.keypoints[match.trainIdx].pt\n    return local_dict\n\n\n@timeit\ndef find_initial_camera_matrices(K: np.ndarray, keypoints_one: np.ndarray, keypoints_two: np.ndarray, **kwargs) -> tuple[np.ndarray, np.ndarray]:\n    E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n    # TODO: use mask to filter out outliers\n    _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K)\n    return (R, t) if check_coherent_rotation(R) else (None, None)\n\n\n@timeit\ndef find_next_camera_matrices(\n        images: Images,\n        image_one: Image,\n        image_two: Image, \n        K_matrix: np.ndarray, \n        global_dict: dict[np.ndarray, set[tuple[int]]],\n        **kwargs\n    ) -> tuple[np.ndarray, np.ndarray]:\n    image_set_name = kwargs[\'image_set_name\']\n    if image_one is not None:\n        log_to_file(\n            "logs/tune.log",\n            f"Using Images {image_one.img_id} and {image_two.img_id} in find_next_camera_matrices",\n        )\n    local_dict: dict[np.ndarray, np.ndarray] = find_3D_2D_correspondences(image_two, images.feature_matches, global_dict, image_set_name=image_set_name)\n    objectPoints = np.array(list(local_dict.keys())).reshape(-1, 3)\n    imagePoints = np.array(list(local_dict.values())).reshape(-1, 2)\n    log_to_file(\n        "logs/tune.log",\n        f"Found {objectPoints.shape[0]} 3D Points and {imagePoints.shape[0]} Image Points 3D-2D correspondences",\n    )\n    _, rvec, tvec, _ = OpenCV.solvePnPRansac(objectPoints, imagePoints, K_matrix, None)\n    R, _ = OpenCV.Rodrigues(rvec)\n    return R, tvec\n\n\n@timeit\ndef compute_points_3D(\n        P1: np.ndarray, \n        P2: np.ndarray, \n        image_one: Image,\n        image_two: Image,\n        keypoints_one: np.ndarray,\n        keypoints_two: np.ndarray,\n        global_dict: dict[np.ndarray, set[tuple[int]]],\n        **kwargs\n    ) -> np.ndarray:\n    image_set_name = kwargs[\'image_set_name\']\n    data_path: str = f"../../data/{image_set_name}"\n    points_3D = np.empty((3, len(keypoints_one)))\n    for point_counter, (keypoint_one, keypoint_two) in enumerate(zip(keypoints_one, keypoints_two)):\n        point_4D = OpenCV.triangulatePoints(P1, P2, keypoint_one.T, keypoint_two.T)  # 4x1\n        point_3D = (point_4D / point_4D[3])[:3]  # 3x1\n        if to_tuple(point_3D) in global_dict:\n            global_dict[to_tuple(point_3D)].add((image_one.img_id, to_tuple(keypoint_one)))\n            global_dict[to_tuple(point_3D)].add((image_two.img_id, to_tuple(keypoint_two)))\n        else:\n            global_dict[to_tuple(point_3D)] = {\n                (image_one.img_id, to_tuple(keypoint_one)),\n                (image_two.img_id, to_tuple(keypoint_two))\n            }\n        points_3D[:, point_counter] = point_3D.flatten()\n    log_to_file(\n        "logs/tune.log",\n        f"Computed {points_3D.shape[1]} 3D Points for Image pairs {image_one.img_id} and {image_two.img_id}",\n    )\n    return points_3D\n\n\n@timeit\ndef find_cluster_feature_matches( \n        images: Images, \n        values: list[Image],\n        **kwargs\n    ) -> list[FeatureMatches]: # [1,2,3] ----\x3e [1,2],[1,3]\n    image_set_name = kwargs[\'image_set_name\']\n    data_path: str = f"../../data/{image_set_name}"\n    cluster_reference_image = values[0]\n    cluster_feature_matches: list[FeatureMatches] = []\n    import itertools\n    for image, matched_image in itertools.combinations(values, 2):\n        if image.img_id != cluster_reference_image.img_id:\n            log_to_file(\n                "logs/tune.log",\n                f"Breaking itertools loop for {image.img_id} and {matched_image.img_id} in find_cluster_feature_matches\\n",\n            )\n            break\n        else:\n            appended_pair: FeatureMatches = next(\n                fm for fm in images.feature_matches\n                if fm.image_one.img_id == image.img_id and fm.image_two.img_id == matched_image.img_id\n            )\n            log_to_file("logs/tune.log", f"appended_pair: {appended_pair}")\n            cluster_feature_matches.append(appended_pair)\n    return cluster_feature_matches\n\n\n@timeit\ndef generate_points_cloud(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n    image_set_name = kwargs[\'image_set_name\']\n    points_cloud: list[list[np.ndarray]] = []\n    global_dict: dict[np.ndarray, set[tuple[int]]] = {}\n    camera_matrices: list[np.ndarray] = [(np.eye(3), np.zeros((3, 1)))]\n    for cluster, values in images.similar_images.items():\n        log_to_file(\n            "logs/tune.log",\n            f"--------------------- Entering Cluster {cluster} ---------------------",\n        )\n        cluster_feature_matches:list[FeatureMatches] = find_cluster_feature_matches(images, values, image_set_name=image_set_name)\n        log_to_file(\n            "logs/tune.log",\n            f"cluster_feature_matches: {cluster_feature_matches}\\n",\n        )\n        if cluster == list(images.similar_images.keys())[0]: # First cluster\n            P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1))))\n            for feature_match in cluster_feature_matches:\n                image_one = feature_match.image_one\n                image_two = feature_match.image_two\n                keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n                keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n                if feature_match == cluster_feature_matches[0]:  # First Feature Match Pair in the First Cluster, where we use recoverPose\n                    log_to_file(\n                        "logs/tune.log",\n                        f"Using Images {image_one.img_id} and {image_two.img_id} in recoverPose",\n                    )\n                    R, t = find_initial_camera_matrices(K_matrix, keypoints_one, keypoints_two, image_set_name=image_set_name)\n                    P2 = K_matrix @ np.hstack((R, t))\n                    camera_matrices.append((R, t))\n                else:\n                    R, tvec = find_next_camera_matrices(images, image_one, image_two, K_matrix, global_dict, image_set_name=image_set_name)\n                    P2 = K_matrix @ np.hstack((R, tvec))\n                    camera_matrices.append((R, tvec))\n                points_3D = compute_points_3D(P1, P2, image_one, image_two, keypoints_one, keypoints_two, global_dict, image_set_name=image_set_name)\n                points_cloud.append(points_3D)\n                log_to_file(\n                    "logs/tune.log",\n                    f"Global Dict 3D Points Size: {len(global_dict.keys())} \\n",\n                )\n        else: # Next Clusters\n            for feature_match in cluster_feature_matches:\n                image_one = feature_match.image_one\n                image_two = feature_match.image_two\n                keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n                keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n                if feature_match == cluster_feature_matches[0]: # First Iteration of the next Cluster\n                # Computing new P1 for the new cluster\n                    log_to_file(\n                        "logs/tune.log",\n                        f"Entered First Iteration of the cluster {cluster}",\n                    )\n                    log_to_file(\n                        "logs/tune.log",\n                        f"Using Image {image_one.img_id} as Reference Image in cluster {cluster} to compute P1 for cluster {cluster}",\n                    )\n                    P1_R, P1_tvec = find_next_camera_matrices(images, None, image_one, K_matrix, global_dict, image_set_name=image_set_name)\n                    P1 = K_matrix @ np.hstack((P1_R, P1_tvec))\n                R, tvec = find_next_camera_matrices(images, image_one, image_two, K_matrix, global_dict, image_set_name=image_set_name)\n                P2 = K_matrix @ np.hstack((R, tvec))\n                camera_matrices.append((R, tvec))\n                points_3D = compute_points_3D(P1, P2, image_one, image_two, keypoints_one, keypoints_two, global_dict, image_set_name=image_set_name)\n                points_cloud.append(points_3D)\n                log_to_file(\n                    "logs/tune.log",\n                    f"Global Dict 3D Points Size: {len(global_dict.keys())} \\n",\n                )\n        log_to_file(\n            "logs/tune.log",\n            f"--------------------- End of cluster {cluster} ---------------------\\n\\n",\n        )\n    points_cloud = np.hstack(points_cloud).T\n    log_to_file("logs/tune.log", "Done generating points cloud")\n    return points_cloud, camera_matrices\n\n\n@timeit\ndef create_camera_frustum(P: np.ndarray, scale: float) -> o3d.geometry.TriangleMesh:\n    vertices = np.array([[0.5, 0.5, 0], [0.5, -0.5, 0], [-0.5, -0.5, 0], [-0.5, 0.5, 0], [0, 0, -1]])\n    vertices *= scale\n    faces = np.array([[0, 1, 4], [1, 2, 4], [2, 3, 4], [3, 0, 4], [1, 0, 3]])\n    R, t = P\n    vertices = vertices @ R.T + t[:3].T\n    mesh = o3d.geometry.TriangleMesh()\n    mesh.vertices = o3d.utility.Vector3dVector(vertices)\n    mesh.triangles = o3d.utility.Vector3iVector(faces)\n    vertex_colors = np.ones((len(vertices), 3)) * [1, 0, 0]\n    mesh.vertex_colors = o3d.utility.Vector3dVector(vertex_colors)\n    # draw camera rod\n    start_point = np.array([0, 0, 0])\n    end_point = np.array([0, 0, 1])*scale\n    start_point = start_point @ R.T + t[:3].T\n    end_point = end_point @ R.T + t[:3].T\n    rod = o3d.geometry.TriangleMesh.create_cylinder(radius=0.02*scale, height=np.linalg.norm(end_point-start_point), resolution=20, split=4)\n    rod.vertices = o3d.utility.Vector3dVector(np.asarray(rod.vertices) + start_point)\n    vertex_colors = np.ones((len(rod.vertices), 3)) * [0, 0, 0]\n    rod.vertex_colors = o3d.utility.Vector3dVector(vertex_colors)\n    return mesh, rod\n')))}p.isMDXComponent=!0}}]);