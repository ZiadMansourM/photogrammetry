{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from typing import Final, Optional\n",
    "\n",
    "import cv2 as OpenCV\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, mask, keypoints, descriptors, path):\n",
    "        self.img_id: int = int(img_id)\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.mask: Image = mask\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def draw_sift_features(self):\n",
    "        image_with_sift = OpenCV.drawKeypoints(self.rgb_image, self.keypoints, None, flags=OpenCV.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        plt.imshow(image_with_sift)\n",
    "        plt.title(\"Image with SIFT Features\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_rgb_image(self, title: Optional[str] = None):\n",
    "        image = self.rgb_image\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_gray_image(self, title: Optional[str] = None):\n",
    "        image = self.gray_image\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_mask_image(self, title: Optional[str] = None):\n",
    "        image = self.mask\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.show()\n",
    "        \n",
    "    def display_dialated_image(self, title: Optional[str] = None):\n",
    "        print(self.mask.shape)\n",
    "        print(self.rgb_image.shape)\n",
    "        image = OpenCV.bitwise_and(self.rgb_image, self.rgb_image, mask=self.mask)\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.show()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def draw_matches(self, output_filename: str) -> None:\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "        for match in self.matches:\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "        OpenCV.imwrite(output_filename, combined_image)\n",
    "        \n",
    "    def animate_matches(self, output_directory: str) -> None:\n",
    "        for match in self.matches:\n",
    "            combined_image = OpenCV.hconcat([\n",
    "                self.image_one.rgb_image,\n",
    "                self.image_two.rgb_image\n",
    "            ])\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Write match.queryIdx at the top left corner\n",
    "            OpenCV.putText(\n",
    "                combined_image,\n",
    "                f\"{match.queryIdx}\",\n",
    "                (50, 150),  # position: 10 pixels from left, 20 pixels from top\n",
    "                OpenCV.FONT_HERSHEY_SIMPLEX,  # font\n",
    "                5,  # font scale\n",
    "                (0, 255, 0),  # font color (green)\n",
    "                5,  # thickness\n",
    "                OpenCV.LINE_AA  # line type\n",
    "            )\n",
    "            # Write match.trainIdx at the top right corner\n",
    "            image_two_width = self.image_one.rgb_image.shape[1]\n",
    "            OpenCV.putText(\n",
    "                combined_image,\n",
    "                f\"{match.trainIdx}\",\n",
    "                (image_two_width + 50, 150),  # position: 10 pixels from right, 20 pixels from top\n",
    "                OpenCV.FONT_HERSHEY_SIMPLEX,  # font\n",
    "                5,  # font scale\n",
    "                (0, 255, 0),  # font color (green)\n",
    "                5,  # thickness\n",
    "                OpenCV.LINE_AA  # line type\n",
    "            )\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "            # if path does not exist, create it\n",
    "            if not os.path.exists(output_directory):\n",
    "                os.makedirs(output_directory)\n",
    "            OpenCV.imwrite(\n",
    "                f\"{output_directory}/{match.queryIdx}_{match.trainIdx}.jpg\",\n",
    "                combined_image,\n",
    "            )\n",
    "        framerate = 120\n",
    "\n",
    "        # Get a list of image files in the directory\n",
    "        image_files = [f for f in os.listdir(output_directory) if f.endswith(\".jpg\")]\n",
    "        image_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
    "\n",
    "        print(\"debugging\")\n",
    "        print(image_files[0])\n",
    "\n",
    "        temp_input_file_txt_path = f\"{output_directory}/input_files.txt\"\n",
    "\n",
    "        # Create a temporary file with a list of input images\n",
    "        with open(temp_input_file_txt_path, \"w\") as f:\n",
    "            for image_file in image_files:\n",
    "                f.write(f\"file '{image_file}'\\n\")\n",
    "\n",
    "        # Run FFmpeg command to create a video\n",
    "        command = f'ffmpeg -y -f concat -safe 0 -i \"{temp_input_file_txt_path}\" -framerate {framerate} -c:v libx264 -pix_fmt yuv420p \"{output_directory.rsplit(\"/\", 1)[0]}/output-{self.image_one.img_id}_{self.image_two.img_id}.mp4\"'\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        # Remove temporary file\n",
    "        os.remove(temp_input_file_txt_path)\n",
    "        # Remove all image files\n",
    "        for image_file in image_files:\n",
    "            os.remove(os.path.join(output_directory, image_file))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "        self.similar_images: dict[list[Image]] = {}\n",
    "        self.num_clusters: int = 50\n",
    "\n",
    "    def save_feature_matches(self):\n",
    "        for match in self.feature_matches:\n",
    "            match.draw_matches(f\"data/{self.image_set_name}/output/feature-match/{match.image_one.img_id}_{match.image_two.img_id}.jpg\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def display_similar_images(self, key):\n",
    "        print(f\"cluster {key}\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        for value in self.similar_images[key]:\n",
    "            print(value)\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(value.path), OpenCV.COLOR_BGR2RGB)\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.title(value.path)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    def save_similar_images(self):\n",
    "        for cluster in self.similar_images.keys():\n",
    "            if not os.path.exists(f\"data/{self.image_set_name}/output/image-match/{cluster}\"):\n",
    "                os.makedirs(f\"data/{self.image_set_name}/output/image-match/{cluster}\")\n",
    "            for value in self.similar_images[cluster]:\n",
    "                OpenCV.imwrite(f\"data/{self.image_set_name}/output/image-match/{cluster}/{value.img_id}.jpg\", value.rgb_image)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Image:\n",
    "        for image in self.images:\n",
    "            if image.img_id == key:\n",
    "                return image\n",
    "        raise KeyError(f'Image with img_id {key} not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray,\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    matcher = OpenCV.BFMatcher(crossCheck=True)\n",
    "    return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "\n",
    "def apply_ransac(matches, keypoints1, keypoints2, threshold = 3.0):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    _, mask = OpenCV.findHomography(src_pts, dst_pts, OpenCV.RANSAC, threshold)\n",
    "    matches_mask = mask.ravel().tolist()\n",
    "    return [m for m, keep in zip(matches, matches_mask) if keep]\n",
    "\n",
    "\n",
    "def data_feature_matching(images: Images) -> None:\n",
    "    import itertools\n",
    "    for _, values in images.similar_images.items():\n",
    "        print(images.similar_images.items())\n",
    "        for image, matched_image in itertools.combinations(values, 2):\n",
    "            if image.img_id != images.similar_images[\"0\"][0].img_id:\n",
    "                continue\n",
    "            print(f\"Matching {image.img_id} with {matched_image.img_id}\")\n",
    "            feature_matching_output = feature_matching(image.descriptors, matched_image.descriptors)\n",
    "            ransac_output = apply_ransac(feature_matching_output, image.keypoints, matched_image.keypoints, threshold=150)\n",
    "            images.feature_matches.append(FeatureMatches(image, matched_image, ransac_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as OpenCV\n",
    "from typing import List, Tuple\n",
    "\n",
    "def check_coherent_rotation(R: np.ndarray) -> bool:\n",
    "    epsilon = 1e-6\n",
    "    return np.abs(np.linalg.det(R) - 1.0) <= epsilon\n",
    "\n",
    "def find_camera_matrices(K: np.ndarray, keypoints_one: np.ndarray, keypoints_two: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K)\n",
    "    # Check if the resulting rotation is coherent\n",
    "    if not check_coherent_rotation(R):\n",
    "        print(\"Resulting rotation is not coherent\")\n",
    "        return None, None\n",
    "    print(\"Resulting rotation is coherent\")\n",
    "    return np.hstack((R, t))\n",
    "\n",
    "def generate_point_cloud_general(images: Images, K_matrix: np.ndarray, **kwargs) -> Tuple[list[np.ndarray], np.ndarray]:\n",
    "    point_cloud: list[np.ndarray] = []\n",
    "    camera_matrices: list[np.ndarray] = []\n",
    "    P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "    camera_matrices.append((np.eye(3), np.zeros((3, 1))))\n",
    "    for feature_match in images.feature_matches:\n",
    "        image_one = feature_match.image_one\n",
    "        image_two = feature_match.image_two\n",
    "        # Extract matched keypoints\n",
    "        keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "        keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "        # Find Camera Matrices\n",
    "        P2 = find_camera_matrices(K_matrix, keypoints_one, keypoints_two, feature_match.matches)\n",
    "        if P2 is None:\n",
    "            continue\n",
    "        # Create projection matrices\n",
    "        P2 = K_matrix @ P2\n",
    "        # Triangulate points\n",
    "        points_4D = OpenCV.triangulatePoints(P1, P2, keypoints_one.T, keypoints_two.T)\n",
    "        points_3D = (points_4D / points_4D[3])[:3]\n",
    "        R = np.linalg.inv(K_matrix) @ P2[:, :3]\n",
    "        t = np.linalg.inv(K_matrix) @ P2[:, 3:]\n",
    "        camera_matrices.append((R, t))\n",
    "        point_cloud.append(points_3D)\n",
    "    # Combine point cloud and camera matrices\n",
    "    point_cloud = np.hstack(point_cloud).T\n",
    "    # camera_matrices = np.array(camera_matrices)\n",
    "    return point_cloud, camera_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coherent_rotation(R: np.ndarray) -> bool:\n",
    "    epsilon = 1e-6\n",
    "    return np.abs(np.linalg.det(R) - 1.0) <= epsilon\n",
    "\n",
    "def find_camera_matrices(K: np.ndarray, keypoints_one: np.ndarray, keypoints_two: np.ndarray, matches: List) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K)\n",
    "    # Check if the resulting rotation is coherent\n",
    "    if not check_coherent_rotation(R):\n",
    "        print(\"Resulting rotation is not coherent\")\n",
    "        return None, None\n",
    "    print(\"Resulting rotation is coherent\")\n",
    "    return np.hstack((R, t))\n",
    "\n",
    "def find_camera_matrices_from_pnp(K: np.ndarray, keypoints: np.ndarray, point_cloud: list[tuple[np.ndarray, int]]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    inlier_keypoints = np.array([keypoints[p[1]] for p in point_cloud])\n",
    "    inlier_points_3D = np.array([p[0] for p in point_cloud])\n",
    "    _, rvec, tvec, _ = OpenCV.solvePnPRansac(inlier_points_3D, inlier_keypoints, K, None)\n",
    "    R, _ = OpenCV.Rodrigues(rvec)\n",
    "    return (None, None) if not check_coherent_rotation(R) else np.hstack((R, tvec))\n",
    "\n",
    "def generate_point_cloud_ultra(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    point_cloud = []\n",
    "    reference_image = images.similar_images[\"0\"][0]\n",
    "    P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "    for i in range(1, len(images.images)):\n",
    "        image = images.images[i]\n",
    "        # Find the matching features between the reference image and the current image.\n",
    "        matches = None\n",
    "        for feature_match in images.feature_matches:\n",
    "            if feature_match.image_one == reference_image and feature_match.image_two == image:\n",
    "                matches = feature_match.matches\n",
    "                break\n",
    "        if matches is None:\n",
    "            print(f\"no matches found between reference image: {reference_image.img_id}, image: {image.img_id}\")\n",
    "            continue\n",
    "        print(f\"reference image: {reference_image.img_id}, image: {image.img_id}\")\n",
    "        keypoints_one = np.array([reference_image.keypoints[m.queryIdx].pt for m in matches])\n",
    "        keypoints_two = np.array([image.keypoints[m.trainIdx].pt for m in matches])\n",
    "        if image == images.images[1]:\n",
    "            # Compute the initial camera matrix P2 using the reference image.\n",
    "            P2_init = find_camera_matrices(K_matrix, keypoints_one, keypoints_two, matches)\n",
    "            if P2_init is None:\n",
    "                print(\"failed to find initial camera matrix\")\n",
    "                continue\n",
    "            P2 = K_matrix @ P2_init\n",
    "        else:\n",
    "            # Perform incremental reconstruction using Perspective N-Point (PnP).\n",
    "            P2_pnp = find_camera_matrices_from_pnp(K_matrix, keypoints_two, point_cloud.T, matches)\n",
    "            if P2_pnp is None:\n",
    "                print(\"failed to find camera matrix using PnP\")\n",
    "                continue\n",
    "            P2 = K_matrix @ P2_pnp\n",
    "        points_4D = OpenCV.triangulatePoints(P1, P2, keypoints_one.T, keypoints_two.T)\n",
    "        points_3D = (points_4D / points_4D[3])[:3]\n",
    "        point_cloud.append(points_3D)\n",
    "\n",
    "        # Triangulate points and add them to the point cloud\n",
    "        points_4D = OpenCV.triangulatePoints(P1, P2, keypoints_one.T, keypoints_two.T)\n",
    "        points_3D = (points_4D / points_4D[3])[:3]\n",
    "        # Store the 3D points with their corresponding 2D keypoints indices\n",
    "        for i in range(points_3D.shape[1]):\n",
    "            point_cloud.append((points_3D[:, i], matches[i].trainIdx))\n",
    "    return np.hstack(point_cloud).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_images_bak(images_file_path: str, images: Images) -> None:\n",
    "    \"\"\" Dump images to a file \"\"\"\n",
    "    with open(images_file_path, \"wb\") as file:\n",
    "        pickle.dump(images, file)\n",
    "\n",
    "def load_images_bak(images_file_path: str) -> Images:\n",
    "    \"\"\" Load images from a file \"\"\"\n",
    "    with open(images_file_path, \"rb\") as file:\n",
    "        images = pickle.load(file)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_keypoints_descriptors(images: list[Image]) -> None:\n",
    "    sift = OpenCV.SIFT_create(contrastThreshold=0.01)\n",
    "    for img in images.images:\n",
    "        keypoints: list[OpenCV.KeyPoint]\n",
    "        descriptors: np.ndarray\n",
    "        dialated_image = OpenCV.bitwise_and(img.gray_image, img.gray_image, mask=img.mask)\n",
    "        keypoints, descriptors = sift.detectAndCompute(dialated_image, None)\n",
    "        img.keypoints = keypoints\n",
    "        img.descriptors = descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(create_mask = False, **kwargs) -> Images:\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    folder_path = f\"data/{image_set_name}\"\n",
    "    images: Images = Images([], folder_path.split(\"/\")[-1])\n",
    "    files: list[str] = list(\n",
    "        filter(\n",
    "            lambda file: \".jpg\" in file, os.listdir(f\"{folder_path}/images\")\n",
    "        )\n",
    "    )\n",
    "    if create_mask:\n",
    "        from rembg import remove\n",
    "        for file in files:\n",
    "            image_path = f\"{folder_path}/images/{file}\"\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "            gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask = remove(rgb_image)\n",
    "            mask = OpenCV.cvtColor(mask, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask[mask > 0] = 255\n",
    "            OpenCV.imwrite(f\"{folder_path}/masks/{file}\", mask)\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            dilated_mask = OpenCV.dilate(mask, kernel, iterations=30)\n",
    "            images.images.append(Image(file.split(\".\")[0], rgb_image, gray_image, dilated_mask, [], [], image_path))\n",
    "    else:\n",
    "        for file in files:\n",
    "            image_path = f\"{folder_path}/images/{file}\"\n",
    "            mask_path = f\"{folder_path}/masks/{file}\"\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "            gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask = OpenCV.imread(mask_path, OpenCV.IMREAD_GRAYSCALE)\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            dilated_mask = OpenCV.dilate(mask, kernel, iterations=20)\n",
    "            images.images.append(Image(file.split(\".\")[0], rgb_image, gray_image, dilated_mask, [], [], image_path))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set_name = \"small-relief\"\n",
    "create_mask = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images: Optional[Images] = prepare_images(create_mask=create_mask, image_set_name=image_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([image.img_id for image in images.images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image in images.images[5:7]:\n",
    "#     image.display_dialated_image()\n",
    "images[5].display_dialated_image()\n",
    "images[6].display_dialated_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_keypoints_descriptors(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[5].draw_sift_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[6].draw_sift_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.similar_images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.similar_images = {\n",
    "    \"0\": [images[5], images[6]],\n",
    "    # \"1\": [images[11], images[12], images[13], images[14], images[15], images[16], images[17], images[18], images[19], images[20], images[21], \n",
    "    #     images[22], images[23], images[24], images[25], images[26], images[27]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.similar_images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature_matching(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in images.feature_matches:\n",
    "    match.draw_matches(f\"data/{image_set_name}/output/triangulate/{match.image_one.img_id}_{match.image_two.img_id}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of matches for each image pair\n",
    "for match in images.feature_matches:\n",
    "    print(f\"image_one: {match.image_one.img_id}, image_two: {match.image_two.img_id}, matches: {len(match.matches):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.feature_matches[0].animate_matches(f\"data/{image_set_name}/output/triangulate/1_21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_k_matrix(img_path: str, **kwargs) -> np.ndarray:\n",
    "    import numpy as np\n",
    "    focal_length: float = 5300.652188006483\n",
    "    principal_point_x = 1408\n",
    "    principal_point_y = 1056\n",
    "    scaling_factor: float = 1.0\n",
    "    return np.array(\n",
    "        [\n",
    "            [float(focal_length), 0, principal_point_x],\n",
    "            [0, float(focal_length), principal_point_y],\n",
    "            [0, 0, scaling_factor],\n",
    "        ]\n",
    "    )\n",
    "with open(f\"data/{image_set_name}/bak/K_matrix.pickle\", 'wb') as f:\n",
    "    pickle.dump(compute_k_matrix(f\"data/{image_set_name}/images\"), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images.feature_matches[0].matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"data/{image_set_name}/bak/K_matrix.pickle\", 'rb') as f:\n",
    "        K_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for i, j in itertools.product(range(3), range(3)):\n",
    "    print(K_matrix[i][j])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_cloud, camera_matrices = generate_point_cloud_general(images, K_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "hdbscan_model = hdbscan.HDBSCAN().fit(points_cloud)\n",
    "labels = hdbscan_model.labels_\n",
    "core_indices = np.where(labels != -1)[0]\n",
    "core_points_HDBSCAN = points_cloud[core_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# dbscan = DBSCAN(eps=0.5, min_samples=10).fit(points_cloud)\n",
    "# labels = dbscan.labels_\n",
    "# core_indices = np.where(labels != -1)[0]\n",
    "# core_points_DBSCAN = points_cloud[core_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(points_cloud.shape)\n",
    "# print(core_points_HDBSCAN.shape)\n",
    "# print(core_points_DBSCAN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "def create_camera_frustum(P: np.ndarray, scale: float) -> o3d.geometry.TriangleMesh:\n",
    "    vertices = np.array([[0.5, 0.5, 0], [0.5, -0.5, 0], [-0.5, -0.5, 0], [-0.5, 0.5, 0], [0, 0, -1]])\n",
    "    vertices *= scale\n",
    "    faces = np.array([[0, 1, 4], [1, 2, 4], [2, 3, 4], [3, 0, 4], [1, 0, 3]])\n",
    "    R, t = P\n",
    "    vertices = vertices @ R.T + t[:3].T\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(vertices)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    vertex_colors = np.ones((len(vertices), 3)) * [1, 0, 0]\n",
    "    mesh.vertex_colors = o3d.utility.Vector3dVector(vertex_colors)\n",
    "    # draw camera rod\n",
    "    start_point = np.array([0, 0, 0])\n",
    "    end_point = np.array([0, 0, 1])*scale\n",
    "    start_point = start_point @ R.T + t[:3].T\n",
    "    end_point = end_point @ R.T + t[:3].T\n",
    "    rod = o3d.geometry.TriangleMesh.create_cylinder(radius=0.02*scale, height=np.linalg.norm(end_point-start_point), resolution=20, split=4)\n",
    "    rod.vertices = o3d.utility.Vector3dVector(np.asarray(rod.vertices) + start_point)\n",
    "    vertex_colors = np.ones((len(rod.vertices), 3)) * [0, 0, 0]\n",
    "    rod.vertex_colors = o3d.utility.Vector3dVector(vertex_colors)\n",
    "    return mesh, rod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Create a point cloud object\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(core_points_HDBSCAN)\n",
    "point_cloud.paint_uniform_color([0, 0, 1])  # Set the point cloud color to blue for better visibility\n",
    "\n",
    "# Loop through the camera_matrices and create a red pyramid for each camera\n",
    "camera_meshes = []\n",
    "camera_lines = []\n",
    "for camera_matrix in camera_matrices:\n",
    "    camera_mesh, camera_line = create_camera_frustum(camera_matrix, scale=0.3)\n",
    "    camera_meshes.append(camera_mesh)\n",
    "    camera_lines.append(camera_line)\n",
    "\n",
    "# Visualize the camera pyramids and point cloud together\n",
    "# o3d.visualization.draw(camera_meshes + camera_lines + [point_cloud])\n",
    "\n",
    "# Combine camera meshes, camera lines, and point cloud into a single mesh\n",
    "combined_mesh = o3d.geometry.TriangleMesh()\n",
    "for mesh in camera_meshes + camera_lines:\n",
    "    combined_mesh += mesh\n",
    "\n",
    "# Save the point cloud to a .ply file\n",
    "point_cloud_file = f\"data/{image_set_name}/output/triangulate/point_cloud_5_6.ply\"\n",
    "o3d.io.write_point_cloud(point_cloud_file, point_cloud)\n",
    "\n",
    "# Save the combined mesh to a .ply file\n",
    "mesh_file = f\"data/{image_set_name}/output/triangulate/camera_proj_5_6.ply\"\n",
    "o3d.io.write_triangle_mesh(mesh_file, combined_mesh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Points Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "point_cloud_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/point_cloud_5_6.ply\"\n",
    ")\n",
    "mesh_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/camera_proj_5_6.ply\"\n",
    ")\n",
    "\n",
    "point_cloud = o3d.io.read_point_cloud(point_cloud_file_path)\n",
    "mesh = o3d.io.read_triangle_mesh(mesh_file_path)\n",
    "\n",
    "o3d.visualization.draw_geometries([point_cloud, mesh])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
