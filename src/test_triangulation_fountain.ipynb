{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangulate Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from typing import Final, Optional\n",
    "\n",
    "import cv2 as OpenCV\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial import Delaunay\n",
    "import cv2\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, mask, keypoints, descriptors, path):\n",
    "        self.img_id: int = int(img_id)\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.mask: Image = mask\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def draw_sift_features(self):\n",
    "        image_with_sift = OpenCV.drawKeypoints(self.rgb_image, self.keypoints, None, flags=OpenCV.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        plt.imshow(image_with_sift)\n",
    "        plt.title(\"Image with SIFT Features\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_rgb_image(self, title: Optional[str] = None):\n",
    "        image = self.rgb_image\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_gray_image(self, title: Optional[str] = None):\n",
    "        image = self.gray_image\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_mask_image(self, title: Optional[str] = None):\n",
    "        image = self.mask\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        # plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_dialated_image(self, title: Optional[str] = None):\n",
    "        print(self.mask.shape)\n",
    "        print(self.rgb_image.shape)\n",
    "        image = OpenCV.bitwise_and(self.rgb_image, self.rgb_image, mask=self.mask)\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        # plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def draw_matches(self, output_filename: str) -> None:\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "        for match in self.matches:\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "        OpenCV.imwrite(output_filename, combined_image)\n",
    "        \n",
    "    def animate_matches(self, output_filename: str) -> None:\n",
    "        # for match in self.matches:\n",
    "        #     combined_image = OpenCV.hconcat([\n",
    "        #         self.image_one.rgb_image,\n",
    "        #         self.image_two.rgb_image\n",
    "        #     ])\n",
    "        #     x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "        #     x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "        #     # Write match.queryIdx at the top left corner\n",
    "        #     OpenCV.putText(\n",
    "        #         combined_image,\n",
    "        #         f\"{match.queryIdx}\",\n",
    "        #         (50, 150),  # position: 10 pixels from left, 20 pixels from top\n",
    "        #         OpenCV.FONT_HERSHEY_SIMPLEX,  # font\n",
    "        #         5,  # font scale\n",
    "        #         (0, 255, 0),  # font color (green)\n",
    "        #         5,  # thickness\n",
    "        #         OpenCV.LINE_AA  # line type\n",
    "        #     )\n",
    "        #     # Write match.trainIdx at the top right corner\n",
    "        #     image_two_width = self.image_one.rgb_image.shape[1]\n",
    "        #     OpenCV.putText(\n",
    "        #         combined_image,\n",
    "        #         f\"{match.trainIdx}\",\n",
    "        #         (image_two_width + 50, 150),  # position: 10 pixels from right, 20 pixels from top\n",
    "        #         OpenCV.FONT_HERSHEY_SIMPLEX,  # font\n",
    "        #         5,  # font scale\n",
    "        #         (0, 255, 0),  # font color (green)\n",
    "        #         5,  # thickness\n",
    "        #         OpenCV.LINE_AA  # line type\n",
    "        #     )\n",
    "        #     # Draw a line connecting the matched keypoints\n",
    "        #     OpenCV.line(\n",
    "        #         combined_image, \n",
    "        #         (int(x1), int(y1)), \n",
    "        #         (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "        #         (0, 255, 0), \n",
    "        #         1\n",
    "        #     )\n",
    "        #     OpenCV.imwrite(output_filename + f\"/{match.queryIdx}_{match.trainIdx}.jpg\", combined_image)\n",
    "        framerate = 120\n",
    "\n",
    "        # Get a list of image files in the directory\n",
    "        image_files = [f for f in os.listdir(output_filename) if f.endswith(\".jpg\")]\n",
    "        image_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
    "\n",
    "        # Create a temporary file with a list of input images\n",
    "        with open(\"input_files.txt\", \"w\") as f:\n",
    "            for image_file in image_files:\n",
    "                f.write(f\"file '{os.path.join(output_filename, image_file)}'\\n\")\n",
    "\n",
    "        # Run FFmpeg command to create a video\n",
    "        command = f'ffmpeg -y -f concat -safe 0 -i \"input_files.txt\" -framerate {framerate} -c:v libx264 -pix_fmt yuv420p \"{output_filename}/output.mp4\"'\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        # Remove temporary file\n",
    "        os.remove(\"input_files.txt\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "        self.similar_images: dict[list[Image]] = {}\n",
    "        self.num_clusters: int = 50\n",
    "\n",
    "    def save_feature_matches(self):\n",
    "        for match in self.feature_matches:\n",
    "            match.draw_matches(f\"data/{self.image_set_name}/output/feature-match/{match.image_one.img_id}_{match.image_two.img_id}.jpg\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def display_similar_images(self, key):\n",
    "        print(f\"cluster {key}\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        for value in self.similar_images[key]:\n",
    "            print(value)\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(value.path), OpenCV.COLOR_BGR2RGB)\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.title(value.path)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    def save_similar_images(self):\n",
    "        for cluster in self.similar_images.keys():\n",
    "            if not os.path.exists(f\"data/{self.image_set_name}/output/image-match/{cluster}\"):\n",
    "                os.makedirs(f\"data/{self.image_set_name}/output/image-match/{cluster}\")\n",
    "            for value in self.similar_images[cluster]:\n",
    "                OpenCV.imwrite(f\"data/{self.image_set_name}/output/image-match/{cluster}/{value.img_id}.jpg\", value.rgb_image)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Image:\n",
    "        for image in self.images:\n",
    "            if image.img_id == key:\n",
    "                return image\n",
    "        raise KeyError(f'Image with img_id {key} not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray,\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    matcher = OpenCV.BFMatcher(crossCheck=True)\n",
    "    return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "\n",
    "def apply_ransac(matches, keypoints1, keypoints2, threshold = 3.0):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    _, mask = OpenCV.findHomography(src_pts, dst_pts, OpenCV.RANSAC, threshold)\n",
    "    matches_mask = mask.ravel().tolist()\n",
    "    return [m for m, keep in zip(matches, matches_mask) if keep]\n",
    "\n",
    "\n",
    "def data_feature_matching(images: Images) -> None:\n",
    "    import itertools\n",
    "    for _, values in images.similar_images.items():\n",
    "        print(images.similar_images.items())\n",
    "        for image, matched_image in itertools.combinations(values, 2):\n",
    "            if image.img_id != images.similar_images[\"0\"][0].img_id:\n",
    "                continue\n",
    "            print(f\"Matching {image.img_id} with {matched_image.img_id}\")\n",
    "            feature_matching_output = feature_matching(image.descriptors, matched_image.descriptors)\n",
    "            ransac_output = apply_ransac(feature_matching_output, image.keypoints, matched_image.keypoints, threshold=150)\n",
    "            images.feature_matches.append(FeatureMatches(image, matched_image, ransac_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as OpenCV\n",
    "from typing import List, Tuple\n",
    "\n",
    "def check_coherent_rotation(R: np.ndarray) -> bool:\n",
    "    epsilon = 1e-6\n",
    "    return np.abs(np.linalg.det(R) - 1.0) <= epsilon\n",
    "\n",
    "\n",
    "def find_camera_matrices(K: np.ndarray, keypoints_one: np.ndarray, keypoints_two: np.ndarray, matches: List) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K)\n",
    "\n",
    "    # Check if the resulting rotation is coherent\n",
    "    if not check_coherent_rotation(R):\n",
    "        print(\"Resulting rotation is not coherent\")\n",
    "        return None, None\n",
    "    print(\"Resulting rotation is coherent\")\n",
    "\n",
    "    return np.hstack((R, t))\n",
    "\n",
    "\n",
    "def generate_point_cloud_general(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    point_cloud = []\n",
    "\n",
    "    for feature_match in images.feature_matches:\n",
    "        image_one = feature_match.image_one\n",
    "        image_two = feature_match.image_two\n",
    "\n",
    "        # Extract matched keypoints\n",
    "        keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "        keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "\n",
    "        # Find Camera Matrices\n",
    "        P2 = find_camera_matrices(K_matrix, keypoints_one, keypoints_two, feature_match.matches)\n",
    "        if P2 is None:\n",
    "            continue\n",
    "\n",
    "        # Create projection matrices\n",
    "        P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "        P2 = K_matrix @ P2\n",
    "\n",
    "        # Triangulate points\n",
    "        points_4D = OpenCV.triangulatePoints(P1, P2, keypoints_one.T, keypoints_two.T)\n",
    "        points_3D = (points_4D / points_4D[3])[:3]\n",
    "\n",
    "        point_cloud.append(points_3D)\n",
    "\n",
    "    return np.hstack(point_cloud).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coherent_rotation(R: np.ndarray) -> bool:\n",
    "    epsilon = 1e-6\n",
    "    return np.abs(np.linalg.det(R) - 1.0) <= epsilon\n",
    "\n",
    "def find_camera_matrices(K: np.ndarray, keypoints_one: np.ndarray, keypoints_two: np.ndarray, matches: List) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K)\n",
    "    # Check if the resulting rotation is coherent\n",
    "    if not check_coherent_rotation(R):\n",
    "        print(\"Resulting rotation is not coherent\")\n",
    "        return None, None\n",
    "    print(\"Resulting rotation is coherent\")\n",
    "    return np.hstack((R, t))\n",
    "\n",
    "def find_camera_matrices_from_pnp(K: np.ndarray, keypoints: np.ndarray, points_3D: np.ndarray, inliers: List[bool]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    inlier_keypoints = keypoints[inliers]\n",
    "    inlier_points_3D = points_3D[inliers]\n",
    "    _, rvec, tvec, _ = OpenCV.solvePnPRansac(inlier_points_3D, inlier_keypoints, K, None)\n",
    "    R, _ = OpenCV.Rodrigues(rvec)\n",
    "    if not check_coherent_rotation(R):\n",
    "        return None, None\n",
    "    return np.hstack((R, tvec))\n",
    "\n",
    "def generate_point_cloud_ultra(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    point_cloud = []\n",
    "    reference_image = images.similar_images[\"0\"][0]\n",
    "    for i in range(1, len(images.images)):\n",
    "        image = images.images[i]\n",
    "        # Find the matching features between the reference image and the current image.\n",
    "        matches = None\n",
    "        for feature_match in images.feature_matches:\n",
    "            if feature_match.image_one == reference_image and feature_match.image_two == image:\n",
    "                matches = feature_match.matches\n",
    "                break\n",
    "        if matches is None:\n",
    "            print(f\"no matches found between reference image: {reference_image.img_id}, image: {image.img_id}\")\n",
    "            continue\n",
    "        print(f\"reference image: {reference_image.img_id}, image: {image.img_id}\")\n",
    "        keypoints_one = np.array([reference_image.keypoints[m.queryIdx].pt for m in matches])\n",
    "        keypoints_two = np.array([image.keypoints[m.trainIdx].pt for m in matches])\n",
    "        if image == images.images[1]:\n",
    "            # Compute the initial camera matrix P2 using the reference image.\n",
    "            P2_init = find_camera_matrices(K_matrix, keypoints_one, keypoints_two, matches)\n",
    "            if P2_init is None:\n",
    "                print(\"failed to find initial camera matrix\")\n",
    "                continue\n",
    "            P2 = K_matrix @ P2_init\n",
    "        else:\n",
    "            # Perform incremental reconstruction using Perspective N-Point (PnP).\n",
    "            P2_pnp = find_camera_matrices_from_pnp(K_matrix, keypoints_two, point_cloud.T, matches)\n",
    "            if P2_pnp is None:\n",
    "                print(\"failed to find camera matrix using PnP\")\n",
    "                continue\n",
    "            P2 = K_matrix @ P2_pnp\n",
    "        P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "        points_4D = OpenCV.triangulatePoints(P1, P2, keypoints_one.T, keypoints_two.T)\n",
    "        points_3D = (points_4D / points_4D[3])[:3]\n",
    "        point_cloud.append(points_3D)\n",
    "    return np.hstack(point_cloud).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_images_bak(images_file_path: str, images: Images) -> None:\n",
    "    \"\"\" Dump images to a file \"\"\"\n",
    "    with open(images_file_path, \"wb\") as file:\n",
    "        pickle.dump(images, file)\n",
    "\n",
    "def load_images_bak(images_file_path: str) -> Images:\n",
    "    \"\"\" Load images from a file \"\"\"\n",
    "    with open(images_file_path, \"rb\") as file:\n",
    "        images = pickle.load(file)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_keypoints_descriptors(images: list[Image]) -> None:\n",
    "    sift = OpenCV.SIFT_create(contrastThreshold=0.01)\n",
    "    for img in images.images:\n",
    "        keypoints: list[OpenCV.KeyPoint]\n",
    "        descriptors: np.ndarray\n",
    "        dialated_image = OpenCV.bitwise_and(img.gray_image, img.gray_image, mask=img.mask)\n",
    "        keypoints, descriptors = sift.detectAndCompute(dialated_image, None)\n",
    "        img.keypoints = keypoints\n",
    "        img.descriptors = descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(create_mask = False, **kwargs) -> Images:\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    folder_path = f\"data/{image_set_name}\"\n",
    "    images: Images = Images([], folder_path.split(\"/\")[-1])\n",
    "    files: list[str] = list(filter(lambda file: \".jpg\" in file, os.listdir(folder_path + \"/images\")))\n",
    "    if create_mask:\n",
    "        from rembg import remove\n",
    "        for file in files:\n",
    "            image_path = f\"{folder_path}/images/{file}\"\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "            gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask = remove(rgb_image)\n",
    "            mask = OpenCV.cvtColor(mask, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask[mask > 0] = 255\n",
    "            OpenCV.imwrite(f\"{folder_path}/masks/{file}\", mask)\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            dilated_mask = OpenCV.dilate(mask, kernel, iterations=20)\n",
    "            images.images.append(Image(file.split(\".\")[0], rgb_image, gray_image, dilated_mask, [], [], image_path))\n",
    "    else:\n",
    "        for file in files:\n",
    "            image_path = f\"{folder_path}/images/{file}\"\n",
    "            mask_path = f\"{folder_path}/masks/{file}\"\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "            gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask = OpenCV.imread(mask_path, OpenCV.IMREAD_GRAYSCALE)\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            dilated_mask = OpenCV.dilate(mask, kernel, iterations=20)\n",
    "            images.images.append(Image(file.split(\".\")[0], rgb_image, gray_image, dilated_mask, [], [], image_path))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images: Optional[Images] = prepare_images(create_mask=True, image_set_name=\"fountain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([image.img_id for image in images.images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.images[0].display_rgb_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_keypoints_descriptors(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[2].draw_sift_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[3].draw_sift_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.similar_images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.similar_images = {\n",
    "    \"0\": [images[1],images[2],images[3],images[4]],\n",
    "    # \"1\": [images[11], images[12], images[13], images[14], images[15], images[16], images[17], images[18], images[19], images[20], images[21], \n",
    "    #     images[22], images[23], images[24], images[25], images[26], images[27]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.similar_images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.similar_images[\"0\"][0].display_rgb_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature_matching(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in images.feature_matches:\n",
    "    match.draw_matches(f\"data/fountain/output/triangulate/{match.image_one.img_id}_{match.image_two.img_id}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.feature_matches[0].animate_matches(\"data/fountain/output/triangulate/2_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_k_matrix(img_path: str, **kwargs) -> np.ndarray:\n",
    "    import numpy as np\n",
    "    # distortion_coefficients = exif['EXIF MakerNote'].values[0]\n",
    "    # Calculate the scaling factor for the K-matrix\n",
    "    focal_length = 3708.232031805074\n",
    "    principal_point_x = 1536\n",
    "    principal_point_y = 1024\n",
    "    scaling_factor = 1.0\n",
    "    return np.array(\n",
    "        [\n",
    "            [float(focal_length), 0, principal_point_x],\n",
    "            [0, float(focal_length), principal_point_y],\n",
    "            [0, 0, scaling_factor],\n",
    "        ]\n",
    "    )\n",
    "with open(f\"data/fountain/bak/K_matrix.pickle\", 'wb') as f:\n",
    "        pickle.dump(compute_k_matrix(\"data/fountain/images\"), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images.feature_matches[0].matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/fountain/bak/K_matrix.pickle\", 'rb') as f:\n",
    "        K_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(K_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering from: 10 and 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra_points_cloud: np.ndarray = generate_point_cloud_ultra(images, K_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "hdbscan_model = hdbscan.HDBSCAN().fit(ultra_points_cloud)\n",
    "labels = hdbscan_model.labels_\n",
    "core_indices = np.where(labels != -1)[0]\n",
    "ultra_core_points_HDBSCAN = ultra_points_cloud[core_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# dbscan = DBSCAN(eps=0.5, min_samples=10).fit(one_11_points_cloud)\n",
    "# labels = dbscan.labels_\n",
    "# core_indices = np.where(labels != -1)[0]\n",
    "# core_points_1_11_DBSCAN = one_11_points_cloud[core_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ultra_core_points_HDBSCAN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(ultra_core_points_HDBSCAN[:,:3])\n",
    "o3d.io.write_point_cloud(\n",
    "    \"data/fountain/output/triangulate/ultra_core_points_HDBSCAN.ply\", pcd\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Points Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "file_path = (\n",
    "    \"data/fountain/output/triangulate/ultra_core_points_HDBSCAN.ply\"\n",
    ")\n",
    "point_cloud = o3d.io.read_point_cloud(file_path)\n",
    "o3d.visualization.draw_geometries([point_cloud])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
