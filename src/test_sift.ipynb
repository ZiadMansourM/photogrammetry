{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from typing import Final, Optional\n",
    "\n",
    "import cv2 as OpenCV\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial import Delaunay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, mask, keypoints, descriptors, path):\n",
    "        self.img_id: int = int(img_id)\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "        self.mask: Image = mask\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def draw_sift_features(self):\n",
    "        image_with_sift = OpenCV.drawKeypoints(self.rgb_image, self.keypoints, None, flags=OpenCV.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        plt.imshow(image_with_sift)\n",
    "        plt.title(\"Image with SIFT Features\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_rgb_image(self, title: Optional[str] = None):\n",
    "        image = self.rgb_image\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_gray_image(self, title: Optional[str] = None):\n",
    "        image = self.gray_image\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_mask_image(self, title: Optional[str] = None):\n",
    "        image = self.mask\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        # plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_dialated_image(self, title: Optional[str] = None):\n",
    "        print(self.mask.shape)\n",
    "        print(self.rgb_image.shape)\n",
    "        image = OpenCV.bitwise_and(self.rgb_image, self.rgb_image, mask=self.mask)\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        # plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def draw_matches(self, output_filename: str) -> None:\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "\n",
    "        for match in self.matches:\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "\n",
    "        OpenCV.imwrite(output_filename, combined_image)\n",
    "        \n",
    "    def simulate_matches(self, output_dir: str) -> None:\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "\n",
    "        for i, match in enumerate(self.matches):\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "            \n",
    "            resized_image = OpenCV.resize(combined_image, None, fx=0.5, fy=0.5, interpolation=OpenCV.INTER_AREA)\n",
    "            OpenCV.imwrite(f\"{output_dir}/{i}.jpg\", resized_image)\n",
    "\n",
    "    def animate_matches(self, output_dir: str, scale_factor: float = 0.5) -> None:\n",
    "        import imageio\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "\n",
    "        # Create a GIF writer\n",
    "        with imageio.get_writer(f\"{output_dir}/animated_matches.gif\", mode='I', duration=0.001) as writer:\n",
    "            for i, match in enumerate(self.matches):\n",
    "                x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "                x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "                # Draw a line connecting the matched keypoints\n",
    "                OpenCV.line(\n",
    "                    combined_image, \n",
    "                    (int(x1), int(y1)), \n",
    "                    (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                    (0, 255, 0), \n",
    "                    1\n",
    "                )\n",
    "                # Resize the image\n",
    "                resized_image = OpenCV.resize(combined_image, None, fx=scale_factor, fy=scale_factor, interpolation=OpenCV.INTER_AREA)\n",
    "                # Convert the image from OpenCV BGR format to RGB format\n",
    "                rgb_image = OpenCV.cvtColor(resized_image, OpenCV.COLOR_BGR2RGB)\n",
    "                # Append the frame to the GIF\n",
    "                writer.append_data(rgb_image)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "        self.similar_images: dict[list[Image]] = {}\n",
    "        self.num_clusters: int = 50\n",
    "\n",
    "    def save_feature_matches(self):\n",
    "        for match in self.feature_matches:\n",
    "            match.draw_matches(f\"data/hammer/output/feature-match/{match.image_one.img_id}_{match.image_two.img_id}.jpg\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def display_similar_images(self, key):\n",
    "        print(f\"cluster {key}\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        for value in self.similar_images[key]:\n",
    "            print(value)\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(value.path), OpenCV.COLOR_BGR2RGB)\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.title(value.path)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    def save_similar_images(self):\n",
    "        for cluster in self.similar_images.keys():\n",
    "            if not os.path.exists(f\"data/{self.image_set_name}/output/image-match/{cluster}\"):\n",
    "                os.makedirs(f\"data/{self.image_set_name}/output/image-match/{cluster}\")\n",
    "            for value in self.similar_images[cluster]:\n",
    "                OpenCV.imwrite(f\"data/{self.image_set_name}/output/image-match/{cluster}/{value.img_id}.jpg\", value.rgb_image)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Image:\n",
    "        for image in self.images:\n",
    "            if image.img_id == key:\n",
    "                return image\n",
    "        raise KeyError(f'Image with img_id {key} not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = Images([], \"hammer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r'C:\\Users\\yousf\\OneDrive\\Desktop\\University\\Graduation Project\\Codes\\photogrammetry\\src\\data\\hammer\\images\\1.jpg'\n",
    "mask_path = r'C:\\Users\\yousf\\OneDrive\\Desktop\\University\\Graduation Project\\Codes\\photogrammetry\\src\\data\\hammer\\masks\\1.jpg'\n",
    "rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "mask = OpenCV.imread(mask_path, OpenCV.IMREAD_GRAYSCALE)\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "dilated_mask = OpenCV.dilate(mask, kernel, iterations=20)\n",
    "images.images.append(Image(1, rgb_image, gray_image, dilated_mask, [], [], image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r'C:\\Users\\yousf\\OneDrive\\Desktop\\University\\Graduation Project\\Codes\\photogrammetry\\src\\data\\hammer\\images\\2.jpg'\n",
    "mask_path = r'C:\\Users\\yousf\\OneDrive\\Desktop\\University\\Graduation Project\\Codes\\photogrammetry\\src\\data\\hammer\\masks\\2.jpg'\n",
    "rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "mask = OpenCV.imread(mask_path, OpenCV.IMREAD_GRAYSCALE)\n",
    "dilated_mask = OpenCV.dilate(mask, kernel, iterations=20)\n",
    "images.images.append(Image(2, rgb_image, gray_image, dilated_mask, [], [], image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_keypoints_descriptors(images: list[Image], SIFT: OpenCV.SIFT) -> None:\n",
    "    for img in images.images:\n",
    "        keypoints: list[OpenCV.KeyPoint]\n",
    "        descriptors: np.ndarray\n",
    "        dialated_image = OpenCV.bitwise_and(img.gray_image, img.gray_image, mask=img.mask)\n",
    "        keypoints, descriptors = SIFT.detectAndCompute(dialated_image, None)\n",
    "        img.keypoints = keypoints\n",
    "        img.descriptors = descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = OpenCV.SIFT_create(contrastThreshold=0.01)\n",
    "compute_keypoints_descriptors(images, sift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray,\n",
    "        checker: bool = False\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    if checker:\n",
    "        matcher = OpenCV.BFMatcher(crossCheck=True)\n",
    "        return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "    else:\n",
    "        matcher = OpenCV.BFMatcher()\n",
    "        return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "def apply_ransac(matches, keypoints1, keypoints2, threshold = 3.0):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    _, mask = OpenCV.findHomography(src_pts, dst_pts, OpenCV.RANSAC, threshold)\n",
    "    matches_mask = mask.ravel().tolist()\n",
    "    return [m for m, keep in zip(matches, matches_mask) if keep]\n",
    "\n",
    "def data_feature_matching(images: Images) -> None:\n",
    "    import itertools\n",
    "    for key, values in images.similar_images.items():\n",
    "        for image, matched_image in itertools.combinations(values, 2):\n",
    "            feature_matching_output = feature_matching(image.descriptors, matched_image.descriptors)\n",
    "            ransac_output = apply_ransac(feature_matching_output, image.keypoints, matched_image.keypoints, threshold=150)\n",
    "            images.feature_matches.append(FeatureMatches(image, matched_image, ransac_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24756\n"
     ]
    }
   ],
   "source": [
    "feature_matching_output = feature_matching(images.images[0].descriptors, images.images[1].descriptors)\n",
    "feature_matcher = FeatureMatches(images.images[0], images.images[1], feature_matching_output)\n",
    "feature_matcher.draw_matches(\"data/hammer/testing_outputs/1_2_Virgin.jpg\")\n",
    "print(len(feature_matcher.matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10340\n"
     ]
    }
   ],
   "source": [
    "feature_matching_output = feature_matching(images.images[0].descriptors, images.images[1].descriptors, True)\n",
    "feature_matcher = FeatureMatches(images.images[0], images.images[1], feature_matching_output)\n",
    "feature_matcher.draw_matches(\"data/hammer/testing_outputs/1_2_Check.jpg\")\n",
    "print(len(feature_matcher.matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7362\n"
     ]
    }
   ],
   "source": [
    "feature_matching_output = feature_matching(images.images[0].descriptors, images.images[1].descriptors, True)\n",
    "ransac_output = apply_ransac(feature_matching_output, images.images[0].keypoints, images.images[1].keypoints, threshold=150)\n",
    "feature_matcher = FeatureMatches(images.images[0], images.images[1], ransac_output)\n",
    "feature_matcher.draw_matches(\"data/hammer/testing_outputs/1_2_ransac.jpg\")\n",
    "print(len(feature_matcher.matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_matcher.simulate_matches(\"data/hammer/testing_feature_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matcher.simulate_matches(\"data/hammer/testing_feature_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_point_cloud(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    point_cloud = []\n",
    "\n",
    "    for feature_match in images.feature_matches:\n",
    "        image_one = feature_match.image_one\n",
    "        image_two = feature_match.image_two\n",
    "\n",
    "        # Extract matched keypoints\n",
    "        keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "        keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "\n",
    "        # Estimate the essential matrix\n",
    "        E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K_matrix, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n",
    "        _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K_matrix)\n",
    "\n",
    "        # Create projection matrices\n",
    "        P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "        P2 = K_matrix @ np.hstack((R, t))\n",
    "\n",
    "        # Triangulate points\n",
    "        points_4D = OpenCV.triangulatePoints(P1, P2, keypoints_one.T, keypoints_two.T)\n",
    "        points_3D = (points_4D / points_4D[3])[:3]\n",
    "\n",
    "        point_cloud.append(points_3D)\n",
    "\n",
    "    # Merge all point clouds into one\n",
    "    point_cloud = np.hstack(point_cloud).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
