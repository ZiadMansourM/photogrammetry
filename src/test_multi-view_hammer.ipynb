{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangulate Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from typing import Final, Optional\n",
    "\n",
    "import cv2 as OpenCV\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial import Delaunay\n",
    "import cv2\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, mask, keypoints, descriptors, path):\n",
    "        self.img_id: int = int(img_id)\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.mask: Image = mask\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def draw_sift_features(self):\n",
    "        image_with_sift = OpenCV.drawKeypoints(self.rgb_image, self.keypoints, None, flags=OpenCV.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        plt.imshow(image_with_sift)\n",
    "        plt.title(\"Image with SIFT Features\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_rgb_image(self, title: Optional[str] = None):\n",
    "        image = self.rgb_image\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_gray_image(self, title: Optional[str] = None):\n",
    "        image = self.gray_image\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_mask_image(self, title: Optional[str] = None):\n",
    "        image = self.mask\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        # plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_dialated_image(self, title: Optional[str] = None):\n",
    "        print(self.mask.shape)\n",
    "        print(self.rgb_image.shape)\n",
    "        image = OpenCV.bitwise_and(self.rgb_image, self.rgb_image, mask=self.mask)\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        # plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def draw_matches(self, output_filename: str) -> None:\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "        for match in self.matches:\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "        OpenCV.imwrite(output_filename, combined_image)\n",
    "        \n",
    "    def animate_matches(self, output_filename: str) -> None:\n",
    "        # for match in self.matches:\n",
    "        #     combined_image = OpenCV.hconcat([\n",
    "        #         self.image_one.rgb_image,\n",
    "        #         self.image_two.rgb_image\n",
    "        #     ])\n",
    "        #     x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "        #     x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "        #     # Write match.queryIdx at the top left corner\n",
    "        #     OpenCV.putText(\n",
    "        #         combined_image,\n",
    "        #         f\"{match.queryIdx}\",\n",
    "        #         (50, 150),  # position: 10 pixels from left, 20 pixels from top\n",
    "        #         OpenCV.FONT_HERSHEY_SIMPLEX,  # font\n",
    "        #         5,  # font scale\n",
    "        #         (0, 255, 0),  # font color (green)\n",
    "        #         5,  # thickness\n",
    "        #         OpenCV.LINE_AA  # line type\n",
    "        #     )\n",
    "        #     # Write match.trainIdx at the top right corner\n",
    "        #     image_two_width = self.image_one.rgb_image.shape[1]\n",
    "        #     OpenCV.putText(\n",
    "        #         combined_image,\n",
    "        #         f\"{match.trainIdx}\",\n",
    "        #         (image_two_width + 50, 150),  # position: 10 pixels from right, 20 pixels from top\n",
    "        #         OpenCV.FONT_HERSHEY_SIMPLEX,  # font\n",
    "        #         5,  # font scale\n",
    "        #         (0, 255, 0),  # font color (green)\n",
    "        #         5,  # thickness\n",
    "        #         OpenCV.LINE_AA  # line type\n",
    "        #     )\n",
    "        #     # Draw a line connecting the matched keypoints\n",
    "        #     OpenCV.line(\n",
    "        #         combined_image, \n",
    "        #         (int(x1), int(y1)), \n",
    "        #         (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "        #         (0, 255, 0), \n",
    "        #         1\n",
    "        #     )\n",
    "        #     OpenCV.imwrite(output_filename + f\"/{match.queryIdx}_{match.trainIdx}.jpg\", combined_image)\n",
    "        framerate = 120\n",
    "\n",
    "        # Get a list of image files in the directory\n",
    "        image_files = [f for f in os.listdir(output_filename) if f.endswith(\".jpg\")]\n",
    "        image_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
    "\n",
    "        # Create a temporary file with a list of input images\n",
    "        with open(\"input_files.txt\", \"w\") as f:\n",
    "            for image_file in image_files:\n",
    "                f.write(f\"file '{os.path.join(output_filename, image_file)}'\\n\")\n",
    "\n",
    "        # Run FFmpeg command to create a video\n",
    "        command = f'ffmpeg -y -f concat -safe 0 -i \"input_files.txt\" -framerate {framerate} -c:v libx264 -pix_fmt yuv420p \"{output_filename}/output.mp4\"'\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        # Remove temporary file\n",
    "        os.remove(\"input_files.txt\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "        self.similar_images: dict[list[Image]] = {}\n",
    "        self.num_clusters: int = 50\n",
    "\n",
    "    def save_feature_matches(self):\n",
    "        for match in self.feature_matches:\n",
    "            match.draw_matches(f\"data/{self.image_set_name}/output/feature-match/{match.image_one.img_id}_{match.image_two.img_id}.jpg\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def display_similar_images(self, key):\n",
    "        print(f\"cluster {key}\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        for value in self.similar_images[key]:\n",
    "            print(value)\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(value.path), OpenCV.COLOR_BGR2RGB)\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.title(value.path)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    def save_similar_images(self):\n",
    "        for cluster in self.similar_images.keys():\n",
    "            if not os.path.exists(f\"data/{self.image_set_name}/output/image-match/{cluster}\"):\n",
    "                os.makedirs(f\"data/{self.image_set_name}/output/image-match/{cluster}\")\n",
    "            for value in self.similar_images[cluster]:\n",
    "                OpenCV.imwrite(f\"data/{self.image_set_name}/output/image-match/{cluster}/{value.img_id}.jpg\", value.rgb_image)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Image:\n",
    "        for image in self.images:\n",
    "            if image.img_id == key:\n",
    "                return image\n",
    "        raise KeyError(f'Image with img_id {key} not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray,\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    matcher = OpenCV.BFMatcher(crossCheck=True)\n",
    "    return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "def apply_ransac(matches, keypoints1, keypoints2, threshold = 3.0):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    _, mask = OpenCV.findHomography(src_pts, dst_pts, OpenCV.RANSAC, threshold)\n",
    "    matches_mask = mask.ravel().tolist()\n",
    "    return [m for m, keep in zip(matches, matches_mask) if keep]\n",
    "\n",
    "def data_feature_matching(images: Images) -> None:\n",
    "    import itertools\n",
    "    print(images.similar_images.items(),\"\\n\")\n",
    "    for key, values in images.similar_images.items():\n",
    "        print(f\"Cluster {key}:\")\n",
    "        for image, matched_image in itertools.combinations(values, 2):\n",
    "            print(f\"Matching {image.img_id} with {matched_image.img_id}\")\n",
    "            feature_matching_output = feature_matching(image.descriptors, matched_image.descriptors)\n",
    "            ransac_output = apply_ransac(feature_matching_output, image.keypoints, matched_image.keypoints, threshold=150)\n",
    "            images.feature_matches.append(FeatureMatches(image, matched_image, ransac_output))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as OpenCV\n",
    "\n",
    "to_tuple = lambda x: tuple(x.flatten())\n",
    "\n",
    "def check_coherent_rotation(R: np.ndarray) -> bool:\n",
    "    return np.abs(np.linalg.det(R) - 1.0) <= 1e-6\n",
    "\n",
    "def find_camera_matrices(K: np.ndarray, keypoints_one: np.ndarray, keypoints_two: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n",
    "    # TODO: use mask to filter out outliers\n",
    "    _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K)\n",
    "    return (None, None) if not check_coherent_rotation(R) else (R, t)\n",
    "\n",
    "def find_3d_2d_correspondences(\n",
    "        image_two: Image,\n",
    "        feature_matches: list[FeatureMatches], \n",
    "        global_dict: dict[np.ndarray, set[tuple[int]]]\n",
    "    ) -> dict[np.ndarray, np.ndarray]:\n",
    "    local_dict: dict[np.ndarray, np.ndarray] = {}\n",
    "    for feature_match in feature_matches: # 1, 2, 3, 4 -> [(1, 2), \"(1, 3)\", (1, 4), \"(2, 3)\", (2, 4), (3, 4)]\n",
    "        if feature_match.image_two != image_two:\n",
    "            continue\n",
    "        for match in feature_match.matches:\n",
    "            search_keypoint_one = feature_match.image_one.keypoints[match.queryIdx].pt\n",
    "            search_img_id = feature_match.image_one.img_id\n",
    "            search_tuple = (search_img_id, search_keypoint_one)\n",
    "            for key, values in global_dict.items():\n",
    "                if search_tuple in values:\n",
    "                    if key not in local_dict:\n",
    "                        # print(f\"Added {key} to local_dict got it from {search_img_id}\")\n",
    "                        local_dict[key] = image_two.keypoints[match.trainIdx].pt\n",
    "    return local_dict\n",
    "\n",
    "def generate_point_cloud_general(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    camera_matrices: list[np.ndarray] = []\n",
    "    point_cloud: list[list[np.ndarray]] = []\n",
    "    global_dict: dict[np.ndarray, set[tuple[int]]] = {}\n",
    "    P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "    camera_matrices.append((np.eye(3), np.zeros((3, 1))))\n",
    "    for feature_match in images.feature_matches: # 1, 2, 3, 4 -> (1, 2) and (1, 3) and (1, 4)\n",
    "        if feature_match.image_one != images.feature_matches[0].image_one:\n",
    "            print(f\"Skipping feature match {feature_match.image_one.img_id} with {feature_match.image_two.img_id}\")\n",
    "            continue\n",
    "        image_one = feature_match.image_one\n",
    "        image_two = feature_match.image_two\n",
    "        print(f\"Triangulating pair {image_one.img_id} with {image_two.img_id}\")\n",
    "        keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "        keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "        if feature_match == images.feature_matches[0]: # No Intial Structure \"first iteration\"\n",
    "            print(\"Using recover pose\")\n",
    "            R, t = find_camera_matrices(K_matrix, keypoints_one, keypoints_two)\n",
    "            P2 = K_matrix @ np.hstack((R, t))\n",
    "            camera_matrices.append((R, t))\n",
    "        else:\n",
    "            print(\"Using solvePnPRansac\")\n",
    "            local_dict: dict[np.ndarray, np.ndarray] = find_3d_2d_correspondences(image_two, images.feature_matches, global_dict)\n",
    "            objectPoints = np.array(list(local_dict.keys())).reshape(-1, 3)\n",
    "            imagePoints = np.array(list(local_dict.values())).reshape(-1, 2)\n",
    "            _, rvec, tvec, _ = OpenCV.solvePnPRansac(objectPoints, imagePoints, K_matrix, None)\n",
    "            R, _ = OpenCV.Rodrigues(rvec)\n",
    "            P2 = K_matrix @ np.hstack((R, tvec))\n",
    "            camera_matrices.append((R, tvec))\n",
    "        points_3D = np.empty((3, len(keypoints_one)))\n",
    "        for point_counter, (keypoint_one, keypoint_two) in enumerate(zip(keypoints_one, keypoints_two)):\n",
    "            point_4D = OpenCV.triangulatePoints(P1, P2, keypoint_one.T, keypoint_two.T) # 4x1\n",
    "            point_3D = (point_4D / point_4D[3])[:3] # 3x1\n",
    "            if to_tuple(point_3D) in global_dict:\n",
    "                global_dict[to_tuple(point_3D)].add((image_one.img_id, to_tuple(keypoint_one)))\n",
    "                global_dict[to_tuple(point_3D)].add((image_two.img_id, to_tuple(keypoint_two)))\n",
    "            else:\n",
    "                global_dict[to_tuple(point_3D)] = {\n",
    "                    (image_one.img_id, to_tuple(keypoint_one)), \n",
    "                    (image_two.img_id, to_tuple(keypoint_two))\n",
    "                }\n",
    "            points_3D[:, point_counter] = point_3D.flatten()\n",
    "        print(f\"Found {points_3D.shape[1]} points in image pair {image_one.img_id} and {image_two.img_id}\")\n",
    "        point_cloud.append(points_3D)\n",
    "        points_cloud = np.hstack(point_cloud).T\n",
    "    return points_cloud, camera_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### DEBUGGING ###############################\n",
    "import numpy as np\n",
    "import cv2 as OpenCV\n",
    "\n",
    "to_tuple = lambda x: tuple(x.flatten())\n",
    "\n",
    "def check_coherent_rotation(R: np.ndarray) -> bool:\n",
    "    return np.abs(np.linalg.det(R) - 1.0) <= 1e-6\n",
    "\n",
    "def find_3D_2D_correspondences(\n",
    "        image_two: Image,\n",
    "        feature_matches: list[FeatureMatches], \n",
    "        global_dict: dict[np.ndarray, set[tuple[int]]] # (x,y,z): {(img_id, (x,y))}\n",
    "    ) -> dict[np.ndarray, np.ndarray]:\n",
    "    local_dict: dict[np.ndarray, np.ndarray] = {}\n",
    "    for feature_match in feature_matches: # 1, 2, 3, 4 -> [(1, 2), \"(1, 3)\", (1, 4), \"(2, 3)\", (2, 4), (3, 4)]\n",
    "        if feature_match.image_two != image_two:\n",
    "            continue\n",
    "        for match in feature_match.matches:\n",
    "            search_keypoint_one = feature_match.image_one.keypoints[match.queryIdx].pt\n",
    "            search_img_id = feature_match.image_one.img_id\n",
    "            search_tuple = (search_img_id, search_keypoint_one)\n",
    "            for key, values in global_dict.items():\n",
    "                if search_tuple in values:\n",
    "                    if key not in local_dict:\n",
    "                        # print(f\"Added {key} to local_dict got it from {search_img_id}\")\n",
    "                        local_dict[key] = image_two.keypoints[match.trainIdx].pt\n",
    "    # List comprehension for printing the local_dict\n",
    "    return local_dict\n",
    "\n",
    "def find_initial_camera_matrices(K: np.ndarray, keypoints_one: np.ndarray, keypoints_two: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n",
    "    # TODO: use mask to filter out outliers\n",
    "    _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K)\n",
    "    return (None, None) if not check_coherent_rotation(R) else (R, t)\n",
    "\n",
    "def find_next_camera_matrices(\n",
    "        images: Images,\n",
    "        image_one: Image,\n",
    "        image_two: Image, \n",
    "        K_matrix: np.ndarray, \n",
    "        global_dict: dict[np.ndarray, set[tuple[int]]]\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "    if image_one is not None:\n",
    "        print(f\"Using Images {image_one.img_id} and {image_two.img_id} in find_next_camera_matrices\")\n",
    "    local_dict: dict[np.ndarray, np.ndarray] = find_3D_2D_correspondences(image_two, images.feature_matches, global_dict)\n",
    "    objectPoints = np.array(list(local_dict.keys())).reshape(-1, 3)\n",
    "    imagePoints = np.array(list(local_dict.values())).reshape(-1, 2)\n",
    "    print(f\"Found {objectPoints.shape[0]} 3D Points and {imagePoints.shape[0]} Image Points 3D-2D correspondences\")\n",
    "    _, rvec, tvec, _ = OpenCV.solvePnPRansac(objectPoints, imagePoints, K_matrix, None)\n",
    "    R, _ = OpenCV.Rodrigues(rvec)\n",
    "    return R, tvec\n",
    "\n",
    "def compute_points_3D(\n",
    "        P1: np.ndarray, \n",
    "        P2: np.ndarray, \n",
    "        image_one: Image,\n",
    "        image_two: Image,\n",
    "        keypoints_one: np.ndarray,\n",
    "        keypoints_two: np.ndarray,\n",
    "        global_dict: dict[np.ndarray, set[tuple[int]]],\n",
    "    ) -> np.ndarray:\n",
    "    points_3D = np.empty((3, len(keypoints_one)))\n",
    "    for point_counter, (keypoint_one, keypoint_two) in enumerate(zip(keypoints_one, keypoints_two)):\n",
    "        point_4D = OpenCV.triangulatePoints(P1, P2, keypoint_one.T, keypoint_two.T)  # 4x1\n",
    "        point_3D = (point_4D / point_4D[3])[:3]  # 3x1\n",
    "        if to_tuple(point_3D) in global_dict:\n",
    "            global_dict[to_tuple(point_3D)].add((image_one.img_id, to_tuple(keypoint_one)))\n",
    "            global_dict[to_tuple(point_3D)].add((image_two.img_id, to_tuple(keypoint_two)))\n",
    "        else:\n",
    "            global_dict[to_tuple(point_3D)] = {\n",
    "                (image_one.img_id, to_tuple(keypoint_one)),\n",
    "                (image_two.img_id, to_tuple(keypoint_two))\n",
    "            }\n",
    "        points_3D[:, point_counter] = point_3D.flatten()\n",
    "    print(f\"Computed {points_3D.shape[1]} 3D Points for Image pairs {image_one.img_id} and {image_two.img_id}\")\n",
    "    return points_3D\n",
    "\n",
    "def generate_points_cloud_debug(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    # sourcery skip: low-code-quality\n",
    "    points_cloud: list[list[np.ndarray]] = []\n",
    "    first_iteration: bool = False\n",
    "    global_dict: dict[np.ndarray, set[tuple[int]]] = {}\n",
    "    camera_matrices: list[np.ndarray] = [(np.eye(3), np.zeros((3, 1)))]\n",
    "    for cluster, values in images.similar_images.items():\n",
    "        print(f\"--------------------- Entering Cluster {cluster} ---------------------\")\n",
    "        if cluster == list(images.similar_images.keys())[0]: # First cluster [1,2,3,4]\n",
    "            P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "            # Checks if those image pairs are in the cluster\n",
    "            for feature_match in images.feature_matches:\n",
    "                if all(\n",
    "                    img.img_id\n",
    "                    not in [\n",
    "                        feature_match.image_one.img_id,\n",
    "                        feature_match.image_two.img_id,\n",
    "                    ]\n",
    "                    for img in images.similar_images[cluster]\n",
    "                ):\n",
    "                    print(f\"Skipping Images {feature_match.image_one.img_id} and {feature_match.image_two.img_id} as they aren't in the cluster {cluster} Feature Matches\")\n",
    "                    continue\n",
    "                image_one = feature_match.image_one\n",
    "                image_two = feature_match.image_two\n",
    "                keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "                keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "                if feature_match.image_one != values[0]:\n",
    "                    print(f\"Skipping Feature Match {image_one.img_id} with {image_two.img_id}\")\n",
    "                    continue\n",
    "                if feature_match == images.feature_matches[0]:  # No Initial Structure \"first iteration\"\n",
    "                    print(f\"Using Images {image_one.img_id} and {image_two.img_id} in recoverPose\")\n",
    "                    R, t = find_initial_camera_matrices(K_matrix, keypoints_one, keypoints_two)\n",
    "                    P2 = K_matrix @ np.hstack((R, t))\n",
    "                    camera_matrices.append((R, t))\n",
    "                else:\n",
    "                    R, tvec = find_next_camera_matrices(images, image_one, image_two, K_matrix, global_dict)\n",
    "                    P2 = K_matrix @ np.hstack((R, tvec))\n",
    "                    camera_matrices.append((R, tvec))\n",
    "                points_3D = compute_points_3D(P1, P2, image_one, image_two, keypoints_one, keypoints_two, global_dict)\n",
    "                points_cloud.append(points_3D)\n",
    "                print(f\"Global Dict 3D Points Size: {len(global_dict.keys())} \\n\")\n",
    "        else:\n",
    "            # Checks if those image pairs are in the cluster\n",
    "            for feature_match in images.feature_matches:\n",
    "                if all(\n",
    "                    img.img_id\n",
    "                    not in [\n",
    "                        feature_match.image_one.img_id,\n",
    "                    ]\n",
    "                    for img in images.similar_images[cluster]\n",
    "                ):\n",
    "                    print(f\"Skipping Images {feature_match.image_one.img_id} and {feature_match.image_two.img_id} as they aren't in the cluster {cluster}\")\n",
    "                    continue\n",
    "                image_one = feature_match.image_one\n",
    "                image_two = feature_match.image_two\n",
    "                keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "                keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "                if first_iteration == False: # First Iteration of the next Clusters\n",
    "                #Computing new P1 for the new cluster\n",
    "                    print(\"Entered First Iteration of the next cluster\")\n",
    "                    print(f\"Using Image {image_one.img_id} as Reference Image in cluster {cluster} to compute P1 for cluster {cluster}\")\n",
    "                    P1_R, P1_tvec = find_next_camera_matrices(images, None, image_one, K_matrix, global_dict)\n",
    "                    P1 = K_matrix @ np.hstack((P1_R, P1_tvec))\n",
    "                    first_iteration = True\n",
    "                elif feature_match.image_one != values[0]:\n",
    "                    print(f\"Skipping feature match {image_one.img_id} with {image_two.img_id}\")\n",
    "                    continue\n",
    "                R, tvec = find_next_camera_matrices(images, image_one, image_two, K_matrix, global_dict)\n",
    "                P2 = K_matrix @ np.hstack((R, tvec))\n",
    "                camera_matrices.append((R, tvec))\n",
    "                points_3D = compute_points_3D(P1, P2, image_one, image_two, keypoints_one, keypoints_two, global_dict)\n",
    "                points_cloud.append(points_3D)\n",
    "                print(f\"Global Dict 3D Points Size: {len(global_dict.keys())} \\n\")\n",
    "        first_iteration = False\n",
    "        print(f\"--------------------- End of cluster {cluster} ---------------------\\n\\n\")\n",
    "\n",
    "    points_cloud = np.hstack(points_cloud).T\n",
    "    print(\"Done generating points cloud\")\n",
    "    return points_cloud, camera_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_images_bak(images_file_path: str, images: Images) -> None:\n",
    "    \"\"\" Dump images to a file \"\"\"\n",
    "    with open(images_file_path, \"wb\") as file:\n",
    "        pickle.dump(images, file)\n",
    "\n",
    "def load_images_bak(images_file_path: str) -> Images:\n",
    "    \"\"\" Load images from a file \"\"\"\n",
    "    with open(images_file_path, \"rb\") as file:\n",
    "        images = pickle.load(file)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_keypoints_descriptors(images: list[Image]) -> None:\n",
    "    sift = OpenCV.SIFT_create(contrastThreshold=0.01)\n",
    "    for img in images.images:\n",
    "        keypoints: list[OpenCV.KeyPoint]\n",
    "        descriptors: np.ndarray\n",
    "        dialated_image = OpenCV.bitwise_and(img.gray_image, img.gray_image, mask=img.mask)\n",
    "        keypoints, descriptors = sift.detectAndCompute(dialated_image, None)\n",
    "        img.keypoints = keypoints\n",
    "        img.descriptors = descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(create_mask = False, **kwargs) -> Images:\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    folder_path = f\"data/{image_set_name}\"\n",
    "    images: Images = Images([], folder_path.split(\"/\")[-1])\n",
    "    files: list[str] = list(\n",
    "        filter(\n",
    "            lambda file: \".jpg\" in file, os.listdir(f\"{folder_path}/images\")\n",
    "        )\n",
    "    )\n",
    "    if create_mask:\n",
    "        from rembg import remove\n",
    "        for file in files:\n",
    "            image_path = f\"{folder_path}/images/{file}\"\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "            gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask = remove(rgb_image)\n",
    "            mask = OpenCV.cvtColor(mask, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask[mask > 0] = 255\n",
    "            OpenCV.imwrite(f\"{folder_path}/masks/{file}\", mask)\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            dilated_mask = OpenCV.dilate(mask, kernel, iterations=20)\n",
    "            images.images.append(Image(file.split(\".\")[0], rgb_image, gray_image, dilated_mask, [], [], image_path))\n",
    "    else:\n",
    "        for file in files:\n",
    "            image_path = f\"{folder_path}/images/{file}\"\n",
    "            mask_path = f\"{folder_path}/masks/{file}\"\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "            gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask = OpenCV.imread(mask_path, OpenCV.IMREAD_GRAYSCALE)\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            dilated_mask = OpenCV.dilate(mask, kernel, iterations=20)\n",
    "            images.images.append(Image(file.split(\".\")[0], rgb_image, gray_image, dilated_mask, [], [], image_path))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set_name = \"hammer\"\n",
    "create_mask = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images: Optional[Images] = prepare_images(create_mask=create_mask, image_set_name=image_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images.images[:5]:\n",
    "    image.display_dialated_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_keypoints_descriptors(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[2].draw_sift_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[3].draw_sift_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.similar_images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.similar_images = {\n",
    "    \"0\": [images[1], images[2], images[3], images[4], images[5]],\n",
    "    \"1\": [images[5], images[6], images[7]],\n",
    "    \"2\": [images[7], images[8], images[9], images[10], images[11], images[12]],\n",
    "    \"3\": [images[12], images[13], images[14], images[15]],\n",
    "    \"4\": [images[15], images[16], images[17], images[18]],\n",
    "    \"5\": [images[18], images[19], images[20]],\n",
    "    \"6\": [images[20], images[21], images[22], images[23]],\n",
    "    \"7\": [images[23], images[24], images[25]],\n",
    "    \"8\": [images[25], images[26], images[27], images[28]],\n",
    "    \"9\": [images[28], images[29], images[30], images[31]],\n",
    "    \"10\": [images[31], images[32], images[33]],\n",
    "    \"11\": [images[33], images[34], images[35]],\n",
    "    \"12\": [images[35], images[36], images[37], images[38]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(images\u001b[39m.\u001b[39msimilar_images\u001b[39m.\u001b[39mitems())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "print(images.similar_images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(any(img.img_id in [5, 1] for img in images.similar_images[\"0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature_matching(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in images.feature_matches:\n",
    "    match.draw_matches(f\"data/{image_set_name}/output/triangulate/{match.image_one.img_id}_{match.image_two.img_id}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of matches for each image pair\n",
    "for match in images.feature_matches:\n",
    "    print(f\"image_one: {match.image_one.img_id}, image_two: {match.image_two.img_id}, matches: {len(match.matches):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.feature_matches[0].animate_matches(f\"data/{image_set_name}/output/triangulate/2_3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hammer's K-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hammer K matrix\n",
    "def compute_k_matrix(img_path: str, **kwargs) -> np.ndarray:\n",
    "    import numpy as np\n",
    "    focal_length: float = 7600\n",
    "    principal_point_x = 2736\n",
    "    principal_point_y = 1824\n",
    "    scaling_factor: float = 1.0\n",
    "    return np.array(\n",
    "        [\n",
    "            [focal_length, 0, principal_point_x],\n",
    "            [0, focal_length, principal_point_y],\n",
    "            [0, 0, scaling_factor],\n",
    "        ]\n",
    "    )\n",
    "with open(f\"data/{image_set_name}/bak/K_matrix.pickle\", 'wb') as f:\n",
    "    pickle.dump(compute_k_matrix(f\"data/{image_set_name}/images\"), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"data/{image_set_name}/bak/K_matrix.pickle\", 'rb') as f:\n",
    "        K_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    row = [K_matrix[i][j] for j in range(3)]\n",
    "    print(*(f\"{value:.1f},\" for value in row))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangulation and HDBScan Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(f\"data/{image_set_name}/bak/point-cloud.pkl\"):\n",
    "    with open(f\"data/{image_set_name}/bak/point-cloud.pkl\", 'rb') as f:\n",
    "        points_cloud: np.ndarray = pickle.load(f)\n",
    "    with open(f\"data/{image_set_name}/bak/camera-matrices.pkl\", 'rb') as f:\n",
    "        camera_matrices: np.ndarray = pickle.load(f)\n",
    "else:\n",
    "    points_cloud, camera_matrices = generate_points_cloud_debug(images, K_matrix)\n",
    "    with open(f\"data/{image_set_name}/bak/point-cloud.pkl\", 'wb') as f:\n",
    "            pickle.dump(points_cloud, f)\n",
    "    with open(f\"data/{image_set_name}/bak/camera-matrices.pkl\", 'wb') as f:\n",
    "            pickle.dump(camera_matrices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "hdbscan_model = hdbscan.HDBSCAN().fit(points_cloud)\n",
    "labels = hdbscan_model.labels_\n",
    "core_indices = np.where(labels != -1)[0]\n",
    "core_points_HDBSCAN = points_cloud[core_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(points_cloud.shape)\n",
    "print(core_points_HDBSCAN.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cameras TriangleMesh Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "def create_camera_frustum(P: np.ndarray, scale: float) -> o3d.geometry.TriangleMesh:\n",
    "    vertices = np.array([[0.5, 0.5, 0], [0.5, -0.5, 0], [-0.5, -0.5, 0], [-0.5, 0.5, 0], [0, 0, -1]])\n",
    "    vertices *= scale\n",
    "    faces = np.array([[0, 1, 4], [1, 2, 4], [2, 3, 4], [3, 0, 4], [1, 0, 3]])\n",
    "    R, t = P\n",
    "    vertices = vertices @ R.T + t[:3].T\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(vertices)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    vertex_colors = np.ones((len(vertices), 3)) * [1, 0, 0]\n",
    "    mesh.vertex_colors = o3d.utility.Vector3dVector(vertex_colors)\n",
    "    # draw camera rod\n",
    "    start_point = np.array([0, 0, 0])\n",
    "    end_point = np.array([0, 0, 1])*scale\n",
    "    start_point = start_point @ R.T + t[:3].T\n",
    "    end_point = end_point @ R.T + t[:3].T\n",
    "    rod = o3d.geometry.TriangleMesh.create_cylinder(radius=0.02*scale, height=np.linalg.norm(end_point-start_point), resolution=20, split=4)\n",
    "    rod.vertices = o3d.utility.Vector3dVector(np.asarray(rod.vertices) + start_point)\n",
    "    vertex_colors = np.ones((len(rod.vertices), 3)) * [0, 0, 0]\n",
    "    rod.vertex_colors = o3d.utility.Vector3dVector(vertex_colors)\n",
    "    return mesh, rod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Create a point cloud object\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(core_points_HDBSCAN)\n",
    "point_cloud.paint_uniform_color([0, 0, 1])  # Set the point cloud color to blue for better visibility\n",
    "\n",
    "# Loop through the camera_matrices and create a red pyramid for each camera\n",
    "camera_meshes = []\n",
    "camera_lines = []\n",
    "for camera_matrix in camera_matrices:\n",
    "    camera_mesh, camera_line = create_camera_frustum(camera_matrix, scale=0.3)\n",
    "    camera_meshes.append(camera_mesh)\n",
    "    camera_lines.append(camera_line)\n",
    "\n",
    "# Combine camera meshes, camera lines, and point cloud into a single mesh\n",
    "combined_mesh = o3d.geometry.TriangleMesh()\n",
    "for mesh in camera_meshes + camera_lines:\n",
    "    combined_mesh += mesh\n",
    "\n",
    "# Save the point cloud to a .ply file\n",
    "point_cloud_file = f\"data/{image_set_name}/output/triangulate/core_points_cloud_HDBScan.ply\"\n",
    "o3d.io.write_point_cloud(point_cloud_file, point_cloud)\n",
    "\n",
    "# Save the combined mesh to a .ply file\n",
    "mesh_file = f\"data/{image_set_name}/output/triangulate/camera_proj_HDBScan.ply\"\n",
    "o3d.io.write_triangle_mesh(mesh_file, combined_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Create a point cloud object\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(points_cloud)\n",
    "point_cloud.paint_uniform_color([0, 0, 1])  # Set the point cloud color to blue for better visibility\n",
    "\n",
    "# Loop through the camera_matrices and create a red pyramid for each camera\n",
    "camera_meshes = []\n",
    "camera_lines = []\n",
    "for camera_matrix in camera_matrices:\n",
    "    camera_mesh, camera_line = create_camera_frustum(camera_matrix, scale=0.3)\n",
    "    camera_meshes.append(camera_mesh)\n",
    "    camera_lines.append(camera_line)\n",
    "\n",
    "# Combine camera meshes, camera lines, and point cloud into a single mesh\n",
    "combined_mesh = o3d.geometry.TriangleMesh()\n",
    "for mesh in camera_meshes + camera_lines:\n",
    "    combined_mesh += mesh\n",
    "\n",
    "# Save the point cloud to a .ply file\n",
    "point_cloud_file = f\"data/{image_set_name}/output/triangulate/points_cloud.ply\"\n",
    "o3d.io.write_point_cloud(point_cloud_file, point_cloud)\n",
    "\n",
    "# Save the combined mesh to a .ply file\n",
    "mesh_file = f\"data/{image_set_name}/output/triangulate/camera_proj.ply\"\n",
    "o3d.io.write_triangle_mesh(mesh_file, combined_mesh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Cameras and Points Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "point_cloud_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/points_cloud.ply\"\n",
    ")\n",
    "mesh_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/camera_proj.ply\"\n",
    ")\n",
    "\n",
    "point_cloud = o3d.io.read_point_cloud(point_cloud_file_path)\n",
    "mesh = o3d.io.read_triangle_mesh(mesh_file_path)\n",
    "\n",
    "o3d.visualization.draw_geometries([point_cloud, mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "point_cloud_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/core_points_cloud_HDBScan.ply\"\n",
    ")\n",
    "mesh_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/camera_proj_HDBScan.ply\"\n",
    ")\n",
    "\n",
    "point_cloud = o3d.io.read_point_cloud(point_cloud_file_path)\n",
    "mesh = o3d.io.read_triangle_mesh(mesh_file_path)\n",
    "\n",
    "o3d.visualization.draw_geometries([point_cloud, mesh])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
