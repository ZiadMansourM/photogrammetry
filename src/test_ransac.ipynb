{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from typing import Final, Optional\n",
    "\n",
    "import cv2 as OpenCV\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial import Delaunay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, keypoints, descriptors, path):\n",
    "        self.img_id: int = int(img_id)\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def draw_sift_features(self):\n",
    "        image_with_sift = OpenCV.drawKeypoints(self.rgb_image, self.keypoints, None, flags=OpenCV.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        plt.imshow(image_with_sift)\n",
    "        plt.title(\"Image with SIFT Features\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_rgb_image(self, title: Optional[str] = None):\n",
    "        image = self.rgb_image\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_gray_image(self, title: Optional[str] = None):\n",
    "        image = self.gray_image\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axes('off')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def draw_matches(self, output_filename: str) -> None:\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "\n",
    "        for match in self.matches:\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "\n",
    "        OpenCV.imwrite(output_filename, combined_image)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "        self.similar_images: dict[list[Image]] = {}\n",
    "        self.num_clusters: int = 50\n",
    "\n",
    "    def save_feature_matches(self):\n",
    "        for match in self.feature_matches:\n",
    "            match.draw_matches(f\"data/snow-man/output/feature-match/{match.image_one.img_id}_{match.image_two.img_id}.jpg\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def display_similar_images(self, key):\n",
    "        print(f\"cluster {key}\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        for value in self.similar_images[key]:\n",
    "            print(value)\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(value.path), OpenCV.COLOR_BGR2RGB)\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.title(value.path)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    def save_similar_images(self):\n",
    "        for cluster in self.similar_images.keys():\n",
    "            if not os.path.exists(f\"data/{self.image_set_name}/output/image-match/{cluster}\"):\n",
    "                os.makedirs(f\"data/{self.image_set_name}/output/image-match/{cluster}\")\n",
    "            for value in self.similar_images[cluster]:\n",
    "                OpenCV.imwrite(f\"data/{self.image_set_name}/output/image-match/{cluster}/{value.img_id}.jpg\", value.rgb_image)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Image:\n",
    "        for image in self.images:\n",
    "            if image.img_id == key:\n",
    "                return image\n",
    "        raise KeyError(f'Image with img_id {key} not found.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray,\n",
    "        knn: bool = False,\n",
    "        **kwargs\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    if knn:\n",
    "        matcher = OpenCV.BFMatcher()\n",
    "        return matcher.knnMatch(img_one_descriptors, img_two_descriptors, k=1)\n",
    "    else:\n",
    "        matcher = OpenCV.BFMatcher(crossCheck=True)\n",
    "        return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "\n",
    "def apply_ransac(matches, keypoints1, keypoints2, threshold = 3.0, **kwargs):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    _, mask = OpenCV.findHomography(src_pts, dst_pts, OpenCV.RANSAC, threshold)\n",
    "    matches_mask = mask.ravel().tolist()\n",
    "    return [m for m, keep in zip(matches, matches_mask) if keep]\n",
    "\n",
    "def data_feature_matching(images: Images) -> None:\n",
    "    import itertools\n",
    "    for key, values in images.similar_images.items():\n",
    "        for image, matched_image in itertools.combinations(values, 2):\n",
    "            feature_matching_output = feature_matching(image.descriptors, matched_image.descriptors)\n",
    "            ransac_output = apply_ransac(feature_matching_output, image.keypoints, matched_image.keypoints, threshold=150)\n",
    "            images.feature_matches.append(FeatureMatches(image, matched_image, ransac_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_bak(images_file_path: str) -> Images:\n",
    "    \"\"\" Load images from a file \"\"\"\n",
    "    with open(images_file_path, \"rb\") as file:\n",
    "        images = pickle.load(file)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_bak(f\"data/snow-man/bak/feature-matching-output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_top_k_matches(matches, k):\n",
    "    sorted_matches = sorted(matches, key=lambda x: x.distance)\n",
    "    return sorted_matches[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4624, 2604, 3) (4624, 2604, 3)\n",
      "7557\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "image_1 = images[34]\n",
    "image_2 = images[35]\n",
    "\n",
    "print(image_1.rgb_image.shape, image_2.rgb_image.shape)\n",
    "\n",
    "feature_matching_output = feature_matching(image_1.descriptors, image_2.descriptors)\n",
    "print(len(feature_matching_output))\n",
    "matches_before = FeatureMatches(image_1, image_2, feature_matching_output)\n",
    "matches_before.draw_matches(\"test_feature_match.jpg\")\n",
    "\n",
    "# knn_feature_matching_output = feature_matching(image_1.descriptors, image_2.descriptors, True)\n",
    "# print(len(knn_feature_matching_output))\n",
    "# matches_before = FeatureMatches(image_1, image_2, knn_feature_matching_output)\n",
    "# matches_before.draw_matches(\"test_knn_feature_match.jpg\")\n",
    "\n",
    "ransac_output = apply_ransac(feature_matching_output, image_1.keypoints, image_2.keypoints, threshold=50)\n",
    "matches = FeatureMatches(image_1, image_2, ransac_output)\n",
    "matches.draw_matches(\"test_feature_match_ransac.jpg\")\n",
    "print(len(ransac_output))\n",
    "\n",
    "# ransac_output = apply_ransac(knn_feature_matching_output, image_1.keypoints, image_2.keypoints, threshold=150)\n",
    "# matches = FeatureMatches(image_1, image_2, ransac_output)\n",
    "# matches.draw_matches(\"test_knn_feature_match_ransac.jpg\")\n",
    "# print(len(ransac_output))\n",
    "\n",
    "\n",
    "# feature_matching_output = keep_top_k_matches(feature_matching_output, 1000)\n",
    "# matches_after = FeatureMatches(image_1, image_2, feature_matching_output)\n",
    "# matches_after.draw_matches(\"test_feature_match_sort.jpg\")\n",
    "# print(len(feature_matching_output))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def compute_deep_features(images: list[Image], **kwargs) -> None:\n",
    "    \"\"\"Compute deep features for each image in the list of images using ResNet50 model.\n",
    "    Modifies each image in the list of images by adding its deep features as an attribute.\n",
    "    \n",
    "    Args:\n",
    "    - images: List of images to compute deep features for.\n",
    "\n",
    "    Returns:\n",
    "    - None.\n",
    "    \"\"\"\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    for img in images.images:\n",
    "        img_data = keras_image.img_to_array(img.color_image)\n",
    "        img_data = np.expand_dims(img_data, axis=0)\n",
    "        img_data = preprocess_input(img_data)\n",
    "\n",
    "        deep_features = model.predict(img_data)\n",
    "        img.deep_features = deep_features\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Img({img.img_id}, {img.path}) has deep features of shape {img.deep_features.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "@timeit\n",
    "def data_feature_matching(images: Images, similarity_threshold: float = 0.8, **kwargs) -> None:\n",
    "    \"\"\" Match features between images using cosine similarity\n",
    "    Args:\n",
    "        images: Images object containing images with their deep features\n",
    "        similarity_threshold: Threshold to consider two images as matching (default: 0.8)\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    for key, values in images.similar_images.items():\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Started Feature Match for cluster number {key}:\")\n",
    "        for image, matched_image in itertools.combinations(values, 2):\n",
    "            similarity = cosine_similarity(image.deep_features, matched_image.deep_features)[0][0]\n",
    "            \n",
    "            if similarity > similarity_threshold:\n",
    "                images.feature_matches.append(FeatureMatches(image, matched_image, similarity))\n",
    "                log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"({image.img_id}, {matched_image.img_id}) with similarity {similarity}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
