{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from typing import Final, Optional\n",
    "\n",
    "import cv2 as OpenCV\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial import Delaunay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils Functions / Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_file(file_name: str, message: str):\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_message = f\"[{timestamp}] {message}\"\n",
    "    with open(file_name, \"a\") as f:\n",
    "        f.write(f\"{log_message}\\n\")\n",
    "\n",
    "\n",
    "def print_size(obj, obj_name=\"N/A\"):\n",
    "    from pympler import asizeof\n",
    "    memory_usage = asizeof.asizeof(obj)\n",
    "    # Convert memory usage to a more readable format\n",
    "    if memory_usage < 1024:\n",
    "        memory_usage_str = f\"{memory_usage} bytes\"\n",
    "    elif memory_usage < 1024 ** 2:\n",
    "        memory_usage_str = f\"{memory_usage / 1024} KB\"\n",
    "    elif memory_usage < 1024 ** 3:\n",
    "        memory_usage_str = f\"{memory_usage / (1024 ** 2)} MB\"\n",
    "    else:\n",
    "        memory_usage_str = f\"{memory_usage / (1024 ** 3)} GB\"\n",
    "    # Print the memory usage and object name\n",
    "    log_to_file(\"logs/tune.log\", f\"Memory usage of {obj_name}: {memory_usage_str}\")\n",
    "\n",
    "\n",
    "def mem_usage():\n",
    "    import psutil\n",
    "    # Get the current process ID\n",
    "    pid = psutil.Process()\n",
    "    # Get the memory usage in bytes\n",
    "    memory_usage = pid.memory_info().rss\n",
    "    # Convert memory usage to gigabytes\n",
    "    memory_usage_gb = memory_usage / (1024 ** 3)\n",
    "    # Print the memory usage in gigabytes\n",
    "    print(f\"Memory usage: {memory_usage_gb:.2f} GB\")\n",
    "\n",
    "def timeit(func):\n",
    "    from functools import wraps\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        log_to_file(\"logs/tune.log\", f\"Started {func.__name__}...\")\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        log_to_file(\"logs/tune.log\", f\"Done {func.__name__} took {end_time - start_time:,} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalibrationError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "class IntrinsicParametersNotFoundError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, keypoints, descriptors, path):\n",
    "        self.img_id: int = img_id\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "        self.similar_images: list[tuple[Image, float]] = []\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def draw_sift_features(self):\n",
    "        image_with_sift = OpenCV.drawKeypoints(self.rgb_image, self.keypoints, None, flags=OpenCV.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        plt.imshow(image_with_sift)\n",
    "        plt.title(\"Image with SIFT Features\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_rgb_image(self, title: Optional[str] = None):\n",
    "        image = self.rgb_image\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_gray_image(self, title: Optional[str] = None):\n",
    "        image = self.gray_image\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axes('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_similar_images(self):\n",
    "        print(f\"Ref image: Img({self.img_id}, {self.path})\")\n",
    "        self.display_rgb_image()\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        for sim_img, perc in self.similar_images:\n",
    "            print(f\"[{sim_img.img_id}, {sim_img.path}], image percentage {perc}, {sim_img.path}\")\n",
    "            sim_img.display_rgb_image()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def draw_matches(self, output_filename: str) -> None:\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "\n",
    "        for match in self.matches:\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "\n",
    "        OpenCV.imwrite(output_filename, combined_image)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step One: Read and Load Images\n",
    "Inputs: \n",
    "- folder_path: str\n",
    "\n",
    "Outputs:\n",
    "- images: Images\n",
    "\n",
    "Main Functions:\n",
    "1. prepare_images: read and load images from a folder into an Images object\n",
    "\n",
    "Utils Functions:\n",
    "1. dump_images: dump images to a pickle file\n",
    "2. load_images: load images from a pickle file\n",
    "\"\"\"\n",
    "\n",
    "def prepare_images(folder_path: str) -> Images:\n",
    "    \"\"\" Read and load images \"\"\"\n",
    "    images: Images = Images([], folder_path.split(\"/\")[-1])\n",
    "    files: list[str] = filter(lambda file: \".jpg\" in file, os.listdir(folder_path))\n",
    "    for i, file in enumerate(files):\n",
    "        image_path = f\"{folder_path}/{file}\"\n",
    "        rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "        gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "        images.images.append(Image(i, rgb_image, gray_image, [], [], image_path))\n",
    "    return images\n",
    "\n",
    "def dump_images_bak(images_file_path: str, images: Images) -> None:\n",
    "    \"\"\" Dump images to a file \"\"\"\n",
    "    with open(images_file_path, \"wb\") as file:\n",
    "        pickle.dump(images, file)\n",
    "\n",
    "def load_images_bak(images_file_path: str) -> Images:\n",
    "    \"\"\" Load images from a file \"\"\"\n",
    "    with open(images_file_path, \"rb\") as file:\n",
    "        images = pickle.load(file)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Two: Feature Extraction\n",
    "Inputs:\n",
    "- images: Images\n",
    "- SIFT: OpenCV.SIFT\n",
    "\n",
    "Outputs:\n",
    "- image: Image\n",
    "--> image.keypoints: list[OpenCV.KeyPoint]\n",
    "--> image.descriptors: np.ndarray\n",
    "\n",
    "Main Functions:\n",
    "1. compute_keypoints_descriptors\n",
    "\"\"\"\n",
    "\n",
    "@timeit\n",
    "def compute_keypoints_descriptors(images: list[Image], SIFT: OpenCV.SIFT) -> None:\n",
    "    \"\"\"Compute keypoints and descriptors for each image in the list of images using SIFT algorithm.\n",
    "    Modifies each image in the list of images by adding its keypoints and descriptors as attributes.\n",
    "    \n",
    "    Args:\n",
    "    - images: List of images to compute keypoints and descriptors for.\n",
    "    - SIFT: OpenCV SIFT object used to detect and compute keypoints and descriptors.\n",
    "\n",
    "    Returns:\n",
    "    - None.\n",
    "    \"\"\"\n",
    "    for img in images.images:\n",
    "        keypoints: list[OpenCV.KeyPoint]\n",
    "        descriptors: np.ndarray\n",
    "        keypoints, descriptors = SIFT.detectAndCompute(img.gray_image, None)\n",
    "        img.keypoints = keypoints\n",
    "        img.descriptors = descriptors\n",
    "        log_to_file(\"logs/tune.log\", f\"Img({img.img_id}, {img.path}) has {len(img.keypoints)} keypoints and {len(img.descriptors)} descriptors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step Three: Image Matching\n",
    "Inputs:\n",
    "- descriptors: list[np.ndarray]\n",
    "\n",
    "Outputs:\n",
    "- centroids: np.ndarray\n",
    "- variance: np.ndarray\n",
    "- CLUSTER_COUNT: int\n",
    "- matches_ids: list[list[tuple[int, float]]]\n",
    "\n",
    "Main Functions:\n",
    "1. get_matches\n",
    "\n",
    "Utils Functions:\n",
    "1. get_matches_ids\n",
    "\n",
    "Sub Utils Functions:\n",
    "1. get_visual_words\n",
    "2. get_frequency_vectors\n",
    "3. get_tf_idf\n",
    "4. search_matches\n",
    "\"\"\"\n",
    "\n",
    "@timeit\n",
    "def get_matches(images: Images) -> None:\n",
    "    \"\"\" Match images using k-means clustering.\n",
    "    Args:\n",
    "        images: Obj from Images class.\n",
    "    \"\"\"\n",
    "    all_descriptors = np.concatenate([image.descriptors for image in images.images])\n",
    "    CLUSTER_COUNT: Final[int] = 400\n",
    "    ITER: Final[int] = 2\n",
    "    centroids, _ = kmeans(all_descriptors, CLUSTER_COUNT, ITER)\n",
    "    matches_ids: list[list[tuple[int, float]]] =  get_matches_ids(\n",
    "        [image.descriptors for image in images.images], \n",
    "        centroids, \n",
    "        images.images\n",
    "    )\n",
    "    for i, image in enumerate(images.images):\n",
    "        inner_list: list[Image, float] = [\n",
    "            (images.images[match[0]], match[1]) for match in matches_ids[i]\n",
    "        ]\n",
    "        image.similar_images = inner_list\n",
    "        log_to_file(\"logs/tune.log\", f\"Img({image.img_id}, {image.path}) with similar images:\")\n",
    "        log_message = ' - '.join(f\"({img.img_id}, {perc})\" for img, perc in inner_list)\n",
    "        log_to_file(\"logs/tune.log\", log_message)\n",
    "\n",
    "@timeit\n",
    "def get_visual_words(descriptors: list[np.ndarray], centroids: np.ndarray) -> list[np.ndarray]:\n",
    "    \"\"\" Get the visual words of a list of descriptors.\n",
    "    Args:\n",
    "        descriptors: A list of numpy arrays containing image descriptors.\n",
    "        centroids: A numpy array containing cluster centroids.\n",
    "    Returns:\n",
    "        A list of numpy arrays representing the visual words of each image.\n",
    "    \"\"\"\n",
    "    visual_words = []\n",
    "    for descriptor in descriptors:\n",
    "        words, _ = vq(descriptor, centroids)\n",
    "        visual_words.append(words)\n",
    "    return visual_words\n",
    "\n",
    "@timeit\n",
    "def get_frequency_vectors(visual_words: list[np.ndarray], CLUSTER_COUNT: int) -> np.ndarray:\n",
    "    \"\"\" Get the frequency vectors for a list of visual words.\n",
    "    Args:\n",
    "        visual_words: A list of numpy arrays representing the visual words of each image.\n",
    "        CLUSTER_COUNT: The number of clusters used to generate the visual words.\n",
    "    Returns:\n",
    "        A numpy array containing the frequency vectors for each image.\n",
    "    \"\"\"\n",
    "    frequency_vectors = []\n",
    "    for img_words in visual_words:\n",
    "        histogram = np.zeros(CLUSTER_COUNT)\n",
    "        for word in img_words:\n",
    "            histogram[word] += 1\n",
    "        frequency_vectors.append(histogram)\n",
    "    return np.stack(frequency_vectors)\n",
    "\n",
    "@timeit\n",
    "def get_tf_idf(frequency_vectors, IMAGES_COUNT) -> np.ndarray:\n",
    "    \"\"\" Get the Term Frequency-Inverse Document Frequency (TF-IDF) matrix for a list of frequency vectors.\n",
    "    Args:\n",
    "        frequency_vectors: A numpy array containing the frequency vectors for each image.\n",
    "        IMAGES_COUNT: The total number of images in the dataset.\n",
    "    Returns:\n",
    "        A numpy array containing the TF-IDF matrix for the input frequency vectors.\n",
    "    \"\"\"\n",
    "    df = np.sum(frequency_vectors > 0, axis = 0)\n",
    "    idf = np.log(IMAGES_COUNT/df)\n",
    "    return frequency_vectors * idf\n",
    "\n",
    "\n",
    "def search_matches(i, top_clusters, tf_idf) -> list[tuple[int, float]]:\n",
    "    \"\"\" Search for the top_clusters most similar images to the i-th image\n",
    "    Args:\n",
    "        i: the index of the image to search for similar images\n",
    "        top_clusters: the number of similar images to return\n",
    "        tf_idf: Term Frequency-Inverse Document Frequency\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains the index of a similar image and the cosine similarity \n",
    "        between the i-th image and the similar image. The list is sorted by the cosine similarity in \n",
    "        descending order.\n",
    "    \"\"\"\n",
    "    b = tf_idf\n",
    "    a = tf_idf[i]\n",
    "    b_subset = b[:tf_idf.shape[0]]\n",
    "    cosine_similarity = np.dot(a, b_subset.T)/(norm(a) * norm(b_subset, axis=1))\n",
    "    idx = np.argsort(-cosine_similarity)[:top_clusters]\n",
    "    return list(zip(idx, cosine_similarity[idx]))\n",
    "\n",
    "@timeit\n",
    "def get_matches_ids(descriptors, centroids, images_list) -> list[list[tuple[int, float]]]:\n",
    "    \"\"\"Returns: a list of lists, where each list contains the top 10 most similar images to the i-th image.\"\"\"\n",
    "    visual_words = get_visual_words(descriptors, centroids)\n",
    "    frequency_vectors = get_frequency_vectors(visual_words, centroids.shape[0])\n",
    "    \"\"\" tf_idf: Term Frequency-Inverse Document Frequency \"\"\"\n",
    "    tf_idf = get_tf_idf(frequency_vectors, len(images_list))\n",
    "    return [\n",
    "        search_matches(i, 10, tf_idf)\n",
    "        for i in range(len(images_list))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Four: Feature Matching\n",
    "Inputs:\n",
    "- images: Images\n",
    "\n",
    "Outputs:\n",
    "- None\n",
    "\n",
    "Main Functions:\n",
    "1. data_feature_matching\n",
    "\n",
    "Utils Functions:\n",
    "1. feature_matching\n",
    "\"\"\"\n",
    "\n",
    "@timeit\n",
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    \"\"\" Match features between two images using Brute Force Matcher\n",
    "    Args:\n",
    "        img_id_one: the index of the first image\n",
    "        img_id_two: the index of the second image\n",
    "        descriptors: a list of descriptors of the images\n",
    "    Returns:\n",
    "        A list of OpenCV.DMatch objects.\n",
    "    \"\"\"\n",
    "    matcher = OpenCV.BFMatcher()\n",
    "    return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "@timeit\n",
    "def apply_ransac(matches, keypoints1, keypoints2, threshold = 3.0):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    _, mask = OpenCV.findHomography(src_pts, dst_pts, OpenCV.RANSAC, threshold)\n",
    "    matches_mask = mask.ravel().tolist()\n",
    "    return [m for m, keep in zip(matches, matches_mask) if keep]\n",
    "\n",
    "@timeit\n",
    "def data_feature_matching(images: Images) -> None:\n",
    "    \"\"\" Match features between images using Brute Force Matcher\n",
    "    Args:\n",
    "        matchesIDs: a list of lists of tuples, where each tuple contains the index of a similar image and the cosine similarity \n",
    "            between the i-th image and the similar image. The list is sorted by the cosine similarity in \n",
    "            descending order.\n",
    "        descriptors: a list of descriptors of the images\n",
    "    Returns:\n",
    "        A list of lists, where each list contains \n",
    "        the index of the first image, the index of the second image, \n",
    "        and a list of OpenCV.DMatch objects.\n",
    "    \"\"\"\n",
    "    num_images: int = len(images.images)\n",
    "    checked = np.zeros((num_images, num_images), dtype=int)\n",
    "    for image in images.images:\n",
    "        log_to_file(\"logs/tune.log\", f\"Started Feature Match for Img({image.img_id}, {image.path}):\")\n",
    "        for matched_image, probability in image.similar_images:\n",
    "            if ((checked[image.img_id][matched_image.img_id] == 0 or checked[matched_image.img_id][image.img_id] == 0) and image.img_id != matched_image.img_id and probability > 0.93):\n",
    "                feature_matching_output = feature_matching(image.descriptors, matched_image.descriptors)\n",
    "                ransac_output = apply_ransac(feature_matching_output, image.keypoints, matched_image.keypoints)\n",
    "                images.feature_matches.append(FeatureMatches(image, matched_image, ransac_output))\n",
    "                log_to_file(\"logs/tune.log\", f\"({image.img_id}, {matched_image.img_id}) with {len(feature_matching_output)}.\")\n",
    "                checked[image.img_id][matched_image.img_id], checked[matched_image.img_id][image.img_id] = 1, 1\n",
    "    # Apply ransic algorithm to remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Five: Camera Calibration\n",
    "Inputs:\n",
    "- None.\n",
    "\n",
    "Outputs:\n",
    "- k_matrix: np.ndarray\n",
    "\n",
    "ToDo:\n",
    "1- generate K_matrix.pickle for each camera using Chess board pattern.\n",
    "\"\"\"\n",
    "\n",
    "def compute_k_matrix(img_path: str) -> np.ndarray:\n",
    "    import exifread\n",
    "\n",
    "    # Open the image file\n",
    "    image = open(img_path, \"rb\")\n",
    "    # Read the EXIF data\n",
    "    exif = exifread.process_file(image)\n",
    "    # Debug\n",
    "    # print(exif.keys())\n",
    "    # Extract the intrinsic parameters\n",
    "    focal_length = exif['EXIF FocalLength'].values[0]\n",
    "    sensor_width = exif['EXIF ExifImageWidth'].values[0]\n",
    "    sensor_height = exif['EXIF ExifImageLength'].values[0]\n",
    "    principal_point_x = exif['EXIF ExifImageWidth'].values[0] / 2\n",
    "    principal_point_y = exif['EXIF ExifImageLength'].values[0] / 2\n",
    "    # distortion_coefficients = exif['EXIF MakerNote'].values[0]\n",
    "    # Calculate the scaling factor for the K-matrix\n",
    "    scaling_factor = 1.0\n",
    "    # Create the K-matrix\n",
    "    K = np.array([[float(focal_length), 0, principal_point_x],\n",
    "                [0, float(focal_length), principal_point_y],\n",
    "                [0, 0, scaling_factor]])\n",
    "    # Print the intrinsic parameters\n",
    "    print(\"Intrinsic parameters:\")\n",
    "    print('Focal Length:', focal_length)\n",
    "    print('Sensor Width:', sensor_width)\n",
    "    print('Sensor Height:', sensor_height)\n",
    "    print('Principal Point (X):', principal_point_x)\n",
    "    print('Principal Point (Y):', principal_point_y)\n",
    "    # print('Distortion Coefficients:', distortion_coefficients)\n",
    "    # Print the K-matrix\n",
    "    print(\"K-matrix:\")\n",
    "    print(K)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Six: Triangulation (3D Reconstruction)\n",
    "Inputs:\n",
    "- feature_matches_list: list[list[int, int, list[OpenCV.DMatch]]]\n",
    "    -> A list of lists, where each list contains \n",
    "        the index of the first image, the index of the second image, \n",
    "        and a list of OpenCV.DMatch objects.\n",
    "- K_matrix: np.ndarray\n",
    "    -> The camera matrix of the camera used to take the images.\n",
    "\n",
    "Outputs:\n",
    "- point_cloud: list[np.ndarray]; each element is a 3D point.\n",
    "\n",
    "Main Functions:\n",
    "1. generate_point_cloud\n",
    "\n",
    "Utils Functions:\n",
    "1. triangulatePoints\n",
    "\"\"\"\n",
    "\n",
    "def triangulatePoints(P1, P2, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Triangulates the given matching points from two images using the given camera matrices.\n",
    "\n",
    "    Parameters:\n",
    "    P1 (numpy.ndarray): 3x4 camera matrix of the first image.\n",
    "    P2 (numpy.ndarray): 3x4 camera matrix of the second image.\n",
    "    pts1 (numpy.ndarray): Nx2 matrix containing the coordinates of matching points in the first image.\n",
    "    pts2 (numpy.ndarray): Nx2 matrix containing the coordinates of matching points in the second image.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Nx3 matrix containing the triangulated 3D points.\n",
    "    \"\"\"\n",
    "    pts4D = OpenCV.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    pts4D /= pts4D[3]\n",
    "    return pts4D[:3].T\n",
    "\n",
    "\n",
    "def generate_point_cloud(images: Images, K_matrix):\n",
    "    \"\"\"\n",
    "    Generates a cloud of 3D points using triangulation from feature matches and camera calibration matrix.\n",
    "\n",
    "    Parameters:\n",
    "    feature_matches_list (list): List of feature matches between images.\n",
    "    K_matrix (numpy.ndarray): 3x3 camera calibration matrix.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Nx3 matrix containing the cloud of 3D points.\n",
    "    \"\"\"\n",
    "    point_cloud = []\n",
    "    feature_matches_list = images.feature_matches\n",
    "    for match in feature_matches_list:\n",
    "        img_one = match.image_one\n",
    "        img_two = match.image_two\n",
    "        matches = match.matches\n",
    "        pts1 = np.float32([img_one.keypoints[m.queryIdx].pt for m in matches])\n",
    "        pts2 = np.float32([img_two.keypoints[m.trainIdx].pt for m in matches])\n",
    "        E, _ = OpenCV.findEssentialMat(pts1, pts2, K_matrix)\n",
    "        R1, R2, t = OpenCV.decomposeEssentialMat(E)\n",
    "        P1 = np.hstack((np.eye(3), np.zeros((3,1))))\n",
    "        P2 = np.hstack((R1, t))\n",
    "        pts_3d = triangulatePoints(P1, P2, pts1, pts2)\n",
    "        point_cloud.append(pts_3d)\n",
    "    return np.concatenate(point_cloud, axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class Mode(enum.Enum):\n",
    "    OPTMIZED = \"optimized\"\n",
    "    DEBUG = \"debug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_to_file(\"logs/tune.log\", \"Welcome ScanMate...\")\n",
    "# image_set_name = \"rubik-cube\"\n",
    "# image_set_name = \"snow-man\"\n",
    "image_set_name = \"test\"\n",
    "mode: enum = Mode.DEBUG\n",
    "log_to_file(\"logs/tune.log\", f\"Running image_set_name {image_set_name} in {mode} mode...\")\n",
    "images: Optional[Images] = None\n",
    "\n",
    "# 0. Reload the last state\n",
    "last_state: str\n",
    "if os.path.isfile(f\"bak/{image_set_name}/feature-matching-output.pkl\"):\n",
    "    last_state = \"Feature Matching Step\"\n",
    "elif os.path.isfile(f\"bak/{image_set_name}/images-matched.pkl\"):\n",
    "    last_state = \"Images Matching Step\"\n",
    "elif os.path.isfile(f\"bak/{image_set_name}/sift-features.pkl\"):\n",
    "    last_state = \"SIFT Features Step\"\n",
    "else:\n",
    "    last_state = \"Images Loading Step\"\n",
    "log_to_file(\"logs/tune.log\", f\"Last state for {image_set_name} is {last_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and prepare Images\n",
    "if last_state == \"Images Loading Step\":\n",
    "    if os.path.isfile(f\"bak/{image_set_name}/images.pkl\"):\n",
    "        log_to_file(\"logs/tune.log\", f\"File [bak/{image_set_name}/sift-images.pkl] exists\")\n",
    "        log_to_file(\"logs/tune.log\", \"Loading images from pickle file...\")\n",
    "        images: Images = load_images_bak(f\"bak/{image_set_name}/images.pkl\")\n",
    "    else:\n",
    "        log_to_file(\"logs/tune.log\", f\"File [bak/{image_set_name}/images.pkl] does not exist\")\n",
    "        log_to_file(\"logs/tune.log\", \"Loading images from images directory...\")\n",
    "        images: Images = prepare_images(f\"images/{image_set_name}\")\n",
    "        log_to_file(\"logs/tune.log\", \"Saving images to pickle file...\")\n",
    "        dump_images_bak(f\"bak/{image_set_name}/images.pkl\", images)\n",
    "    log_to_file(\"logs/tune.log\", \"Images loaded successfully\")\n",
    "    last_state = \"SIFT Features Step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Extraction: SIFT\n",
    "if last_state == \"SIFT Features Step\":\n",
    "    if os.path.isfile(f\"bak/{image_set_name}/sift-features.pkl\"):\n",
    "        log_to_file(\"logs/tune.log\", f\"File [bak/{image_set_name}/sift-features.pkl] exists\")\n",
    "        if images: \n",
    "            del images\n",
    "        images: Images = load_images_bak(f\"bak/{image_set_name}/sift-features.pkl\")\n",
    "    else:\n",
    "        log_to_file(\"logs/tune.log\", \"File [bak/{image_set_name}/sift-features.pkl] DO NOT exists\")\n",
    "        log_to_file(\"logs/tune.log\", \"Extracting SIFT features...\")\n",
    "        sift = OpenCV.SIFT_create()\n",
    "        log_to_file(\"logs/tune.log\", f\"Ref count of images before: {sys.getrefcount(images)}\")\n",
    "        print_size(images, \"images\")\n",
    "        compute_keypoints_descriptors(images, sift)\n",
    "        print_size(images, \"images\")\n",
    "        log_to_file(\"logs/tune.log\", f\"Ref count of images after: {sys.getrefcount(images)}\")\n",
    "        dump_images_bak(f\"bak/{image_set_name}/sift-features.pkl\", images)\n",
    "        # remove bak/{image_set_name}/images.pkl\n",
    "        if mode == Mode.OPTMIZED:\n",
    "            if os.path.exists(f\"bak/{image_set_name}/images.pkl\"):\n",
    "                os.remove(f\"bak/{image_set_name}/images.pkl\")\n",
    "                log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/images.pkl removed successfully.\")\n",
    "            else:\n",
    "                log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/images.pkl does not exist.\")\n",
    "    log_to_file(\"logs/tune.log\", \"Feature Extraction: SIFT DONE...\")\n",
    "    last_state = \"Images Matching Step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Image Matching\n",
    "if last_state == \"Images Matching Step\":\n",
    "    if os.path.isfile(f\"bak/{image_set_name}/images-matched.pkl\"):\n",
    "        log_to_file(\"logs/tune.log\", f\"File [bak/{image_set_name}/images-matched.pkl] exists\")\n",
    "        if images: \n",
    "            del images\n",
    "        images: Images = load_images_bak(f\"bak/{image_set_name}/images-matched.pkl\")\n",
    "    else:\n",
    "        log_to_file(\"logs/tune.log\", f\"File [bak/{image_set_name}/images-matched.pkl] DO NOT exists\")\n",
    "        log_to_file(\"logs/tune.log\", \"Matching images...\")\n",
    "        log_to_file(\"logs/tune.log\", f\"Ref count of images before: {sys.getrefcount(images)}\")\n",
    "        print_size(images, \"images\")\n",
    "        get_matches(images)\n",
    "        print_size(images, \"images\")\n",
    "        log_to_file(\"logs/tune.log\", f\"Ref count of images after: {sys.getrefcount(images)}\")\n",
    "        dump_images_bak(f\"bak/{image_set_name}/images-matched.pkl\", images)\n",
    "        # remove bak/{image_set_name}/sift-features.pkl\n",
    "        if mode == Mode.OPTMIZED:\n",
    "            if os.path.exists(f\"bak/{image_set_name}/sift-features.pkl\"):\n",
    "                os.remove(f\"bak/{image_set_name}/sift-features.pkl\")\n",
    "                log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/sift-features.pkl removed successfully.\")\n",
    "            else:\n",
    "                log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/sift-features.pkl does not exist.\")\n",
    "    log_to_file(\"logs/tune.log\", \"Done Image Matching Step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Matching\n",
    "if os.path.isfile(f\"bak/{image_set_name}/feature-matching-output.pkl\"):\n",
    "    log_to_file(\"logs/tune.log\", f\"File [bak/{image_set_name}/feature-matching-output.pkl] exists\")\n",
    "    if images: \n",
    "        del images\n",
    "    images: Images = load_images_bak(f\"bak/{image_set_name}/feature-matching-output.pkl\")\n",
    "else:\n",
    "    log_to_file(\"logs/tune.log\", \"File [bak/{image_set_name}/feature-matching-output.pkl] Do NOT exists\")\n",
    "    log_to_file(\"logs/tune.log\", \"Matching features...\")\n",
    "    log_to_file(\"logs/tune.log\", f\"Ref count of images before: {sys.getrefcount(images)}\")\n",
    "    print_size(images, \"images\")\n",
    "    data_feature_matching(images)\n",
    "    print_size(images, \"images\")\n",
    "    log_to_file(\"logs/tune.log\", f\"Ref count of images after: {sys.getrefcount(images)}\")\n",
    "    dump_images_bak(f\"bak/{image_set_name}/feature-matching-output.pkl\", images)\n",
    "    # remove bak/{image_set_name}/images-matched.pkl\n",
    "    if mode == Mode.OPTMIZED:\n",
    "        if os.path.exists(f\"bak/{image_set_name}/images-matched.pkl\"):\n",
    "            os.remove(f\"bak/{image_set_name}/images-matched.pkl\")\n",
    "            log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/images-matched.pkl removed successfully.\")\n",
    "        else:\n",
    "            log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/images-matched.pkl does not exist.\")\n",
    "log_to_file(\"logs/tune.log\", \"Done Feature Matching Step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.feature_matches[0].draw_matches(\"output/test.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Camera Calibration\n",
    "log_to_file(\"logs/tune.log\", \"Camera Calibration starts ....\")\n",
    "if not os.path.isfile(f\"bak/{image_set_name}/K_matrix.pickle\"):\n",
    "    log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/K_matrix.pickle does not exist\")\n",
    "    K_matrix = compute_k_matrix(images.images[0].path)\n",
    "    with open(f\"bak/{image_set_name}/K_matrix.pickle\", 'wb') as f:\n",
    "        pickle.dump(K_matrix, f)\n",
    "    log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/K_matrix.pickle saved successfully\")\n",
    "else:\n",
    "    log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/K_matrix.pickle exists\")\n",
    "    with open(f\"bak/{image_set_name}/K_matrix.pickle\", 'rb') as f:\n",
    "        K_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Triangulation\n",
    "log_to_file(\"logs/tune.log\", \"Triangulation starts ....\")\n",
    "if os.path.isfile(f\"bak/{image_set_name}/point-cloud.pkl\"):\n",
    "    log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/point-cloud.pkl exists\")\n",
    "    with open(f\"bak/{image_set_name}/point-cloud.pkl\", 'rb') as f:\n",
    "        points_cloud: np.ndarray = pickle.load(f)\n",
    "else:\n",
    "    log_to_file(\"logs/tune.log\", f\"File bak/{image_set_name}/point-cloud.pkl does not exist\")\n",
    "    log_to_file(\"logs/tune.log\", \"Triangulating...\")\n",
    "    log_to_file(\"logs/tune.log\", f\"Ref count of images before: {sys.getrefcount(images)}\")\n",
    "    print_size(images, \"images\")\n",
    "    points_cloud: np.ndarray = generate_point_cloud(images, K_matrix)\n",
    "    print_size(images, \"images\")\n",
    "    log_to_file(\"logs/tune.log\", f\"Ref count of images after: {sys.getrefcount(images)}\")\n",
    "    # Pickle the point cloud\n",
    "    with open(f\"bak/{image_set_name}/point-cloud.pkl\", 'wb') as f:\n",
    "        pickle.dump(points_cloud, f)\n",
    "log_to_file(\"logs/tune.log\", \"Done Point Cloud Step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"points_cloud.shape: {points_cloud.shape}\")\n",
    "# np.savetxt(f\"output/{image_set_name}/point-cloud.txt\", points_cloud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean memory before Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_to_file(\"logs/tune.log\", f\"Ref count of images before: {sys.getrefcount(images)}\")\n",
    "print_size(images, \"images\")\n",
    "print_size(points_cloud, \"points_cloud\")\n",
    "images = None\n",
    "log_to_file(\"logs/tune.log\", gc.collect())\n",
    "print_size(images, \"images\")\n",
    "log_to_file(\"logs/tune.log\", f\"Reference count<images>: {sys.getrefcount(images)}\")\n",
    "log_to_file(\"logs/tune.log\", gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 3D reconstruction\n",
    "log_to_file(\"logs/tune.log\", \"3D reconstruction starts ....\")\n",
    "import open3d as o3d\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_cloud[:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it as a .PLY file\n",
    "o3d.io.write_point_cloud(\"point_cloud.ply\", pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path to the PLY file\n",
    "# file_path = \"/Users/ziadh/Desktop/college/gp/temp/mesh/point_cloud.ply\"\n",
    "\n",
    "# # Load the point cloud from the PLY file\n",
    "# point_cloud = o3d.io.read_point_cloud(file_path)\n",
    "\n",
    "# # Visualize the point cloud\n",
    "# o3d.visualization.draw_geometries([point_cloud])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
