{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from typing import Final, Optional\n",
    "\n",
    "import cv2 as OpenCV\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils Functions / Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_file(file_name: str, message: str):\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_message = f\"[{timestamp}] {message}\"\n",
    "    with open(file_name, \"a\") as f:\n",
    "        f.write(f\"{log_message}\\n\")\n",
    "\n",
    "\n",
    "def print_size(file_name: str, obj, obj_name=\"N/A\"):\n",
    "    from pympler import asizeof\n",
    "    memory_usage = asizeof.asizeof(obj)\n",
    "    # Convert memory usage to a more readable format\n",
    "    if memory_usage < 1024:\n",
    "        memory_usage_str = f\"{memory_usage} bytes\"\n",
    "    elif memory_usage < 1024 ** 2:\n",
    "        memory_usage_str = f\"{memory_usage / 1024} KB\"\n",
    "    elif memory_usage < 1024 ** 3:\n",
    "        memory_usage_str = f\"{memory_usage / (1024 ** 2)} MB\"\n",
    "    else:\n",
    "        memory_usage_str = f\"{memory_usage / (1024 ** 3)} GB\"\n",
    "    # Print the memory usage and object name\n",
    "    log_to_file(file_name, f\"Memory usage of {obj_name}: {memory_usage_str}\")\n",
    "\n",
    "def timeit(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        image_set_name = kwargs['image_set_name']\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Started {func.__name__}...\")\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Done {func.__name__} took {end_time - start_time:,} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalibrationError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "class IntrinsicParametersNotFoundError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, mask, keypoints, descriptors, path):\n",
    "        self.img_id: int = int(img_id)\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.mask: Image = mask\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def draw_sift_features(self):\n",
    "        image_with_sift = OpenCV.drawKeypoints(self.rgb_image, self.keypoints, None, flags=OpenCV.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        plt.imshow(image_with_sift)\n",
    "        plt.title(\"Image with SIFT Features\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_rgb_image(self, title: Optional[str] = None):\n",
    "        image = self.rgb_image\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_gray_image(self, title: Optional[str] = None):\n",
    "        image = self.gray_image\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_mask_image(self, title: Optional[str] = None):\n",
    "        image = self.mask\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        # plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_dialated_image(self, title: Optional[str] = None):\n",
    "        print(self.mask.shape)\n",
    "        print(self.rgb_image.shape)\n",
    "        image = OpenCV.bitwise_and(self.rgb_image, self.rgb_image, mask=self.mask)\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        # plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    def save_dilated_image(self, image_set_name: str):\n",
    "        image = OpenCV.bitwise_and(self.rgb_image, self.rgb_image,self.mask)\n",
    "        # Save the dilated image\n",
    "        OpenCV.imwrite(f\"data/{image_set_name}/output/dialated_image_{self.img_id}.jpg\", image)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def draw_matches(self, output_filename: str) -> None:\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "        for match in self.matches:\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "        OpenCV.imwrite(output_filename, combined_image)\n",
    "        \n",
    "    def animate_matches(self, output_filename: str) -> None:\n",
    "        import subprocess\n",
    "        for match in self.matches:\n",
    "            combined_image = OpenCV.hconcat([\n",
    "                self.image_one.rgb_image,\n",
    "                self.image_two.rgb_image\n",
    "            ])\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Write match.queryIdx at the top left corner\n",
    "            OpenCV.putText(\n",
    "                combined_image,\n",
    "                f\"{match.queryIdx}\",\n",
    "                (50, 150),  # position: 10 pixels from left, 20 pixels from top\n",
    "                OpenCV.FONT_HERSHEY_SIMPLEX,  # font\n",
    "                5,  # font scale\n",
    "                (0, 255, 0),  # font color (green)\n",
    "                5,  # thickness\n",
    "                OpenCV.LINE_AA  # line type\n",
    "            )\n",
    "            # Write match.trainIdx at the top right corner\n",
    "            image_two_width = self.image_one.rgb_image.shape[1]\n",
    "            OpenCV.putText(\n",
    "                combined_image,\n",
    "                f\"{match.trainIdx}\",\n",
    "                (image_two_width + 50, 150),  # position: 10 pixels from right, 20 pixels from top\n",
    "                OpenCV.FONT_HERSHEY_SIMPLEX,  # font\n",
    "                5,  # font scale\n",
    "                (0, 255, 0),  # font color (green)\n",
    "                5,  # thickness\n",
    "                OpenCV.LINE_AA  # line type\n",
    "            )\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "            OpenCV.imwrite(\n",
    "                f\"{output_filename}/{match.queryIdx}_{match.trainIdx}.jpg\",\n",
    "                combined_image,\n",
    "            )\n",
    "        framerate = 120\n",
    "        # Get a list of image files in the directory\n",
    "        image_files = [f for f in os.listdir(output_filename) if f.endswith(\".jpg\")]\n",
    "        image_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
    "        # Create a temporary file with a list of input images\n",
    "        with open(\"input_files.txt\", \"w\") as f:\n",
    "            for image_file in image_files:\n",
    "                f.write(f\"file '{os.path.join(output_filename, image_file)}'\\n\")\n",
    "        # Run FFmpeg command to create a video\n",
    "        command = f'ffmpeg -y -f concat -safe 0 -i \"input_files.txt\" -framerate {framerate} -c:v libx264 -pix_fmt yuv420p \"{output_filename}/output.mp4\"'\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        # Remove temporary file\n",
    "        os.remove(\"input_files.txt\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "        self.similar_images: dict[list[Image]] = {}\n",
    "        self.num_clusters: int = 50\n",
    "\n",
    "    def save_feature_matches(self):\n",
    "        for match in self.feature_matches:\n",
    "            match.draw_matches(f\"data/{self.image_set_name}/output/feature-match/{match.image_one.img_id}_{match.image_two.img_id}.jpg\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def display_similar_images(self, key):\n",
    "        print(f\"cluster {key}\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        for value in self.similar_images[key]:\n",
    "            print(value)\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(value.path), OpenCV.COLOR_BGR2RGB)\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.title(value.path)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    def save_similar_images(self):\n",
    "        for cluster in self.similar_images.keys():\n",
    "            if not os.path.exists(f\"data/{self.image_set_name}/output/image-match/{cluster}\"):\n",
    "                os.makedirs(f\"data/{self.image_set_name}/output/image-match/{cluster}\")\n",
    "            for value in self.similar_images[cluster]:\n",
    "                OpenCV.imwrite(f\"data/{self.image_set_name}/output/image-match/{cluster}/{value.img_id}.jpg\", value.rgb_image)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Image:\n",
    "        for image in self.images:\n",
    "            if image.img_id == key:\n",
    "                return image\n",
    "        raise KeyError(f'Image with img_id {key} not found.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_images_bak(images_file_path: str, images: Images) -> None:\n",
    "    \"\"\" Dump images to a file \"\"\"\n",
    "    with open(images_file_path, \"wb\") as file:\n",
    "        pickle.dump(images, file)\n",
    "\n",
    "def load_images_bak(images_file_path: str) -> Images:\n",
    "    \"\"\" Load images from a file \"\"\"\n",
    "    with open(images_file_path, \"rb\") as file:\n",
    "        images = pickle.load(file)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step One: Read and Load Images\n",
    "Inputs: \n",
    "- folder_path: str\n",
    "\n",
    "Outputs:\n",
    "- images: Images\n",
    "\n",
    "Main Functions:\n",
    "1. prepare_images: read and load images from a folder into an Images object\n",
    "\n",
    "Utils Functions:\n",
    "1. dump_images: dump images to a pickle file\n",
    "2. load_images: load images from a pickle file\n",
    "\"\"\"\n",
    "\n",
    "@timeit\n",
    "def prepare_images(create_mask = False, **kwargs) -> Images:\n",
    "    \"\"\" Read and load images \"\"\"\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    folder_path = f\"data/{image_set_name}\"\n",
    "    images: Images = Images([], folder_path.split(\"/\")[-1])\n",
    "    files: list[str] = list(\n",
    "        filter(\n",
    "            lambda file: \".jpg\" in file, os.listdir(f\"{folder_path}/images\")\n",
    "        )\n",
    "    )\n",
    "    if create_mask:\n",
    "        from rembg import remove\n",
    "        for file in files:\n",
    "            image_path = f\"{folder_path}/images/{file}\"\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "            gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask = remove(rgb_image)\n",
    "            mask = OpenCV.cvtColor(mask, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask[mask > 0] = 255\n",
    "            OpenCV.imwrite(f\"{folder_path}/masks/{file}\", mask)\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            dilated_mask = OpenCV.dilate(mask, kernel, iterations=20)\n",
    "            images.images.append(Image(file.split(\".\")[0], rgb_image, gray_image, dilated_mask, [], [], image_path))\n",
    "    else:\n",
    "        for file in files:\n",
    "            image_path = f\"{folder_path}/images/{file}\"\n",
    "            mask_path = f\"{folder_path}/masks/{file}\"\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "            gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "            mask = OpenCV.imread(mask_path, OpenCV.IMREAD_GRAYSCALE)\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            dilated_mask = OpenCV.dilate(mask, kernel, iterations=20)\n",
    "            images.images.append(Image(file.split(\".\")[0], rgb_image, gray_image, dilated_mask, [], [], image_path))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Two: Feature Extraction\n",
    "Inputs:\n",
    "- images: Images\n",
    "- SIFT: OpenCV.SIFT\n",
    "\n",
    "Outputs:\n",
    "- image: Image\n",
    "--> image.keypoints: list[OpenCV.KeyPoint]\n",
    "--> image.descriptors: np.ndarray\n",
    "\n",
    "Main Functions:\n",
    "1. compute_keypoints_descriptors\n",
    "\"\"\"\n",
    "\n",
    "@timeit\n",
    "def compute_keypoints_descriptors(images: list[Image], SIFT: OpenCV.SIFT, **kwargs) -> None:\n",
    "    \"\"\"Compute keypoints and descriptors for each image in the list of images using SIFT algorithm.\n",
    "    Modifies each image in the list of images by adding its keypoints and descriptors as attributes.\n",
    "    \n",
    "    Args:\n",
    "    - images: List of images to compute keypoints and descriptors for.\n",
    "    - SIFT: OpenCV SIFT object used to detect and compute keypoints and descriptors.\n",
    "\n",
    "    Returns:\n",
    "    - None.\n",
    "    \"\"\"\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    for img in images.images:\n",
    "        keypoints: list[OpenCV.KeyPoint]\n",
    "        descriptors: np.ndarray\n",
    "        dialated_image = OpenCV.bitwise_and(img.gray_image, img.gray_image, mask=img.mask)\n",
    "        keypoints, descriptors = SIFT.detectAndCompute(dialated_image, None)\n",
    "        img.keypoints = keypoints\n",
    "        img.descriptors = descriptors\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Img({img.img_id}, {img.path}) has {len(img.keypoints)} keypoints and {len(img.descriptors)} descriptors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Three: Image Matching\n",
    "Inputs:\n",
    "- images: Images\n",
    "\n",
    "Outputs:\n",
    "- image: Image\n",
    "--> image.similar_images: dict[list[Image]]\n",
    "\n",
    "Main Functions:\n",
    "1. image_matching\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "\n",
    "@timeit\n",
    "def overwriting_similar_images(images: Images, **kwargs) -> dict[str, list[Image]]:\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    if image_set_name == 'cottage':\n",
    "        similar_images = {\n",
    "            \"0\": [images[1], images[2], images[3]],\n",
    "            \"1\": [images[3], images[4], images[5]],\n",
    "            \"2\": [images[5], images[6], images[7]],\n",
    "            \"3\": [images[7], images[8], images[9]],\n",
    "            \"4\": [images[9], images[10], images[11]],\n",
    "            \"5\": [images[11], images[12], images[13]],\n",
    "            \"6\": [images[13], images[14], images[15]],\n",
    "            \"7\": [images[15], images[16], images[17]],\n",
    "            \"8\": [images[17], images[18], images[19]],\n",
    "            \"9\": [images[19], images[20], images[21]],\n",
    "            \"10\": [images[21], images[22], images[23]],\n",
    "            \"11\": [images[23], images[24], images[25]],\n",
    "            \"12\": [images[25], images[26], images[27]],\n",
    "            \"13\": [images[27], images[28], images[29]],\n",
    "            \"14\": [images[29], images[30]]\n",
    "        }\n",
    "    elif image_set_name == 'fountain':\n",
    "          similar_images = {\n",
    "          \"0\": [images[1], images[2], images[3], images[4], images[5]],\n",
    "          \"1\": [images[5], images[6], images[7], images[8]],\n",
    "          \"2\": [images[8], images[9], images[10], images[11]]\n",
    "        }\n",
    "    elif image_set_name == 'hammer':\n",
    "        similar_images = {\n",
    "            \"0\": [images[1], images[2], images[3], images[4], images[5]],\n",
    "            \"1\": [images[5], images[6], images[7]],\n",
    "            \"2\": [images[7], images[8], images[9], images[10], images[11], images[12]],\n",
    "            \"3\": [images[12], images[13], images[14], images[15]],\n",
    "            \"4\": [images[15], images[16], images[17], images[18]],\n",
    "            \"5\": [images[18], images[19], images[20]],\n",
    "            \"6\": [images[20], images[21], images[22], images[23]],\n",
    "            \"7\": [images[23], images[24], images[25]],\n",
    "            \"8\": [images[25], images[26], images[27], images[28]],\n",
    "            \"9\": [images[28], images[29], images[30], images[31]],\n",
    "            \"10\": [images[31], images[32], images[33]],\n",
    "            \"11\": [images[33], images[34], images[35]],\n",
    "            \"12\": [images[35], images[36], images[37], images[38]]\n",
    "        }\n",
    "    return similar_images\n",
    "\n",
    "@timeit\n",
    "def image_matching(images_obj: Images, overwrite:bool =False, **kwargs) -> None:\n",
    "  def load_image(image_path, target_size=(224, 224)):\n",
    "    img = keras_image.load_img(image_path, target_size=target_size)\n",
    "    img = keras_image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "  \n",
    "  image_set_name = kwargs['image_set_name']\n",
    "  image_dir = f'data/{image_set_name}/images'\n",
    "  image_files = os.listdir(image_dir)\n",
    "  images = [load_image(os.path.join(image_dir, f)) for f in image_files]\n",
    "  images = np.vstack(images)\n",
    "\n",
    "  import ssl\n",
    "  import certifi\n",
    "\n",
    "  ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
    "  ssl._create_default_https_context = ssl._create_unverified_context\n",
    "  model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "  features = model.predict(images)\n",
    "\n",
    "  kmeans = KMeans(n_clusters=images_obj.num_clusters, random_state=42)\n",
    "  clusters = kmeans.fit_predict(features)\n",
    "\n",
    "  for i, cluster in enumerate(clusters):\n",
    "      if cluster not in images_obj.similar_images:\n",
    "        images_obj.similar_images[cluster] = []\n",
    "      images_obj.similar_images[cluster].append(images_obj[int(image_files[i].split(\".\")[0])])\n",
    "  if overwrite:\n",
    "    images_obj.similar_images = overwriting_similar_images(images_obj, image_set_name=image_set_name)\n",
    "  else:\n",
    "    images_obj.similar_images = {key: value for key, value in images_obj.similar_images.items() if len(value) > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Four: Feature Matching\n",
    "Inputs:\n",
    "- images: Images\n",
    "\n",
    "Outputs:\n",
    "- None\n",
    "\n",
    "Main Functions:\n",
    "1. data_feature_matching\n",
    "\n",
    "Utils Functions:\n",
    "1. feature_matching\n",
    "\"\"\"\n",
    "\n",
    "@timeit\n",
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray,\n",
    "        **kwargs\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    \"\"\" Match features between two images using Brute Force Matcher\n",
    "    Args:\n",
    "        img_id_one: the index of the first image\n",
    "        img_id_two: the index of the second image\n",
    "        descriptors: a list of descriptors of the images\n",
    "    Returns:\n",
    "        A list of OpenCV.DMatch objects.\n",
    "    \"\"\"\n",
    "    matcher = OpenCV.BFMatcher(crossCheck=True)\n",
    "    return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "@timeit\n",
    "def apply_ransac(matches, keypoints1, keypoints2, threshold = 3.0, **kwargs):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    _, mask = OpenCV.findHomography(src_pts, dst_pts, OpenCV.RANSAC, threshold)\n",
    "    matches_mask = mask.ravel().tolist()\n",
    "    return [m for m, keep in zip(matches, matches_mask) if keep]\n",
    "\n",
    "@timeit\n",
    "def data_feature_matching(images: Images, **kwargs) -> None:\n",
    "    \"\"\" Match features between images using Brute Force Matcher\n",
    "    Args:\n",
    "        matchesIDs: a list of lists of tuples, where each tuple contains the index of a similar image and the cosine similarity \n",
    "            between the i-th image and the similar image. The list is sorted by the cosine similarity in \n",
    "            descending order.\n",
    "        descriptors: a list of descriptors of the images\n",
    "    Returns:\n",
    "        A list of lists, where each list contains \n",
    "        the index of the first image, the index of the second image, \n",
    "        and a list of OpenCV.DMatch objects.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    for key, values in images.similar_images.items():\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Started Feature Match for cluster number {key}:\")\n",
    "        for image, matched_image in itertools.combinations(values, 2):\n",
    "            feature_matching_output = feature_matching(image.descriptors, matched_image.descriptors, **kwargs)\n",
    "            ransac_output = apply_ransac(feature_matching_output, image.keypoints, matched_image.keypoints, threshold=150, **kwargs)\n",
    "            images.feature_matches.append(FeatureMatches(image, matched_image, ransac_output))\n",
    "            log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"({image.img_id}, {matched_image.img_id}) with {len(ransac_output)} / {len(feature_matching_output)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Five: Camera Calibration\n",
    "Inputs:\n",
    "- None.\n",
    "\n",
    "Outputs:\n",
    "- k_matrix: np.ndarray\n",
    "\n",
    "[\n",
    "    [focal_length, 0, principal_point_x],\n",
    "    [0, focal_length, principal_point_y],\n",
    "    [0, 0, scaling_factor]\n",
    "]\n",
    "\n",
    "ToDo:\n",
    "1- generate K_matrix.pickle for each camera using Chess board pattern.\n",
    "\"\"\"\n",
    "\n",
    "k_matrix_dict: dict[str, np.ndarray] = {\n",
    "    \"hammer\": np.array([\n",
    "        [7600, 0, 2736],\n",
    "        [0, 7600, 1824],\n",
    "        [0, 0, 1.0],\n",
    "    ]),\n",
    "    \"cottage\": np.array([\n",
    "        [4044.943820224719, 0, 3000],\n",
    "        [0, 4044.943820224719, 2000],\n",
    "        [0, 0, 1.0],\n",
    "    ]),\n",
    "    \"fountain\": np.array([\n",
    "        [3708.232031805074, 0, 1536],\n",
    "        [0, 3708.232031805074, 1024],\n",
    "        [0, 0, 1.0],\n",
    "    ])\n",
    "}\n",
    "\n",
    "@timeit\n",
    "def compute_k_matrix(**kwargs) -> np.ndarray:\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    return k_matrix_dict[image_set_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Six: Triangulation (3D Reconstruction)\n",
    "Inputs:\n",
    "- feature_matches_list: list[list[int, int, list[OpenCV.DMatch]]]\n",
    "    -> A list of lists, where each list contains \n",
    "        the index of the first image, the index of the second image, \n",
    "        and a list of OpenCV.DMatch objects.\n",
    "- K_matrix: np.ndarray\n",
    "    -> The camera matrix of the camera used to take the images.\n",
    "\n",
    "Outputs:\n",
    "- point_cloud: list[np.ndarray]; each element is a 3D point.\n",
    "\n",
    "Main Functions:\n",
    "1. generate_point_cloud\n",
    "\n",
    "Utils Functions:\n",
    "1. triangulatePoints\n",
    "\"\"\"\n",
    "to_tuple = lambda x: tuple(x.flatten())\n",
    "\n",
    "def check_coherent_rotation(R: np.ndarray) -> bool:\n",
    "    return np.abs(np.linalg.det(R) - 1.0) <= 1e-6\n",
    "\n",
    "@timeit\n",
    "def find_3D_2D_correspondences(\n",
    "        image_two: Image,\n",
    "        feature_matches: list[FeatureMatches], \n",
    "        global_dict: dict[np.ndarray, set[tuple[int]]],\n",
    "        **kwargs\n",
    "    ) -> dict[np.ndarray, np.ndarray]:\n",
    "    local_dict: dict[np.ndarray, np.ndarray] = {}\n",
    "    for feature_match in feature_matches: # 1, 2, 3, 4 -> [(1, 2), \"(1, 3)\", (1, 4), \"(2, 3)\", (2, 4), (3, 4)]\n",
    "        if feature_match.image_two != image_two:\n",
    "            continue\n",
    "        for match in feature_match.matches:\n",
    "            search_keypoint_one = feature_match.image_one.keypoints[match.queryIdx].pt\n",
    "            search_img_id = feature_match.image_one.img_id\n",
    "            search_tuple = (search_img_id, search_keypoint_one)\n",
    "            for key, values in global_dict.items():\n",
    "                if search_tuple in values:\n",
    "                    if key not in local_dict:\n",
    "                        local_dict[key] = image_two.keypoints[match.trainIdx].pt\n",
    "    return local_dict\n",
    "\n",
    "@timeit\n",
    "def find_initial_camera_matrices(K: np.ndarray, keypoints_one: np.ndarray, keypoints_two: np.ndarray, **kwargs) -> tuple[np.ndarray, np.ndarray]:\n",
    "    E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n",
    "    # TODO: use mask to filter out outliers\n",
    "    _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K)\n",
    "    return (None, None) if not check_coherent_rotation(R) else (R, t)\n",
    "\n",
    "@timeit\n",
    "def find_next_camera_matrices(\n",
    "        images: Images,\n",
    "        image_one: Image,\n",
    "        image_two: Image, \n",
    "        K_matrix: np.ndarray, \n",
    "        global_dict: dict[np.ndarray, set[tuple[int]]],\n",
    "        **kwargs\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    if image_one is not None:\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Using Images {image_one.img_id} and {image_two.img_id} in find_next_camera_matrices\")\n",
    "    local_dict: dict[np.ndarray, np.ndarray] = find_3D_2D_correspondences(image_two, images.feature_matches, global_dict, image_set_name=image_set_name)\n",
    "    objectPoints = np.array(list(local_dict.keys())).reshape(-1, 3)\n",
    "    imagePoints = np.array(list(local_dict.values())).reshape(-1, 2)\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Found {objectPoints.shape[0]} 3D Points and {imagePoints.shape[0]} Image Points 3D-2D correspondences\")\n",
    "    _, rvec, tvec, _ = OpenCV.solvePnPRansac(objectPoints, imagePoints, K_matrix, None)\n",
    "    R, _ = OpenCV.Rodrigues(rvec)\n",
    "    return R, tvec\n",
    "\n",
    "@timeit\n",
    "def compute_points_3D(\n",
    "        P1: np.ndarray, \n",
    "        P2: np.ndarray, \n",
    "        image_one: Image,\n",
    "        image_two: Image,\n",
    "        keypoints_one: np.ndarray,\n",
    "        keypoints_two: np.ndarray,\n",
    "        global_dict: dict[np.ndarray, set[tuple[int]]],\n",
    "        **kwargs\n",
    "    ) -> np.ndarray:\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    points_3D = np.empty((3, len(keypoints_one)))\n",
    "    for point_counter, (keypoint_one, keypoint_two) in enumerate(zip(keypoints_one, keypoints_two)):\n",
    "        point_4D = OpenCV.triangulatePoints(P1, P2, keypoint_one.T, keypoint_two.T)  # 4x1\n",
    "        point_3D = (point_4D / point_4D[3])[:3]  # 3x1\n",
    "        if to_tuple(point_3D) in global_dict:\n",
    "            global_dict[to_tuple(point_3D)].add((image_one.img_id, to_tuple(keypoint_one)))\n",
    "            global_dict[to_tuple(point_3D)].add((image_two.img_id, to_tuple(keypoint_two)))\n",
    "        else:\n",
    "            global_dict[to_tuple(point_3D)] = {\n",
    "                (image_one.img_id, to_tuple(keypoint_one)),\n",
    "                (image_two.img_id, to_tuple(keypoint_two))\n",
    "            }\n",
    "        points_3D[:, point_counter] = point_3D.flatten()\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Computed {points_3D.shape[1]} 3D Points for Image pairs {image_one.img_id} and {image_two.img_id}\")\n",
    "    return points_3D\n",
    "\n",
    "@timeit\n",
    "def find_cluster_feature_matches( \n",
    "        images: Images, \n",
    "        values: list[Image],\n",
    "        **kwargs\n",
    "    ) -> list[FeatureMatches]: # [1,2,3] ----> [1,2],[1,3]\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    cluster_reference_image = values[0]\n",
    "    cluster_feature_matches: list[FeatureMatches] = []\n",
    "    import itertools\n",
    "    for image, matched_image in itertools.combinations(values, 2):\n",
    "        if image.img_id != cluster_reference_image.img_id:\n",
    "            log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Breaking itertools loop for {image.img_id} and {matched_image.img_id} in find_cluster_feature_matches\\n\")\n",
    "            break\n",
    "        else:\n",
    "            appended_pair: FeatureMatches = next(\n",
    "                fm for fm in images.feature_matches\n",
    "                if fm.image_one.img_id == image.img_id and fm.image_two.img_id == matched_image.img_id\n",
    "            )\n",
    "            log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"appended_pair: {appended_pair}\")\n",
    "            cluster_feature_matches.append(appended_pair)\n",
    "    return cluster_feature_matches\n",
    "\n",
    "@timeit\n",
    "def generate_points_cloud(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    # sourcery skip: low-code-quality\n",
    "    points_cloud: list[list[np.ndarray]] = []\n",
    "    global_dict: dict[np.ndarray, set[tuple[int]]] = {}\n",
    "    camera_matrices: list[np.ndarray] = [(np.eye(3), np.zeros((3, 1)))]\n",
    "    image_set_name = kwargs['image_set_name']\n",
    "    for cluster, values in images.similar_images.items():\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"--------------------- Entering Cluster {cluster} ---------------------\")\n",
    "        cluster_feature_matches:list[FeatureMatches] = find_cluster_feature_matches(images, values, image_set_name=image_set_name)\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"cluster_feature_matches: {cluster_feature_matches}\\n\")\n",
    "        if cluster == list(images.similar_images.keys())[0]: # First cluster\n",
    "            P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "            for feature_match in cluster_feature_matches:\n",
    "                image_one = feature_match.image_one\n",
    "                image_two = feature_match.image_two\n",
    "                keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "                keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "                if feature_match == cluster_feature_matches[0]:  # First Feature Match Pair in the First Cluster, where we use recoverPose\n",
    "                    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Using Images {image_one.img_id} and {image_two.img_id} in recoverPose\")\n",
    "                    R, t = find_initial_camera_matrices(K_matrix, keypoints_one, keypoints_two, image_set_name=image_set_name)\n",
    "                    P2 = K_matrix @ np.hstack((R, t))\n",
    "                    camera_matrices.append((R, t))\n",
    "                else:\n",
    "                    R, tvec = find_next_camera_matrices(images, image_one, image_two, K_matrix, global_dict, image_set_name=image_set_name)\n",
    "                    P2 = K_matrix @ np.hstack((R, tvec))\n",
    "                    camera_matrices.append((R, tvec))\n",
    "                points_3D = compute_points_3D(P1, P2, image_one, image_two, keypoints_one, keypoints_two, global_dict, image_set_name=image_set_name)\n",
    "                points_cloud.append(points_3D)\n",
    "                log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Global Dict 3D Points Size: {len(global_dict.keys())} \\n\")\n",
    "        else: # Next Clusters\n",
    "            for feature_match in cluster_feature_matches:\n",
    "                image_one = feature_match.image_one\n",
    "                image_two = feature_match.image_two\n",
    "                keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "                keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "                if feature_match == cluster_feature_matches[0]: # First Iteration of the next Cluster\n",
    "                # Computing new P1 for the new cluster\n",
    "                    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Entered First Iteration of the cluster {cluster}\")\n",
    "                    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Using Image {image_one.img_id} as Reference Image in cluster {cluster} to compute P1 for cluster {cluster}\")\n",
    "                    P1_R, P1_tvec = find_next_camera_matrices(images, None, image_one, K_matrix, global_dict, image_set_name=image_set_name)\n",
    "                    P1 = K_matrix @ np.hstack((P1_R, P1_tvec))\n",
    "                R, tvec = find_next_camera_matrices(images, image_one, image_two, K_matrix, global_dict, image_set_name=image_set_name)\n",
    "                P2 = K_matrix @ np.hstack((R, tvec))\n",
    "                camera_matrices.append((R, tvec))\n",
    "                points_3D = compute_points_3D(P1, P2, image_one, image_two, keypoints_one, keypoints_two, global_dict, image_set_name=image_set_name)\n",
    "                points_cloud.append(points_3D)\n",
    "                log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Global Dict 3D Points Size: {len(global_dict.keys())} \\n\")\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"--------------------- End of cluster {cluster} ---------------------\\n\\n\")\n",
    "\n",
    "    points_cloud = np.hstack(points_cloud).T\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Done generating points cloud\")\n",
    "    return points_cloud, camera_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "def create_camera_frustum(P: np.ndarray, scale: float) -> o3d.geometry.TriangleMesh:\n",
    "    vertices = np.array([[0.5, 0.5, 0], [0.5, -0.5, 0], [-0.5, -0.5, 0], [-0.5, 0.5, 0], [0, 0, -1]])\n",
    "    vertices *= scale\n",
    "    faces = np.array([[0, 1, 4], [1, 2, 4], [2, 3, 4], [3, 0, 4], [1, 0, 3]])\n",
    "    R, t = P\n",
    "    vertices = vertices @ R.T + t[:3].T\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(vertices)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    vertex_colors = np.ones((len(vertices), 3)) * [1, 0, 0]\n",
    "    mesh.vertex_colors = o3d.utility.Vector3dVector(vertex_colors)\n",
    "    # draw camera rod\n",
    "    start_point = np.array([0, 0, 0])\n",
    "    end_point = np.array([0, 0, 1])*scale\n",
    "    start_point = start_point @ R.T + t[:3].T\n",
    "    end_point = end_point @ R.T + t[:3].T\n",
    "    rod = o3d.geometry.TriangleMesh.create_cylinder(radius=0.02*scale, height=np.linalg.norm(end_point-start_point), resolution=20, split=4)\n",
    "    rod.vertices = o3d.utility.Vector3dVector(np.asarray(rod.vertices) + start_point)\n",
    "    vertex_colors = np.ones((len(rod.vertices), 3)) * [0, 0, 0]\n",
    "    rod.vertex_colors = o3d.utility.Vector3dVector(vertex_colors)\n",
    "    return mesh, rod"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class Mode(enum.Enum):\n",
    "    OPTMIZED = \"optimized\"\n",
    "    DEBUG = \"debug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set_name = \"fountain\"\n",
    "# image_set_name = \"cottage\"\n",
    "# image_set_name = \"hammer\"\n",
    "# image_set_name = \"rubik-cube\"\n",
    "# image_set_name = \"snow-man\"\n",
    "# image_set_name = \"test\"\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Welcome ScanMate...\")\n",
    "mode: enum = Mode.DEBUG\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Running image_set_name {image_set_name} in {mode} mode...\")\n",
    "images: Optional[Images] = None\n",
    "\n",
    "# 0. Reload the last state\n",
    "last_state: str\n",
    "if os.path.isfile(f\"data/{image_set_name}/bak/feature-matching-output.pkl\"):\n",
    "    last_state = \"Feature Matching Step\"\n",
    "elif os.path.isfile(f\"data/{image_set_name}/bak/matched-images.pkl\"):\n",
    "    last_state = \"Images Matching Step\"\n",
    "elif os.path.isfile(f\"data/{image_set_name}/bak/sift-features.pkl\"):\n",
    "    last_state = \"SIFT Features Step\"\n",
    "else:\n",
    "    last_state = \"Images Loading Step\"\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Last state for {image_set_name} is {last_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and prepare Images\n",
    "if last_state == \"Images Loading Step\":\n",
    "    if os.path.isfile(f\"data/{image_set_name}/bak/images.pkl\"):\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File [data/{image_set_name}/bak/sift-images.pkl] exists\")\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Loading images from pickle file...\")\n",
    "        images: Images = load_images_bak(f\"data/{image_set_name}/bak/images.pkl\")\n",
    "    else:\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File [data/{image_set_name}/bak/images.pkl] does not exist\")\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Loading images from images directory...\")\n",
    "        images: Images = prepare_images(image_set_name=image_set_name)\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Saving images to pickle file...\")\n",
    "        dump_images_bak(f\"data/{image_set_name}/bak/images.pkl\", images)\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Images loaded successfully\")\n",
    "    last_state = \"SIFT Features Step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Extraction: SIFT\n",
    "if last_state == \"SIFT Features Step\":\n",
    "    if os.path.isfile(f\"data/{image_set_name}/bak/sift-features.pkl\"):\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File [data/{image_set_name}/bak/sift-features.pkl] exists\")\n",
    "        if images: \n",
    "            del images\n",
    "        images: Images = load_images_bak(f\"data/{image_set_name}/bak/sift-features.pkl\")\n",
    "    else:\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"File [data/{image_set_name}/bak/sift-features.pkl] DO NOT exists\")\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Extracting SIFT features...\")\n",
    "        sift = OpenCV.SIFT_create(contrastThreshold=0.01)\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Ref count of images before: {sys.getrefcount(images)}\")\n",
    "        print_size(f\"data/{image_set_name}/logs/tune.log\", images, \"images\")\n",
    "        compute_keypoints_descriptors(images, sift, image_set_name=image_set_name)\n",
    "        print_size(f\"data/{image_set_name}/logs/tune.log\", images, \"images\")\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Ref count of images after: {sys.getrefcount(images)}\")\n",
    "        dump_images_bak(f\"data/{image_set_name}/bak/sift-features.pkl\", images)\n",
    "        # remove bak/{image_set_name}/images.pkl\n",
    "        if mode == Mode.OPTMIZED:\n",
    "            if os.path.exists(f\"data/{image_set_name}/bak/images.pkl\"):\n",
    "                os.remove(f\"data/{image_set_name}/bak/images.pkl\")\n",
    "                log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/images.pkl removed successfully.\")\n",
    "            else:\n",
    "                log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/images.pkl does not exist.\")\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Feature Extraction: SIFT DONE...\")\n",
    "    last_state = \"Images Matching Step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Similar images for fountain: {2: [Image(1), Image(2), Image(3), Image(4)], 1: [Image(10), Image(11), Image(9)], 0: [Image(5), Image(6), Image(7), Image(8)]}\n",
      "Overwriting similar images for fountain...\n",
      "<class 'dict'>\n",
      "{'0': [Image(1), Image(2), Image(3), Image(4), Image(5)], '1': [Image(5), Image(6), Image(7), Image(8)], '2': [Image(8), Image(9), Image(10), Image(11)]}\n",
      "Similar images for fountain after overwriting : {'0': [Image(1), Image(2), Image(3), Image(4), Image(5)], '1': [Image(5), Image(6), Image(7), Image(8)], '2': [Image(8), Image(9), Image(10), Image(11)]}\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Education\\Graduation Project\\photogrammetry\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Image Matching\n",
    "overwrite: bool = True\n",
    "if last_state == \"Images Matching Step\":\n",
    "    if os.path.isfile(f\"data/{image_set_name}/bak/matched-images.pkl\"):\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File [data/{image_set_name}/bak/matched-images.pkl] exists\")\n",
    "        if images: \n",
    "            del images\n",
    "        images: Images = load_images_bak(f\"data/{image_set_name}/bak/matched-images.pkl\")\n",
    "        print(\"images = \",images)\n",
    "    else:\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File [data/{image_set_name}/bak/matched-images.pkl] DO NOT exists\")\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Matching images...\")\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Ref count of images before: {sys.getrefcount(images)}\")\n",
    "        print_size(f\"data/{image_set_name}/logs/tune.log\", images, \"images\")\n",
    "        images.num_clusters = 3\n",
    "        image_matching(images, overwrite, image_set_name=image_set_name)\n",
    "        log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"image matching done\")\n",
    "        if not overwrite:\n",
    "            images.save_similar_images()\n",
    "            log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"saved image clusters\")\n",
    "            print_size(f\"data/{image_set_name}/logs/tune.log\", images, \"images\")\n",
    "            log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Ref count of images after: {sys.getrefcount(images)}\")\n",
    "            dump_images_bak(f\"data/{image_set_name}/bak/matched-images.pkl\", images)\n",
    "        if mode == Mode.OPTMIZED:\n",
    "            if os.path.exists(f\"data/{image_set_name}/bak/sift-features.pkl\"):\n",
    "                os.remove(f\"data/{image_set_name}/bak/sift-features.pkl\")\n",
    "                log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/sift-features.pkl removed successfully.\")\n",
    "            else:\n",
    "                log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/sift-features.pkl does not exist.\")\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Done Image Matching Step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('0', [Image(1), Image(2), Image(3), Image(4), Image(5)]), ('1', [Image(5), Image(6), Image(7), Image(8)]), ('2', [Image(8), Image(9), Image(10), Image(11)])])\n"
     ]
    }
   ],
   "source": [
    "# 4. Feature Matching\n",
    "if os.path.isfile(f\"data/{image_set_name}/bak/feature-matching-output.pkl\"):\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File [data/{image_set_name}/bak/feature-matching-output.pkl] exists\")\n",
    "    if images: \n",
    "        del images\n",
    "    images: Images = load_images_bak(f\"data/{image_set_name}/bak/feature-matching-output.pkl\")\n",
    "else:\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"File [data/{image_set_name}/bak/feature-matching-output.pkl] Do NOT exists\")\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Matching features...\")\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Ref count of images before: {sys.getrefcount(images)}\")\n",
    "    print_size(f\"data/{image_set_name}/logs/tune.log\", images, \"images\")\n",
    "    data_feature_matching(images, image_set_name=image_set_name)\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"done feature matching\")\n",
    "    images.save_feature_matches()\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"saved feature matching\")\n",
    "    print_size(f\"data/{image_set_name}/logs/tune.log\", images, \"images\")\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Ref count of images after: {sys.getrefcount(images)}\")\n",
    "    dump_images_bak(f\"data/{image_set_name}/bak/feature-matching-output.pkl\", images)\n",
    "    # remove bak/{image_set_name}/matched-images.pkl\n",
    "    if mode == Mode.OPTMIZED:\n",
    "        if os.path.exists(f\"data/{image_set_name}/bak/matched-images.pkl\"):\n",
    "            os.remove(f\"data/{image_set_name}/bak/matched-images.pkl\")\n",
    "            log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/matched-images.pkl removed successfully.\")\n",
    "        else:\n",
    "            log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/matched-images.pkl does not exist.\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Done Feature Matching Step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Camera Calibration\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Camera Calibration starts ....\")\n",
    "if not os.path.isfile(f\"data/{image_set_name}/bak/k-matrix.pkl\"):\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/k-matrix.pkl does not exist\")\n",
    "    K_matrix = compute_k_matrix(image_set_name=image_set_name)\n",
    "    with open(f\"data/{image_set_name}/bak/k-matrix.pkl\", 'wb') as f:\n",
    "        pickle.dump(K_matrix, f)\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/k-matrix.pkl saved successfully\")\n",
    "else:\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/k-matrix.pkl exists\")\n",
    "    with open(f\"data/{image_set_name}/bak/k-matrix.pkl\", 'rb') as f:\n",
    "        K_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Triangulation\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Triangulation starts ....\")\n",
    "if os.path.isfile(f\"data/{image_set_name}/bak/points-cloud.pkl\"):\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/points-cloud.pkl exists\")\n",
    "    with open(f\"data/{image_set_name}/bak/points-cloud.pkl\", 'rb') as f:\n",
    "        points_cloud: np.ndarray = pickle.load(f)\n",
    "    with open(f\"data/{image_set_name}/bak/camera-proj.pkl\", 'rb') as f:\n",
    "        camera_matrices: np.ndarray = pickle.load(f)\n",
    "else:\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"File data/{image_set_name}/bak/points-cloud.pkl does not exist\")\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Triangulating...\")\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Ref count of images before: {sys.getrefcount(images)}\")\n",
    "    print_size(f\"data/{image_set_name}/logs/tune.log\", images, \"images\")\n",
    "    points_cloud, camera_matrices = generate_points_cloud(images, K_matrix, image_set_name=image_set_name)\n",
    "    print_size(f\"data/{image_set_name}/logs/tune.log\", images, \"images\")\n",
    "    log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Ref count of images after: {sys.getrefcount(images)}\")\n",
    "    # Pickle the point cloud\n",
    "    with open(f\"data/{image_set_name}/bak/points-cloud.pkl\", 'wb') as f:\n",
    "        pickle.dump(points_cloud, f)\n",
    "    with open(f\"data/{image_set_name}/bak/camera-proj.pkl\", 'wb') as f:\n",
    "        pickle.dump(camera_matrices, f)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Done Point Cloud Step...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean memory before Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Ref count of images before: {sys.getrefcount(images)}\")\n",
    "print_size(f\"data/{image_set_name}/logs/tune.log\", images, \"images\")\n",
    "print_size(f\"data/{image_set_name}/logs/tune.log\", points_cloud, \"points_cloud\")\n",
    "images = None\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", gc.collect())\n",
    "print_size(f\"data/{image_set_name}/logs/tune.log\", images, \"images\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Reference count<images>: {sys.getrefcount(images)}\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", gc.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"started clustring....\")\n",
    "import hdbscan\n",
    "start_time = time.time()\n",
    "hdbscan_model = hdbscan.HDBSCAN().fit(points_cloud)\n",
    "end_time = time.time()\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"time taken: {end_time - start_time:,} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/{image_set_name}/bak/hdbscan-model.pkl\", 'wb') as f:\n",
    "    pickle.dump(hdbscan_model, f)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"File hdbscan-model.pkl saved successfully...\")\n",
    "print_size(f\"data/{image_set_name}/logs/tune.log\", hdbscan_model, \"hdbscan_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster labels for each point\n",
    "labels = hdbscan_model.labels_\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Labels Done...\")\n",
    "\n",
    "# Get the indices of the core points (i.e., points that are part of a dense region)\n",
    "core_indices = np.where(labels != -1)[0]\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Core Indicies Done...\")\n",
    "\n",
    "# Get the coordinates of the core points\n",
    "core_points = points_cloud[core_indices, :]\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Core Points Done...\")\n",
    "\n",
    "# Get the indices of the outlier points (i.e., points that are not part of any dense region)\n",
    "outlier_indices = np.where(labels == -1)[0]\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Outlier Indicies Done...\")\n",
    "\n",
    "# Get the coordinates of the outlier points\n",
    "outlier_points = points_cloud[outlier_indices, :]\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Outlier Points Done...\")\n",
    "\n",
    "# Log the number of clusters and the number of outlier points\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Number of clusters: {len(np.unique(labels))-1:,}\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Number of core points: {len(core_indices):,}\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Number of outlier points: {len(outlier_indices):,}\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Number of total points: {len(core_indices) + len(outlier_indices):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/{image_set_name}/bak/core-points.pkl\", 'wb') as f:\n",
    "    pickle.dump(core_points, f)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"File core-points.pkl saved successfully...\")\n",
    "print_size(f\"data/{image_set_name}/logs/tune.log\", core_points, \"core_points\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"points_cloud.shape: {points_cloud.shape[0]:,}\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Number of cameras detected: {len(camera_matrices):,}\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"core_points.shape: {core_points.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Create a point cloud object\n",
    "points_cloud_stl = o3d.geometry.PointCloud()\n",
    "points_cloud_stl.points = o3d.utility.Vector3dVector(points_cloud)\n",
    "# points_cloud_stl.paint_uniform_color([0, 0, 1])  # Set the point cloud color to blue for better visibility\n",
    "\n",
    "# Create a core point cloud object\n",
    "core_points_stl = o3d.geometry.PointCloud()\n",
    "core_points_stl.points = o3d.utility.Vector3dVector(core_points)\n",
    "# core_points_stl.paint_uniform_color([0, 0, 1])  # Set the point cloud color to blue for better visibility\n",
    "\n",
    "# Loop through the camera_matrices and create a red pyramid for each camera\n",
    "camera_meshes = []\n",
    "camera_lines = []\n",
    "for camera_matrix in camera_matrices:\n",
    "    camera_mesh, camera_line = create_camera_frustum(camera_matrix, scale=0.3)\n",
    "    camera_meshes.append(camera_mesh)\n",
    "    camera_lines.append(camera_line)\n",
    "\n",
    "# Combine camera meshes, camera lines, and point cloud into a single mesh\n",
    "combined_mesh = o3d.geometry.TriangleMesh()\n",
    "for mesh in camera_meshes + camera_lines:\n",
    "    combined_mesh += mesh\n",
    "\n",
    "# Save the point cloud to a .stl file\n",
    "point_cloud_file = f\"data/{image_set_name}/output/triangulate/points_cloud.ply\"\n",
    "o3d.io.write_point_cloud(point_cloud_file, points_cloud_stl)\n",
    "\n",
    "# Save the point cloud to a .stl file\n",
    "point_cloud_file = f\"data/{image_set_name}/output/triangulate/core_points.ply\"\n",
    "o3d.io.write_point_cloud(point_cloud_file, core_points_stl)\n",
    "\n",
    "# Save the combined mesh to a .stl file\n",
    "mesh_file = f\"data/{image_set_name}/output/triangulate/camera_proj.ply\"\n",
    "o3d.io.write_triangle_mesh(mesh_file, combined_mesh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meshing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inliner Meshing\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# 1. Transforming core points cloud (numpy array) to open3d point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(core_points[:, :3])\n",
    "\n",
    "# 1.1 Estimate normals for the point cloud\n",
    "pcd.estimate_normals()\n",
    "\n",
    "# 1.2 Apply statistical outlier removal to the point cloud\n",
    "_, inlier_indices = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "inlier_pcd = pcd.select_by_index(inlier_indices)\n",
    "\n",
    "# 2. Applying Ball-Pivoting Algorithm on the inlier point cloud\n",
    "distances = inlier_pcd.compute_nearest_neighbor_distance()\n",
    "avg_dist = np.mean(distances)\n",
    "radius = 3 * avg_dist\n",
    "bpa_mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(inlier_pcd, o3d.utility.DoubleVector([radius, radius * 2]))\n",
    "\n",
    "# 3. Downsampling the mesh to an acceptable number of triangles (100,000)\n",
    "dec_mesh = bpa_mesh.simplify_quadric_decimation(100_000)\n",
    "\n",
    "# 4. Smoothing the mesh by removing any weird artifacts\n",
    "dec_mesh.remove_degenerate_triangles()\n",
    "dec_mesh.remove_duplicated_triangles()\n",
    "dec_mesh.remove_duplicated_vertices()\n",
    "dec_mesh.remove_non_manifold_edges()\n",
    "\n",
    "# 5. Exporting the mesh to a .stl file and visualizing it\n",
    "o3d.io.write_triangle_mesh(f\"data/{image_set_name}/output/triangulate/mesh.stl\", dec_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyvista as pv\n",
    "# import vtk\n",
    "# def remove_outliers_vtk(point_cloud, radius, num_neighbors):\n",
    "#     # Create the vtkRadiusOutlierRemoval filter\n",
    "#     outlier_filter = vtk.vtkRadiusOutlierRemoval()\n",
    "#     outlier_filter.SetInputData(point_cloud)\n",
    "#     outlier_filter.SetRadius(radius)\n",
    "#     outlier_filter.SetNumberOfNeighbors(num_neighbors)\n",
    "#     outlier_filter.Update()\n",
    "#     return pv.wrap(outlier_filter.GetOutput())\n",
    "\n",
    "# # Create a PolyData object from core_points\n",
    "# point_cloud = pv.PolyData(core_points)\n",
    "\n",
    "# # Remove outliers\n",
    "# radius = 2.0    # The radius to consider for the nearest neighbors search\n",
    "# num_neighbors = 15  # The minimum number of neighbors that a point should have to be considered inlier\n",
    "# filtered_point_cloud = remove_outliers_vtk(point_cloud, radius, num_neighbors)\n",
    "\n",
    "# # Reconstruct the surface\n",
    "# mesh = filtered_point_cloud.reconstruct_surface()\n",
    "\n",
    "# # Plot the mesh\n",
    "# mesh.save(f\"data/{image_set_name}/output/triangulate/mesh_test.stl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Furthur analytics on the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Analysis of X, Y, Z of Points cloud\")\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"X<{len(points_cloud[:,0]):,}>: {points_cloud[:,0].min():,} to {points_cloud[:,0].max():,}\")\n",
    "x_counter = Counter(points_cloud[:,0])\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"We have {len(x_counter):,} unique X values\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Most Common X: {x_counter.most_common(1)}, Least Two Common X: {x_counter.most_common()[:-3:-1]}\")\n",
    "# log_to_file(f\"data/{image_set_name}/logs/tune.log\", x_counter)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"-----------------------------------------------------\")\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Y<{len(points_cloud[:,1]):,}>: {points_cloud[:,1].min():,} to {points_cloud[:,1].max():,}\")\n",
    "y_counter = Counter(points_cloud[:,1])\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"We have {len(y_counter):,} unique Y values\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Most Common Y: {y_counter.most_common(1)}, Least Two Common Y: {y_counter.most_common()[:-3:-1]}\")\n",
    "# log_to_file(f\"data/{image_set_name}/logs/tune.log\", y_counter)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"-----------------------------------------------------\")\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Z<{len(points_cloud[:,2]):,}>: {points_cloud[:,2].min():,} to {points_cloud[:,2].max():,}\")\n",
    "z_counter = Counter(points_cloud[:,2])\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"We have {len(z_counter):,} unique Z values\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Most Common Z: {z_counter.most_common(1)}, Least Two Common Y: {z_counter.most_common()[:-3:-1]}\")\n",
    "# log_to_file(f\"data/{image_set_name}/logs/tune.log\", z_counter)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Analysis of X, Y, Z of Core Points\")\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"X<{len(core_points[:,0]):,}>: {core_points[:,0].min():,} to {core_points[:,0].max():,}\")\n",
    "x_counter = Counter(core_points[:,0])\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"We have {len(x_counter):,} unique X values\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Most Common X: {x_counter.most_common(1)}, Least Two Common X: {x_counter.most_common()[:-3:-1]}\")\n",
    "# log_to_file(f\"data/{image_set_name}/logs/tune.log\", x_counter)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"-----------------------------------------------------\")\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Y<{len(core_points[:,1]):,}>: {core_points[:,1].min():,} to {core_points[:,1].max():,}\")\n",
    "y_counter = Counter(core_points[:,1])\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"We have {len(y_counter):,} unique Y values\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Most Common Y: {y_counter.most_common(1)}, Least Two Common Y: {y_counter.most_common()[:-3:-1]}\")\n",
    "# log_to_file(f\"data/{image_set_name}/logs/tune.log\", y_counter)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"-----------------------------------------------------\")\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Z<{len(core_points[:,2]):,}>: {core_points[:,2].min():,} to {core_points[:,2].max():,}\")\n",
    "z_counter = Counter(core_points[:,2])\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"We have {len(z_counter):,} unique Z values\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Most Common Z: {z_counter.most_common(1)}, Least Two Common Y: {z_counter.most_common()[:-3:-1]}\")\n",
    "# log_to_file(f\"data/{image_set_name}/logs/tune.log\", z_counter)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"Analysis of X, Y, Z of Outliers Points\")\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"X<{len(outlier_points[:,0]):,}>: {outlier_points[:,0].min():,} to {outlier_points[:,0].max():,}\")\n",
    "x_counter = Counter(outlier_points[:,0])\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"We have {len(x_counter):,} unique X values\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Most Common X: {x_counter.most_common(1)}, Least Two Common X: {x_counter.most_common()[:-3:-1]}\")\n",
    "# log_to_file(f\"data/{image_set_name}/logs/tune.log\", x_counter)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"-----------------------------------------------------\")\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Y<{len(outlier_points[:,1]):,}>: {outlier_points[:,1].min():,} to {outlier_points[:,1].max():,}\")\n",
    "y_counter = Counter(outlier_points[:,1])\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"We have {len(y_counter):,} unique Y values\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Most Common Y: {y_counter.most_common(1)}, Least Two Common Y: {y_counter.most_common()[:-3:-1]}\")\n",
    "# log_to_file(f\"data/{image_set_name}/logs/tune.log\", y_counter)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"-----------------------------------------------------\")\n",
    "\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Z<{len(outlier_points[:,2]):,}>: {outlier_points[:,2].min():,} to {outlier_points[:,2].max():,}\")\n",
    "z_counter = Counter(outlier_points[:,2])\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"We have {len(z_counter):,} unique Z values\")\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", f\"Most Common Z: {z_counter.most_common(1)}, Least Two Common Y: {z_counter.most_common()[:-3:-1]}\")\n",
    "# log_to_file(f\"data/{image_set_name}/logs/tune.log\", z_counter)\n",
    "log_to_file(f\"data/{image_set_name}/logs/tune.log\", \"-----------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "points_cloud_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/points_cloud.ply\"\n",
    ")\n",
    "mesh_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/camera_proj.ply\"\n",
    ")\n",
    "\n",
    "point_cloud = o3d.io.read_point_cloud(points_cloud_file_path)\n",
    "mesh = o3d.io.read_triangle_mesh(mesh_file_path)\n",
    "\n",
    "o3d.visualization.draw_geometries([point_cloud, mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points Cloud with HDBScan\n",
    "import open3d as o3d\n",
    "\n",
    "points_cloud_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/core_points.ply\"\n",
    ")\n",
    "mesh_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/camera_proj.ply\"\n",
    ")\n",
    "\n",
    "point_cloud = o3d.io.read_point_cloud(points_cloud_file_path)\n",
    "mesh = o3d.io.read_triangle_mesh(mesh_file_path)\n",
    "\n",
    "o3d.visualization.draw_geometries([point_cloud, mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mesh from f\"data/{image_set_name}/output/triangulate/mesh.stl\" and visualize it\n",
    "import open3d as o3d\n",
    "\n",
    "mesh_file_path = (\n",
    "    f\"data/{image_set_name}/output/triangulate/mesh.stl\"\n",
    ")\n",
    "\n",
    "mesh = o3d.io.read_triangle_mesh(mesh_file_path)\n",
    "o3d.visualization.draw_geometries([mesh])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
