{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import uuid\n",
    "from typing import Final, Optional\n",
    "\n",
    "import cv2 as OpenCV\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial import Delaunay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalibrationError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "class IntrinsicParametersNotFoundError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, keypoints, descriptors, path):\n",
    "        self.img_id: int = img_id\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "        self.similar_images: list[tuple[Image, float]] = []\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step One: Read and Load Images\n",
    "Inputs: \n",
    "- folder_path: str\n",
    "\n",
    "Outputs:\n",
    "- images: Images\n",
    "\n",
    "Main Functions:\n",
    "1. prepare_images: read and load images from a folder into an Images object\n",
    "\n",
    "Utils Functions:\n",
    "1. dump_images: dump images to a pickle file\n",
    "2. load_images: load images from a pickle file\n",
    "\"\"\"\n",
    "\n",
    "def prepare_images(folder_path: str) -> Images:\n",
    "    \"\"\" Read and load images \"\"\"\n",
    "    images: Images = Images([], folder_path.split(\"/\")[-1])\n",
    "    files: list[str] = filter(lambda file: \".jpg\" in file, os.listdir(folder_path))\n",
    "    for i, file in enumerate(files):\n",
    "        image_path = f\"{folder_path}/{file}\"\n",
    "        rgb_image = OpenCV.cvtColor(OpenCV.imread(image_path), OpenCV.COLOR_BGR2RGB)\n",
    "        gray_image = OpenCV.cvtColor(rgb_image, OpenCV.COLOR_RGB2GRAY)\n",
    "        images.images.append(Image(i, rgb_image, gray_image, [], [], image_path))\n",
    "    return images\n",
    "\n",
    "def dump_images_bak(images_file_path: str, images: Images) -> None:\n",
    "    \"\"\" Dump images to a file \"\"\"\n",
    "    with open(images_file_path, \"wb\") as file:\n",
    "        prepare_images.dump(images, file)\n",
    "\n",
    "def load_images_bak(images_file_path: str) -> Images:\n",
    "    \"\"\" Load images from a file \"\"\"\n",
    "    with open(images_file_path, \"rb\") as file:\n",
    "        images = pickle.load(file)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Two: Feature Extraction\n",
    "Inputs:\n",
    "- images: Images\n",
    "- SIFT: OpenCV.SIFT\n",
    "\n",
    "Outputs:\n",
    "- image: Image\n",
    "--> image.keypoints: list[OpenCV.KeyPoint]\n",
    "--> image.descriptors: np.ndarray\n",
    "\n",
    "Main Functions:\n",
    "1. compute_keypoints_descriptors\n",
    "\"\"\"\n",
    "\n",
    "def compute_keypoints_descriptors(images: list[Image], SIFT: OpenCV.SIFT) -> None:\n",
    "    \"\"\"Compute keypoints and descriptors for each image in the list of images using SIFT algorithm.\n",
    "    Modifies each image in the list of images by adding its keypoints and descriptors as attributes.\n",
    "    \n",
    "    Args:\n",
    "    - images: List of images to compute keypoints and descriptors for.\n",
    "    - SIFT: OpenCV SIFT object used to detect and compute keypoints and descriptors.\n",
    "\n",
    "    Returns:\n",
    "    - None.\n",
    "    \"\"\"\n",
    "    for img in images.images:\n",
    "        keypoints: list[OpenCV.KeyPoint]\n",
    "        descriptors: np.ndarray\n",
    "        keypoints, descriptors = SIFT.detectAndCompute(img.gray_image, None)\n",
    "        img.keypoints = keypoints\n",
    "        img.descriptors = descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step Three: Image Matching\n",
    "Inputs:\n",
    "- descriptors: list[np.ndarray]\n",
    "\n",
    "Outputs:\n",
    "- centroids: np.ndarray\n",
    "- variance: np.ndarray\n",
    "- CLUSTER_COUNT: int\n",
    "- matches_ids: list[list[tuple[int, float]]]\n",
    "\n",
    "Main Functions:\n",
    "1. get_matches\n",
    "\n",
    "Utils Functions:\n",
    "1. get_matches_ids\n",
    "\n",
    "Sub Utils Functions:\n",
    "1. get_visual_words\n",
    "2. get_frequency_vectors\n",
    "3. get_tf_idf\n",
    "4. search_matches\n",
    "\"\"\"\n",
    "\n",
    "def get_matches(images: Images) -> None:\n",
    "    \"\"\" Match images using k-means clustering.\n",
    "    Args:\n",
    "        images: Obj from Images class.\n",
    "    \"\"\"\n",
    "    all_descriptors = np.concatenate([image.descriptors for image in images.images])\n",
    "    CLUSTER_COUNT: Final = 400\n",
    "    ITER: Final = 2\n",
    "    centroids, _ = kmeans(all_descriptors, CLUSTER_COUNT, ITER)\n",
    "    matches_ids: list[list[tuple[int, float]]] =  get_matches_ids([image.descriptors for image in images.images], centroids, images.images)\n",
    "    for i, image in enumerate(images.images):\n",
    "        inner_list: list[Image, float] = [\n",
    "            (images.images[match[0]], match[1]) for match in matches_ids[i]\n",
    "        ]\n",
    "        image.similar_images = inner_list\n",
    "\n",
    "\n",
    "def get_visual_words(descriptors: list[np.ndarray], centroids: np.ndarray) -> list[np.ndarray]:\n",
    "    \"\"\" Get the visual words of a list of descriptors.\n",
    "    Args:\n",
    "        descriptors: A list of numpy arrays containing image descriptors.\n",
    "        centroids: A numpy array containing cluster centroids.\n",
    "    Returns:\n",
    "        A list of numpy arrays representing the visual words of each image.\n",
    "    \"\"\"\n",
    "    visual_words = []\n",
    "    for descriptor in descriptors:\n",
    "        words, _ = vq(descriptor, centroids)\n",
    "        visual_words.append(words)\n",
    "    return visual_words\n",
    "\n",
    "\n",
    "def get_frequency_vectors(visual_words: list[np.ndarray], CLUSTER_COUNT: int) -> np.ndarray:\n",
    "    \"\"\" Get the frequency vectors for a list of visual words.\n",
    "    Args:\n",
    "        visual_words: A list of numpy arrays representing the visual words of each image.\n",
    "        CLUSTER_COUNT: The number of clusters used to generate the visual words.\n",
    "    Returns:\n",
    "        A numpy array containing the frequency vectors for each image.\n",
    "    \"\"\"\n",
    "    frequency_vectors = []\n",
    "    for img_words in visual_words:\n",
    "        histogram = np.zeros(CLUSTER_COUNT)\n",
    "        for word in img_words:\n",
    "            histogram[word] += 1\n",
    "        frequency_vectors.append(histogram)\n",
    "    return np.stack(frequency_vectors)\n",
    "\n",
    "\n",
    "def get_tf_idf(frequency_vectors, IMAGES_COUNT) -> np.ndarray:\n",
    "    \"\"\" Get the Term Frequency-Inverse Document Frequency (TF-IDF) matrix for a list of frequency vectors.\n",
    "    Args:\n",
    "        frequency_vectors: A numpy array containing the frequency vectors for each image.\n",
    "        IMAGES_COUNT: The total number of images in the dataset.\n",
    "    Returns:\n",
    "        A numpy array containing the TF-IDF matrix for the input frequency vectors.\n",
    "    \"\"\"\n",
    "    df = np.sum(frequency_vectors > 0, axis = 0)\n",
    "    idf = np.log(IMAGES_COUNT/df)\n",
    "    return frequency_vectors * idf\n",
    "\n",
    "\n",
    "def search_matches(i, top_clusters, tf_idf) -> list[tuple[int, float]]:\n",
    "    \"\"\" Search for the top_clusters most similar images to the i-th image\n",
    "    Args:\n",
    "        i: the index of the image to search for similar images\n",
    "        top_clusters: the number of similar images to return\n",
    "        tf_idf: Term Frequency-Inverse Document Frequency\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains the index of a similar image and the cosine similarity \n",
    "        between the i-th image and the similar image. The list is sorted by the cosine similarity in \n",
    "        descending order.\n",
    "    \"\"\"\n",
    "    b = tf_idf\n",
    "    a = tf_idf[i]\n",
    "    b_subset = b[:tf_idf.shape[0]]\n",
    "    cosine_similarity = np.dot(a, b_subset.T)/(norm(a) * norm(b_subset, axis=1))\n",
    "    idx = np.argsort(-cosine_similarity)[:top_clusters]\n",
    "    return list(zip(idx, cosine_similarity[idx]))\n",
    "\n",
    "\n",
    "def get_matches_ids(descriptors, centroids, images_list) -> list[list[tuple[int, float]]]:\n",
    "    \"\"\"Returns: a list of lists, where each list contains the top 10 most similar images to the i-th image.\"\"\"\n",
    "    visual_words = get_visual_words(descriptors, centroids)\n",
    "    frequency_vectors = get_frequency_vectors(visual_words, centroids.shape[0])\n",
    "    \"\"\" tf_idf: Term Frequency-Inverse Document Frequency \"\"\"\n",
    "    tf_idf = get_tf_idf(frequency_vectors, len(images_list))\n",
    "    return [\n",
    "        search_matches(i, 10, tf_idf)\n",
    "        for i in range(len(images_list))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Four: Feature Matching\n",
    "Inputs:\n",
    "- images: Images\n",
    "\n",
    "Outputs:\n",
    "- None\n",
    "\n",
    "Main Functions:\n",
    "1. data_feature_matching\n",
    "\n",
    "Utils Functions:\n",
    "1. feature_matching\n",
    "\"\"\"\n",
    "\n",
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    \"\"\" Match features between two images using Brute Force Matcher\n",
    "    Args:\n",
    "        img_id_one: the index of the first image\n",
    "        img_id_two: the index of the second image\n",
    "        descriptors: a list of descriptors of the images\n",
    "    Returns:\n",
    "        A list of OpenCV.DMatch objects.\n",
    "    \"\"\"\n",
    "    matcher = OpenCV.BFMatcher()\n",
    "    return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "def data_feature_matching(images: Images) -> None:\n",
    "    \"\"\" Match features between images using Brute Force Matcher\n",
    "    Args:\n",
    "        matchesIDs: a list of lists of tuples, where each tuple contains the index of a similar image and the cosine similarity \n",
    "            between the i-th image and the similar image. The list is sorted by the cosine similarity in \n",
    "            descending order.\n",
    "        descriptors: a list of descriptors of the images\n",
    "    Returns:\n",
    "        A list of lists, where each list contains \n",
    "        the index of the first image, the index of the second image, \n",
    "        and a list of OpenCV.DMatch objects.\n",
    "    \"\"\"\n",
    "    num_images: int = len(images.images)\n",
    "    checked = np.zeros((num_images, num_images), dtype=int)\n",
    "    for image in images.images:\n",
    "        for matched_image, probability in image.similar_images:\n",
    "            if ((checked[image.img_id][matched_image.img_id] == 0 or checked[matched_image.img_id][image.img_id] == 0) and image.img_id != matched_image.img_id and probability > 0.93):\n",
    "                images.feature_matches.append(FeatureMatches(image, matched_image, feature_matching(image.descriptors, matched_image.descriptors)))\n",
    "                checked[image.img_id][matched_image.img_id], checked[matched_image.img_id][image.img_id] = 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Step Five: Camera Calibration\\nInputs:\\n- None.\\n\\nOutputs:\\n- k_matrix: np.ndarray\\n\\nToDo:\\n1- generate K_matrix.pickle for each camera using Chess board pattern.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Step Five: Camera Calibration\n",
    "Inputs:\n",
    "- None.\n",
    "\n",
    "Outputs:\n",
    "- k_matrix: np.ndarray\n",
    "\n",
    "ToDo:\n",
    "1- generate K_matrix.pickle for each camera using Chess board pattern.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step Six: Triangulation (3D Reconstruction)\n",
    "Inputs:\n",
    "- feature_matches_list: list[list[int, int, list[OpenCV.DMatch]]]\n",
    "    -> A list of lists, where each list contains \n",
    "        the index of the first image, the index of the second image, \n",
    "        and a list of OpenCV.DMatch objects.\n",
    "- K_matrix: np.ndarray\n",
    "    -> The camera matrix of the camera used to take the images.\n",
    "\n",
    "Outputs:\n",
    "- point_cloud: list[np.ndarray]; each element is a 3D point.\n",
    "\n",
    "Main Functions:\n",
    "1. generate_point_cloud\n",
    "\n",
    "Utils Functions:\n",
    "1. triangulatePoints\n",
    "\"\"\"\n",
    "\n",
    "def triangulatePoints(P1, P2, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Triangulates the given matching points from two images using the given camera matrices.\n",
    "\n",
    "    Parameters:\n",
    "    P1 (numpy.ndarray): 3x4 camera matrix of the first image.\n",
    "    P2 (numpy.ndarray): 3x4 camera matrix of the second image.\n",
    "    pts1 (numpy.ndarray): Nx2 matrix containing the coordinates of matching points in the first image.\n",
    "    pts2 (numpy.ndarray): Nx2 matrix containing the coordinates of matching points in the second image.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Nx3 matrix containing the triangulated 3D points.\n",
    "    \"\"\"\n",
    "    pts4D = OpenCV.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    pts4D /= pts4D[3]\n",
    "    return pts4D[:3].T\n",
    "\n",
    "\n",
    "def generate_point_cloud(images: Images, K_matrix):\n",
    "    \"\"\"\n",
    "    Generates a cloud of 3D points using triangulation from feature matches and camera calibration matrix.\n",
    "\n",
    "    Parameters:\n",
    "    feature_matches_list (list): List of feature matches between images.\n",
    "    K_matrix (numpy.ndarray): 3x3 camera calibration matrix.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Nx3 matrix containing the cloud of 3D points.\n",
    "    \"\"\"\n",
    "    point_cloud = []\n",
    "    feature_matches_list = images.feature_matches\n",
    "    for match in feature_matches_list:\n",
    "        img_one = match.image_one\n",
    "        img_two = match.image_two\n",
    "        matches = match.matches\n",
    "        pts1 = np.float32([img_one.keypoints[m.queryIdx].pt for m in matches])\n",
    "        pts2 = np.float32([img_two.keypoints[m.trainIdx].pt for m in matches])\n",
    "        E, _ = OpenCV.findEssentialMat(pts1, pts2, K_matrix)\n",
    "        R1, R2, t = OpenCV.decomposeEssentialMat(E)\n",
    "        P1 = np.hstack((np.eye(3), np.zeros((3,1))))\n",
    "        P2 = np.hstack((R1, t))\n",
    "        pts_3d = triangulatePoints(P1, P2, pts1, pts2)\n",
    "        point_cloud.append(pts_3d)\n",
    "    return np.concatenate(point_cloud, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome ScanMate...\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome ScanMate...\")\n",
    "image_set_name = \"snow-man\"\n",
    "images: Optional[Images] = None\n",
    "# Reload the last state\n",
    "last_state: str\n",
    "if os.path.isfile(f\"bak/{image_set_name}/point-cloud.pkl\"):\n",
    "    last_state = \"Point Cloud Step\"\n",
    "elif os.path.isfile(f\"bak/{image_set_name}/feature-matching-output.pkl\"):\n",
    "    last_state = \"Feature Matching Step\"\n",
    "elif os.path.isfile(f\"bak/{image_set_name}/images-matched.pkl\"):\n",
    "    last_state = \"Images Matching Step\"\n",
    "elif os.path.isfile(f\"bak/{image_set_name}/sift-features.pkl\"):\n",
    "    last_state = \"SIFT Features Step\"\n",
    "else:\n",
    "    last_state = \"Images Loading Step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and prepare Images\n",
    "if last_state == \"Images Loading Step\":\n",
    "    if os.path.isfile(f\"bak/{image_set_name}/images.pkl\"):\n",
    "        print(f\"File [bak/{image_set_name}/sift-images.pkl] exists\")\n",
    "        print(\"Loading images from pickle file...\")\n",
    "        images: Images = load_images_bak(f\"bak/{image_set_name}/images.pkl\")\n",
    "    else:\n",
    "        print(f\"File [bak/{image_set_name}/images.pkl] does not exist\")\n",
    "        print(\"Loading images from images directory...\")\n",
    "        images: Images = prepare_images(f\"images/{image_set_name}\")\n",
    "        print(\"Saving images to pickle file...\")\n",
    "        dump_images_bak(f\"bak/{image_set_name}/images.pkl\", images)\n",
    "    print(\"Images loaded successfully\")\n",
    "    last_state = \"SIFT Features Step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Extraction: SIFT\n",
    "if last_state == \"SIFT Features Step\":\n",
    "    if os.path.isfile(f\"bak/{image_set_name}/sift-features.pkl\"):\n",
    "        print(f\"File [bak/{image_set_name}/sift-features.pkl] exists\")\n",
    "        if images: \n",
    "            del images\n",
    "        images: Images = load_images_bak(f\"bak/{image_set_name}/sift-features.pkl\")\n",
    "    else:\n",
    "        print(\"File [bak/{image_set_name}/sift-features.pkl] DO NOT exists\")\n",
    "        print(\"Extracting SIFT features...\")\n",
    "        sift = OpenCV.SIFT_create()\n",
    "        compute_keypoints_descriptors(images, sift)\n",
    "        dump_images_bak(f\"bak/{image_set_name}/sift-features.pkl\", images)\n",
    "    print(\"Feature Extraction: SIFT DONE...\")\n",
    "    last_state = \"Images Matching Step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Image Matching\n",
    "if last_state == \"Images Matching Step\":\n",
    "    if os.path.isfile(f\"bak/{image_set_name}/images-matched.pkl\"):\n",
    "        print(f\"File [bak/{image_set_name}/images-matched.pkl] exists\")\n",
    "        if images: \n",
    "            del images\n",
    "        images: Images = load_images_bak(f\"bak/{image_set_name}/images-matched.pkl\")\n",
    "    else:\n",
    "        print(f\"File [bak/{image_set_name}/images-matched.pkl] DO NOT exists\")\n",
    "        get_matches(images)\n",
    "        dump_images_bak(f\"bak/{image_set_name}/images-matched.pkl\", images)\n",
    "    print(\"Done Image Matching Step...\")\n",
    "    last_state = \"Feature Matching Step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Matching\n",
    "if last_state == \"Feature Matching Step\":\n",
    "    if os.path.isfile(f\"bak/{image_set_name}/feature-matching-output.pkl\"):\n",
    "        print(f\"File [bak/{image_set_name}/feature-matching-output.pkl] exists\")\n",
    "        if images: \n",
    "            del images\n",
    "        images: Images = load_images_bak(f\"bak/{image_set_name}/feature-matching-output.pkl\")\n",
    "    else:\n",
    "        print(\"File [bak/{image_set_name}/feature-matching-output.pkl] Do NOT exists\")\n",
    "        # logging.info('----> Processing {image_set_name}...')\n",
    "        data_feature_matching(images)\n",
    "        dump_images_bak(f\"bak/{image_set_name}/feature-matching-output.pkl\", images)\n",
    "    print(\"Done Feature Matching Step...\")\n",
    "    last_state = \"Point Cloud Step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Calibration starts ....\n",
      "File bak/snow-man/checker/K_matrix.pickle exists\n"
     ]
    }
   ],
   "source": [
    "# 5. Camera Calibration\n",
    "print(\"Camera Calibration starts ....\")\n",
    "if not os.path.isfile(\"bak/snow-man/checker/K_matrix.pickle\"):\n",
    "    raise IntrinsicParametersNotFoundError(\"Intrinsic parameters not found\")\n",
    "print(\"File bak/snow-man/checker/K_matrix.pickle exists\")\n",
    "with open('bak/snow-man/checker/K_matrix.pickle', 'rb') as f:\n",
    "    K_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triangulation starts ....\n"
     ]
    }
   ],
   "source": [
    "# 6. Triangulation (3D reconstruction)\n",
    "print(\"Triangulation starts ....\")\n",
    "if last_state == \"Point Cloud Step\":\n",
    "    if os.path.isfile(f\"bak/{image_set_name}/point-cloud.pkl\"):\n",
    "        with open(f\"bak/{image_set_name}/point-cloud.pkl\", 'rb') as f:\n",
    "            points_cloud: np.ndarray = pickle.load(f)\n",
    "    else:\n",
    "        points_cloud: np.ndarray = generate_point_cloud(images, K_matrix)\n",
    "        # Pickle the point cloud\n",
    "        with open(f\"bak/{image_set_name}/point-cloud.pkl\", 'wb') as f:\n",
    "            pickle.dump(points_cloud, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points_cloud.shape: (17264381, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"points_cloud.shape: {points_cloud.shape}\")\n",
    "# Save the point cloud to a file\n",
    "# np.savetxt(f\"output/{image_set_name}/point-cloud.txt\", points_cloud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot points_cloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(points_cloud[:,0], points_cloud[:,1], points_cloud[:,2])\n",
    "\n",
    "# ax.set_xlabel('X Label')\n",
    "# ax.set_ylabel('Y Label')\n",
    "# ax.set_zlabel('Z Label')\n",
    "\n",
    "# plt.show()\n",
    "# plt.savefig('points_histo_plt.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statics about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Log to tune.log\n",
    "def log_tune(file_name: str, message: str):\n",
    "    with open(file_name, \"a\") as f:\n",
    "        f.write(f\"{message}\\n\")\n",
    "\n",
    "log_tune(\"tune-before.log\", f\"X<{len(points_cloud[:,0]):,}>: {points_cloud[:,0].min():,} to {points_cloud[:,0].max():,}\")\n",
    "x_counter = Counter(points_cloud[:,0])\n",
    "log_tune(\"tune-before.log\", f\"We have {len(x_counter):,} unique Y values\")\n",
    "log_tune(\"tune-before.log\", f\"Most Common X: {x_counter.most_common(1)}, Least Two Common X: {x_counter.most_common()[:-3:-1]}\")\n",
    "# log_tune(\"tune-before.log\", x_counter)\n",
    "log_tune(\"tune-before.log\", \"-----------------------------------------------------\")\n",
    "log_tune(\"tune-before.log\", f\"Y<{len(points_cloud[:,1]):,}>: {points_cloud[:,1].min():,} to {points_cloud[:,1].max():,}\")\n",
    "y_counter = Counter(points_cloud[:,1])\n",
    "log_tune(\"tune-before.log\", f\"We have {len(y_counter):,} unique Y values\")\n",
    "log_tune(\"tune-before.log\", f\"Most Common Y: {y_counter.most_common(1)}, Least Two Common Y: {y_counter.most_common()[:-3:-1]}\")\n",
    "# log_tune(\"tune-before.log\", y_counter)\n",
    "log_tune(\"tune-before.log\", \"-----------------------------------------------------\")\n",
    "log_tune(\"tune-before.log\", f\"Z<{len(points_cloud[:,2]):,}>: {points_cloud[:,2].min():,} to {points_cloud[:,2].max():,}\")\n",
    "z_counter = Counter(points_cloud[:,2])\n",
    "log_tune(\"tune-before.log\", f\"We have {len(z_counter):,} unique Z values\")\n",
    "log_tune(\"tune-before.log\", f\"Most Common Z: {z_counter.most_common(1)}, Least Two Common Y: {z_counter.most_common()[:-3:-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_points_cloud = [\n",
    "    point\n",
    "    for point in points_cloud\n",
    "    if -10 <= point[0] <= 10\n",
    "    and -10 <= point[1] <= 10\n",
    "    and -10 <= point[2] <= 10\n",
    "]\n",
    "\n",
    "log_tune(\"tune-after.log\", f\"Now we have {len(new_points_cloud):,} points\")\n",
    "new_points_cloud = np.array(new_points_cloud)\n",
    "\n",
    "log_tune(\"tune-after.log\", f\"X<{len(new_points_cloud[:,0]):,}>: {new_points_cloud[:,0].min():,} to {new_points_cloud[:,0].max():,}\")\n",
    "new_x_counter = Counter(new_points_cloud[:,0])\n",
    "log_tune(\"tune-after.log\", f\"We have {len(new_x_counter):,} unique Y values\")\n",
    "log_tune(\"tune-after.log\", f\"Most Common X: {new_x_counter.most_common(1)}, Least Two Common X: {new_x_counter.most_common()[:-3:-1]}\")\n",
    "# log_tune(\"tune-after.log\", x_counter)\n",
    "log_tune(\"tune-after.log\", \"-----------------------------------------------------\")\n",
    "log_tune(\"tune-after.log\", f\"Y<{len(new_points_cloud[:,1]):,}>: {new_points_cloud[:,1].min():,} to {new_points_cloud[:,1].max():,}\")\n",
    "new_y_counter = Counter(new_points_cloud[:,1])\n",
    "log_tune(\"tune-after.log\", f\"We have {len(new_y_counter):,} unique Y values\")\n",
    "log_tune(\"tune-after.log\", f\"Most Common Y: {new_y_counter.most_common(1)}, Least Two Common Y: {new_y_counter.most_common()[:-3:-1]}\")\n",
    "# log_tune(\"tune-after.log\", y_counter)\n",
    "log_tune(\"tune-after.log\", \"-----------------------------------------------------\")\n",
    "log_tune(\"tune-after.log\", f\"Z<{len(new_points_cloud[:,2]):,}>: {new_points_cloud[:,2].min():,} to {new_points_cloud[:,2].max():,}\")\n",
    "new_z_counter = Counter(new_points_cloud[:,2])\n",
    "log_tune(\"tune-after.log\", f\"We have {len(new_z_counter):,} unique Z values\")\n",
    "log_tune(\"tune-after.log\", f\"Most Common Z: {new_z_counter.most_common(1)}, Least Two Common Y: {new_z_counter.most_common()[:-3:-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# # Cluster the points using DBSCAN algorithm\n",
    "# dbscan = DBSCAN(eps=0.5, min_samples=10).fit(points_cloud)\n",
    "\n",
    "# # Get the cluster labels for each point\n",
    "# labels = dbscan.labels_\n",
    "\n",
    "# # Get the indices of the core points (i.e., points that are part of a dense region)\n",
    "# core_indices = np.where(labels != -1)[0]\n",
    "\n",
    "# # Get the coordinates of the core points\n",
    "# core_points = points_cloud[core_indices, :]\n",
    "\n",
    "# # Get the indices of the outlier points (i.e., points that are not part of any dense region)\n",
    "# outlier_indices = np.where(labels == -1)[0]\n",
    "\n",
    "# # Get the coordinates of the outlier points\n",
    "# outlier_points = points_cloud[outlier_indices, :]\n",
    "\n",
    "# # Print the number of clusters and the number of outlier points\n",
    "# print(\"Number of clusters:\", len(np.unique(labels)) - 1)\n",
    "# print(\"Number of outlier points:\", len(outlier_indices))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the histogram of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_PINS: Final[int] = 15_553_866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 14,197,966 unique X values\n",
      "Most Common X: [(0.5158348, 7)], Least Two Common X: [(-0.5582578, 1), (-0.47064885, 1)]\n",
      "Max X: 9.999974250793457, Min X: -9.999930381774902\n"
     ]
    }
   ],
   "source": [
    "# x_counter = {k: v for k, v in x_counter.items() if -4 < k < 4}\n",
    "# new_x_counter = Counter(x_counter)\n",
    "print(f\"We have {len(new_x_counter):,} unique X values\")\n",
    "print(f\"Most Common X: {new_x_counter.most_common(1)}, Least Two Common X: {new_x_counter.most_common()[:-3:-1]}\")\n",
    "print(f\"Max X: {max(new_x_counter)}, Min X: {min(new_x_counter)}\")\n",
    "plt.hist(list(new_x_counter.keys()), weights=list(new_x_counter.values()), bins=NUM_PINS, label='x-axis')\n",
    "plt.title('X Points Cloud Histogram')\n",
    "plt.xlabel('Point X Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('x_points_histo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot points_cloud\n",
    "# import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(new_points_cloud[:,0], new_points_cloud[:,1], new_points_cloud[:,2])\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('points_histo_plt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_counter = {k: v for k, v in y_counter.items() if -10 < k < 10}\n",
    "# y_counter = Counter(y_counter)\n",
    "print(f\"We have {len(new_y_counter):,} unique Y values\")\n",
    "print(f\"Most Common Y: {new_y_counter.most_common(1)}, Least Two Common Y: {new_y_counter.most_common()[:-3:-1]}\")\n",
    "print(f\"Max Y: {max(new_y_counter)}, Min Y: {min(new_y_counter)}\")\n",
    "plt.hist(list(new_y_counter.keys()), weights=list(new_y_counter.values()), bins=NUM_PINS, label='y-axis')\n",
    "plt.title('Y Points Cloud Histogram')\n",
    "plt.xlabel('Point Y Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('y_points_histo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_counter = {k: v for k, v in z_counter.items() if -0.6 < k < 0.6}\n",
    "# z_counter = Counter(z_counter)\n",
    "print(f\"We have {len(new_z_counter):,} unique Z values\")\n",
    "print(f\"Most Common Z: {new_z_counter.most_common(1)}, Least Two Common Z: {new_z_counter.most_common()[:-3:-1]}\")\n",
    "print(f\"Max Z: {max(new_z_counter)}, Min Z: {min(new_z_counter)}\")\n",
    "plt.hist(list(new_z_counter.keys()), weights=list(new_z_counter.values()), bins=NUM_PINS, label='z-axis')\n",
    "plt.title('Y Points Cloud Histogram')\n",
    "plt.xlabel('Point Z Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('z_points_histo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_to_file(filename: str, message: str):\n",
    "#     with open(filename, \"a\") as f:\n",
    "#         f.write(f\"{message}\\n\")\n",
    "\n",
    "# def print_ascii_bar_chart(data, filename, symbol=\"#\"):\n",
    "#     counter = Counter(data).most_common()\n",
    "#     chart = {category: symbol * frequency for category, frequency in counter}\n",
    "#     max_len = max(len(category) for category in chart)\n",
    "#     for category, frequency in chart.items():\n",
    "#         padding = (max_len - len(category)) * \" \"\n",
    "#         log_to_file(filename, f\"{category}{padding} |{frequency}\")\n",
    "# print_ascii_bar_chart(points_cloud[:,0], \"x_values_freq.log\", symbol=\"+\")\n",
    "# print_ascii_bar_chart(points_cloud[:,1], \"y_values_freq.log\", symbol=\"+\")\n",
    "# print_ascii_bar_chart(points_cloud[:,2], \"z_values_freq.log\", symbol=\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig, axs = plt.subplots(1, 3, sharey=True, tight_layout=True)\n",
    "# axs[0].hist(points_cloud[:, 0], bins=17_000_000)\n",
    "# axs[1].hist(points_cloud[:, 1], bins=17_000_000)\n",
    "# axs[2].hist(points_cloud[:, 2], bins=17_000_000)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Generate point cloud\n",
    "# # point_cloud = np.random.rand(10000, 3)\n",
    "\n",
    "# # Define scaling factor\n",
    "# scale_factor = 2.0\n",
    "\n",
    "# # Scale the point cloud\n",
    "# # scaled_point_cloud = scaled_point_cloud * scale_factor\n",
    "\n",
    "# # Plot histogram of scaled_point_cloud\n",
    "# plt.hist(points_cloud.flatten(), bins=17_000_000, alpha=0.5, label='Before Scaling')\n",
    "# # Plot histogram of scaled_point_cloud\n",
    "# # plt.hist(scaled_point_cloud.flatten(), bins=50, alpha=0.5, label='After Scaling')\n",
    "\n",
    "# # Add titles and labels to the plot\n",
    "# plt.title('Point Cloud Histogram')\n",
    "# plt.xlabel('Point Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend(loc='upper right')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END of Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(new_points_cloud[:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it as a.STL file\n",
    "o3d.io.write_point_cloud(\"point_cloud.ply\", pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3d.visualization.draw_geometries([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
