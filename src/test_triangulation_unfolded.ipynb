{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangulate Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from typing import Final, Optional\n",
    "\n",
    "import cv2 as OpenCV\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial import Delaunay\n",
    "import cv2\n",
    "from scipy.optimize import least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    def __init__(self, img_id, rgb_image, gray_image, mask, keypoints, descriptors, path):\n",
    "        self.img_id: int = int(img_id)\n",
    "        self.unique_id: uuid = uuid.uuid4()\n",
    "        self.rgb_image: Image = rgb_image\n",
    "        self.gray_image: Image = gray_image\n",
    "        self.mask: Image = mask\n",
    "        self.keypoints: list[OpenCV.KeyPoint] = keypoints\n",
    "        self.descriptors: np.ndarray = descriptors\n",
    "        self.path: str = path\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return f\"{len(self.keypoints)}\" if len(self.keypoints) == len(self.descriptors) else f\"{len(self.keypoints)}, {len(self.descriptors)}\"\n",
    "    \n",
    "    def draw_sift_features(self):\n",
    "        image_with_sift = OpenCV.drawKeypoints(self.rgb_image, self.keypoints, None, flags=OpenCV.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        plt.imshow(image_with_sift)\n",
    "        plt.title(\"Image with SIFT Features\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_rgb_image(self, title: Optional[str] = None):\n",
    "        image = self.rgb_image\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def display_gray_image(self, title: Optional[str] = None):\n",
    "        image = self.gray_image\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_mask_image(self, title: Optional[str] = None):\n",
    "        image = self.mask\n",
    "        plt.gray()\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        # plt.axes('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def display_dialated_image(self, title: Optional[str] = None):\n",
    "        print(self.mask.shape)\n",
    "        print(self.rgb_image.shape)\n",
    "        image = OpenCV.bitwise_and(self.rgb_image, self.rgb_image, mask=self.mask)\n",
    "        plt.imshow(image)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        # plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Image({self.img_id})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.unique_id == other.unique_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.img_id)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['keypoints'] = [tuple(k.pt) + (k.size, k.angle, k.response, k.octave, k.class_id) for k in self.keypoints]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['keypoints'] = [OpenCV.KeyPoint(x, y, size, angle, response, octave, class_id) for x, y, size, angle, response, octave, class_id in state['keypoints']]\n",
    "        self.__dict__ = state\n",
    "\n",
    "class FeatureMatches:\n",
    "    def __init__(self, image_one: Image, image_two: Image, matches: list[OpenCV.DMatch]):\n",
    "        self.image_one: Image = image_one\n",
    "        self.image_two: Image = image_two\n",
    "        self.matches: list[OpenCV.DMatch] = matches\n",
    "\n",
    "    def draw_matches(self, output_filename: str) -> None:\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "\n",
    "        for match in self.matches:\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "\n",
    "        OpenCV.imwrite(output_filename, combined_image)\n",
    "        \n",
    "    def simulate_matches(self, output_dir: str) -> None:\n",
    "        combined_image = OpenCV.hconcat([\n",
    "            self.image_one.rgb_image,\n",
    "            self.image_two.rgb_image\n",
    "        ])\n",
    "\n",
    "        for i, match in enumerate(self.matches):\n",
    "            x1, y1 = self.image_one.keypoints[match.queryIdx].pt\n",
    "            x2, y2 = self.image_two.keypoints[match.trainIdx].pt\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            OpenCV.line(\n",
    "                combined_image, \n",
    "                (int(x1), int(y1)), \n",
    "                (int(x2) + self.image_one.rgb_image.shape[1], int(y2)), \n",
    "                (0, 255, 0), \n",
    "                1\n",
    "            )\n",
    "            if i % 5 == 0:\n",
    "                resized_image = OpenCV.resize(combined_image, None, fx=0.5, fy=0.5, interpolation=OpenCV.INTER_AREA)\n",
    "                OpenCV.imwrite(f\"{output_dir}/{i}.jpg\", resized_image)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeatureMatches({self.image_one}, {self.image_two} ---> {len(self.matches)})\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['matches'] = [\n",
    "            {'queryIdx': m.queryIdx, 'trainIdx': m.trainIdx, 'distance': m.distance} for m in self.matches\n",
    "        ]\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        state['matches'] = [\n",
    "            OpenCV.DMatch(match['queryIdx'], match['trainIdx'], match['distance']) for match in state['matches']\n",
    "        ]\n",
    "        self.__dict__ = state\n",
    "    \n",
    "class Images:\n",
    "    def __init__(self, images: list[Image], image_set_name: str):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.images: list[Image] = images\n",
    "        self.image_set_name: str = image_set_name\n",
    "        self.feature_matches: list[FeatureMatches] = []\n",
    "        self.similar_images: dict[list[Image]] = {}\n",
    "        self.num_clusters: int = 50\n",
    "\n",
    "    def save_feature_matches(self):\n",
    "        for match in self.feature_matches:\n",
    "            match.draw_matches(f\"data/{self.image_set_name}/output/feature-match/{match.image_one.img_id}_{match.image_two.img_id}.jpg\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def display_similar_images(self, key):\n",
    "        print(f\"cluster {key}\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        for value in self.similar_images[key]:\n",
    "            print(value)\n",
    "            rgb_image = OpenCV.cvtColor(OpenCV.imread(value.path), OpenCV.COLOR_BGR2RGB)\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.title(value.path)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    def save_similar_images(self):\n",
    "        for cluster in self.similar_images.keys():\n",
    "            if not os.path.exists(f\"data/{self.image_set_name}/output/image-match/{cluster}\"):\n",
    "                os.makedirs(f\"data/{self.image_set_name}/output/image-match/{cluster}\")\n",
    "            for value in self.similar_images[cluster]:\n",
    "                OpenCV.imwrite(f\"data/{self.image_set_name}/output/image-match/{cluster}/{value.img_id}.jpg\", value.rgb_image)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Image:\n",
    "        for image in self.images:\n",
    "            if image.img_id == key:\n",
    "                return image\n",
    "        raise KeyError(f'Image with img_id {key} not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matching(\n",
    "        img_one_descriptors: np.ndarray, \n",
    "        img_two_descriptors: np.ndarray,\n",
    "    ) -> list[OpenCV.DMatch]:\n",
    "    matcher = OpenCV.BFMatcher(crossCheck=True)\n",
    "    return matcher.match(img_one_descriptors, img_two_descriptors)\n",
    "\n",
    "\n",
    "def apply_ransac(matches, keypoints1, keypoints2, threshold = 3.0):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    _, mask = OpenCV.findHomography(src_pts, dst_pts, OpenCV.RANSAC, threshold)\n",
    "    matches_mask = mask.ravel().tolist()\n",
    "    return [m for m, keep in zip(matches, matches_mask) if keep]\n",
    "\n",
    "\n",
    "def data_feature_matching(images: Images) -> None:\n",
    "    import itertools\n",
    "    for _, values in images.similar_images.items():\n",
    "        print(images.similar_images.items())\n",
    "        for image, matched_image in itertools.combinations(values, 2):\n",
    "            feature_matching_output = feature_matching(image.descriptors, matched_image.descriptors)\n",
    "            ransac_output = apply_ransac(feature_matching_output, image.keypoints, matched_image.keypoints, threshold=150)\n",
    "            images.feature_matches.append(FeatureMatches(image, matched_image, ransac_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as OpenCV\n",
    "\n",
    "def generate_point_cloud(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    def remove_outliers(points_3D: np.ndarray, threshold: float = 100.0) -> np.ndarray:\n",
    "        # Calculate the mean and standard deviation of the 3D points\n",
    "        mean = points_3D.mean(axis=0)\n",
    "        std_dev = points_3D.std(axis=0)\n",
    "\n",
    "        # Remove points that are farther than the threshold * standard deviation from the mean\n",
    "        inliers = np.all(np.abs(points_3D - mean) < threshold * std_dev, axis=1)\n",
    "\n",
    "        return points_3D[inliers]\n",
    "\n",
    "    point_cloud = []\n",
    "    print(\"Feature Matches No. of Pairs: \", len(images.feature_matches))\n",
    "    for i, feature_match in enumerate(images.feature_matches):\n",
    "        image_one = feature_match.image_one\n",
    "        image_two = feature_match.image_two\n",
    "\n",
    "        # Extract matched keypoints\n",
    "        keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "        keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "\n",
    "        # Estimate the essential matrix\n",
    "        E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K_matrix, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n",
    "        _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K_matrix)\n",
    "\n",
    "        print(f\"R of Pair {i}: \", R)\n",
    "        print(f\"t of Pair {i}: \", t)\n",
    "        print(f\"Essential Matrix of Pair {i}: \", E)\n",
    "        print(\"Keypoint_1 Shape: \", keypoints_one.shape)\n",
    "        print(\"Keypoint_2 Shape: \", keypoints_two.shape)\n",
    "        print(\"\\n\")\n",
    "        # Create projection matrices\n",
    "        P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1)))) #P1 Shape: (3, 4)\n",
    "        P2 = K_matrix @ np.hstack((R, t)) #P2 Shape: (3, 4)\n",
    "\n",
    "        # Triangulate points\n",
    "        points_4D = OpenCV.triangulatePoints(P1, P2, keypoints_one.T, keypoints_two.T)\n",
    "        points_3D = (points_4D / points_4D[3])[:3].T\n",
    "\n",
    "        # Remove outliers in the triangulated points\n",
    "        points_3D = remove_outliers(points_3D)\n",
    "\n",
    "        point_cloud.append(points_3D)\n",
    "\n",
    "    # Merge all point clouds into one\n",
    "    point_cloud = np.vstack(point_cloud)\n",
    "\n",
    "    return point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Main Code ##################\n",
    "\n",
    "# def generate_point_cloud(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n",
    "#     point_cloud = []\n",
    "#     print(\"Feature Matches No. of Pairs: \",len(images.feature_matches))\n",
    "#     for i, feature_match in enumerate(images.feature_matches):\n",
    "#         image_one = feature_match.image_one\n",
    "#         image_two = feature_match.image_two\n",
    "\n",
    "#         # Extract matched keypoints\n",
    "#         keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "#         keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "\n",
    "#         # Estimate the essential matrix\n",
    "#         E, mask = OpenCV.findEssentialMat(keypoints_one, keypoints_two, K_matrix, method=OpenCV.RANSAC, prob=0.999, threshold=1.0)\n",
    "#         _, R, t, _ = OpenCV.recoverPose(E, keypoints_one, keypoints_two, K_matrix)\n",
    "\n",
    "#         print(f\"R of Pair {i}: \", R)\n",
    "#         print(f\"t of Pair {i}: \", t)\n",
    "#         print(f\"Essential Matrix of Pair {i}: \", E)\n",
    "#         print(\"Keypoint_1 Shape: \", keypoints_one.shape)\n",
    "#         print(\"Keypoint_2 Shape: \", keypoints_two.shape)\n",
    "#         print(\"\\n\")\n",
    "#         # Create projection matrices\n",
    "#         P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1)))) #P1 Shape: (3, 4)\n",
    "#         P2 = K_matrix @ np.hstack((R, t)) #P2 Shape: (3, 4)\n",
    "\n",
    "#         # Triangulate points\n",
    "#         points_4D = OpenCV.triangulatePoints(P1, P2, keypoints_one.T, keypoints_two.T)\n",
    "#         points_3D = (points_4D / points_4D[3])[:3]\n",
    "\n",
    "#         point_cloud.append(points_3D)\n",
    "\n",
    "#     # Merge all point clouds into one\n",
    "#     point_cloud = np.hstack(point_cloud).T\n",
    "\n",
    "#     return point_cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def generate_point_cloud(images: Images, K_matrix: np.ndarray, **kwargs) -> np.ndarray:\n",
    "#     point_cloud = []\n",
    "#     print(\"Feature Matches No. of Pairs: \", len(images.feature_matches))\n",
    "\n",
    "#     # Initialize the first camera pose as identity\n",
    "#     R_prev, t_prev = np.eye(3), np.zeros((3, 1))\n",
    "\n",
    "#     for i, feature_match in enumerate(images.feature_matches):\n",
    "#         image_one = feature_match.image_one\n",
    "#         image_two = feature_match.image_two\n",
    "\n",
    "#         # Extract matched keypoints\n",
    "#         keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "#         keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "\n",
    "#         # Estimate the essential matrix\n",
    "#         E, mask = cv2.findEssentialMat(keypoints_one, keypoints_two, K_matrix, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "\n",
    "#         # If this is not the first pair, use PnP method to recover the pose\n",
    "#         if i > 0 and point_cloud_prev.shape[0] >= 4:\n",
    "#             _, R, t, inliers = cv2.solvePnP(point_cloud_prev, keypoints_two[:point_cloud_prev.shape[0]], K_matrix, None)\n",
    "#             R, _ = cv2.Rodrigues(R)\n",
    "#             t = -R @ t_prev + t\n",
    "#             R = R @ R_prev\n",
    "#         else:\n",
    "#             _, R, t, _ = cv2.recoverPose(E, keypoints_one, keypoints_two, K_matrix)\n",
    "\n",
    "#         R_prev, t_prev = R, t\n",
    "\n",
    "#         print(f\"R of Pair {i}: \", R)\n",
    "#         print(f\"t of Pair {i}: \", t)\n",
    "#         print(f\"Essential Matrix of Pair {i}: \", E)\n",
    "#         print(\"Keypoint_1 Shape: \", keypoints_one.shape)\n",
    "#         print(\"Keypoint_2 Shape: \", keypoints_two.shape)\n",
    "#         print(\"\\n\")\n",
    "\n",
    "#         # Create projection matrices\n",
    "#         P1 = K_matrix @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "#         P2 = K_matrix @ np.hstack((R, t))\n",
    "\n",
    "#         # Triangulate points\n",
    "#         points_4D = cv2.triangulatePoints(P1, P2, keypoints_one.T, keypoints_two.T)\n",
    "#         points_3D = (points_4D / points_4D[3])[:3]\n",
    "\n",
    "#         # Store the previous pair's triangulated points\n",
    "#         point_cloud_prev = points_3D.T\n",
    "\n",
    "#         point_cloud.append(points_3D)\n",
    "\n",
    "#     # Merge all point clouds into one\n",
    "#     point_cloud = np.hstack(point_cloud).T\n",
    "\n",
    "#     return point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_point_cloud(images: Images, K_matrix: np.ndarray) -> np.ndarray:\n",
    "#     point_cloud = []\n",
    "#     global_poses = []\n",
    "\n",
    "#     # Choose the first image as the reference frame and set its global pose\n",
    "#     reference_pose = np.eye(4)\n",
    "#     global_poses.append(reference_pose)\n",
    "\n",
    "#     # Iterate through feature matches and compute global poses for each keyframe\n",
    "#     for feature_match in images.feature_matches:\n",
    "#         image_one = feature_match.image_one\n",
    "#         image_two = feature_match.image_two\n",
    "\n",
    "#         # Extract matched keypoints\n",
    "#         keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "#         keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "\n",
    "#         # Estimate the essential matrix\n",
    "#         E, _ = cv2.findEssentialMat(keypoints_one, keypoints_two, K_matrix, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "#         _, R, t, _ = cv2.recoverPose(E, keypoints_one, keypoints_two, K_matrix)\n",
    "\n",
    "#         # # Bundle adjustment\n",
    "#         # R_opt, t_opt = bundle_adjustment(K_matrix, keypoints_one, keypoints_two, R, t)\n",
    "\n",
    "#         # Compute the global pose of image_two using the relative pose (R_opt, t_opt) and the global pose of image_one\n",
    "#         relative_pose = np.eye(4)\n",
    "#         relative_pose[:3, :3] = R\n",
    "#         relative_pose[:3, 3] = t.flatten()\n",
    "#         global_pose = global_poses[-1] @ relative_pose\n",
    "#         global_poses.append(global_pose)\n",
    "\n",
    "#     # Triangulate points using the globally aligned projection matrices and merge the point clouds\n",
    "#     for i, feature_match in enumerate(images.feature_matches):\n",
    "#         image_one = feature_match.image_one\n",
    "#         image_two = feature_match.image_two\n",
    "\n",
    "#         # Extract matched keypoints\n",
    "#         keypoints_one = np.array([image_one.keypoints[m.queryIdx].pt for m in feature_match.matches])\n",
    "#         keypoints_two = np.array([image_two.keypoints[m.trainIdx].pt for m in feature_match.matches])\n",
    "\n",
    "#         # Create projection matrices using the global poses\n",
    "#         P1 = K_matrix @ global_poses[i][:3]\n",
    "#         P2 = K_matrix @ global_poses[i + 1][:3]\n",
    "\n",
    "#         # Triangulate points\n",
    "#         points_4D = cv2.triangulatePoints(P1, P2, keypoints_one.T, keypoints_two.T)\n",
    "#         points_3D = (points_4D / points_4D[3])[:3]\n",
    "\n",
    "#         point_cloud.append(points_3D)\n",
    "\n",
    "#     # Merge all point clouds into one\n",
    "#     point_cloud = np.hstack(point_cloud).T\n",
    "\n",
    "#     return point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_images_bak(images_file_path: str, images: Images) -> None:\n",
    "    \"\"\" Dump images to a file \"\"\"\n",
    "    with open(images_file_path, \"wb\") as file:\n",
    "        pickle.dump(images, file)\n",
    "\n",
    "def load_images_bak(images_file_path: str) -> Images:\n",
    "    \"\"\" Load images from a file \"\"\"\n",
    "    with open(images_file_path, \"rb\") as file:\n",
    "        images = pickle.load(file)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images: Images = load_images_bak(f\"data/cottage/bak/images-matched.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('1', [Image(1), Image(30)]), ('2', [Image(10), Image(11), Image(12)]), ('3', [Image(13), Image(14), Image(15)]), ('4', [Image(16), Image(17), Image(18), Image(19)]), ('5', [Image(2), Image(3)]), ('6', [Image(20), Image(21)]), ('7', [Image(22), Image(23), Image(24)]), ('8', [Image(25), Image(26)]), ('9', [Image(27), Image(28), Image(29)]), ('10', [Image(4), Image(5)]), ('11', [Image(6), Image(7), Image(8), Image(9)])])\n"
     ]
    }
   ],
   "source": [
    "print(images.similar_images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.similar_images = {\n",
    "    \"0\": [images[1], images[2]]\n",
    "    # \"1\": [images[11], images[12], images[13], images[14], images[15], images[16], images[17], images[18], images[19], images[20], images[21], \n",
    "    #     images[22], images[23], images[24], images[25], images[26], images[27]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('0', [Image(1), Image(2)])])\n"
     ]
    }
   ],
   "source": [
    "print(images.similar_images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('0', [Image(1), Image(2)])])\n"
     ]
    }
   ],
   "source": [
    "data_feature_matching(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureMatches(Image(1), Image(2) ---> 31908)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.feature_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #STICK 1\n",
    "# images.feature_matches[0].matches = images.feature_matches[0].matches[:3200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head 1\n",
    "images.feature_matches[0].matches = images.feature_matches[0].matches[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #STICK 2\n",
    "# images.feature_matches[1].matches = images.feature_matches[1].matches[:2700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head 2\n",
    "images.feature_matches[1].matches = images.feature_matches[1].matches[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #STICK 3\n",
    "# images.feature_matches[2].matches = images.feature_matches[2].matches[:1600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head 3\n",
    "images.feature_matches[2].matches = images.feature_matches[2].matches[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.feature_matches[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.feature_matches[0].draw_matches(\"data/cottage/output/triangulate-cluster/1_2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m images\u001b[39m.\u001b[39;49mfeature_matches[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mdraw_matches(\u001b[39m\"\u001b[39m\u001b[39mdata/cottage/output/triangulate-cluster/1_3.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "images.feature_matches[1].draw_matches(\"data/cottage/output/triangulate-cluster/1_3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.feature_matches[2].draw_matches(\"data/cottage/output/triangulate-cluster/1_4.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove images.feature_matches[3]\n",
    "# images.feature_matches.pop(5)\n",
    "# images.feature_matches.pop(4)\n",
    "images.feature_matches.pop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureMatches(Image(1), Image(2) ---> 31908)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.feature_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/cottage/bak/K_matrix.pickle\", 'rb') as f:\n",
    "        K_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4e+01 0.0e+00 3.0e+03]\n",
      " [0.0e+00 2.4e+01 2.0e+03]\n",
      " [0.0e+00 0.0e+00 1.0e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(K_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_points_cloud: np.ndarray = np.array([])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering from: 1 -> 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matches No. of Pairs:  1\n",
      "R of Pair 0:  [[ 9.95786243e-01 -9.16996841e-02  9.62655968e-04]\n",
      " [ 9.16997597e-02  9.95786700e-01 -3.45798849e-05]\n",
      " [-9.55429046e-04  1.22709495e-04  9.99999536e-01]]\n",
      "t of Pair 0:  [[-0.95070559]\n",
      " [ 0.31007994]\n",
      " [-0.00305272]]\n",
      "Essential Matrix of Pair 0:  [[ 1.15442508e-05 -2.17640677e-03 -2.19259453e-01]\n",
      " [ 2.79178807e-03 -2.80434107e-04 -6.72247977e-01]\n",
      " [ 2.79980919e-01  6.49311936e-01  1.87825252e-04]]\n",
      "Keypoint_1 Shape:  (31908, 2)\n",
      "Keypoint_2 Shape:  (31908, 2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_1_1_points_cloud: np.ndarray = generate_point_cloud(images, K_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "hdbscan_model = hdbscan.HDBSCAN().fit(one_1_1_points_cloud)\n",
    "labels = hdbscan_model.labels_\n",
    "core_indices = np.where(labels != -1)[0]\n",
    "core_points_1_1_HDBSCAN = one_1_1_points_cloud[core_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# dbscan = DBSCAN(eps=0.5, min_samples=10).fit(one_11_points_cloud)\n",
    "# labels = dbscan.labels_\n",
    "# core_indices = np.where(labels != -1)[0]\n",
    "# core_points_1_11_DBSCAN = one_11_points_cloud[core_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_points_cloud: np.ndarray = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28775, 3)\n"
     ]
    }
   ],
   "source": [
    "print(core_points_1_1_HDBSCAN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(core_points_1_1_HDBSCAN[:,:3])\n",
    "o3d.io.write_point_cloud(f\"data/cottage/output/triangulate-cluster/point_cloud_1_1_cDefault.ply\", pcd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering from: 11 -> 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images.similar_images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.similar_images = {\n",
    "#     # \"0\": [images[1], images[2], images[3], images[4], images[5], images[6], images[7], images[8], images[9], images[10], images[11]]\n",
    "#     \"1\": [images[11], images[12], images[13], images[14], images[15], images[16], images[17], images[18], images[19], images[20], images[21], \n",
    "#         images[22], images[23], images[24], images[25], images[26], images[27]]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images.similar_images.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.feature_matches = []\n",
    "# data_feature_matching(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eleven_27_points_cloud: np.ndarray = generate_point_cloud(images, K_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hdbscan\n",
    "# hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10).fit(eleven_27_points_cloud)\n",
    "# labels = hdbscan_model.labels_\n",
    "# core_indices = np.where(labels != -1)[0]\n",
    "# core_points_11_27 = eleven_27_points_cloud[core_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open3d as o3d\n",
    "# pcd = o3d.geometry.PointCloud()\n",
    "# pcd.points = o3d.utility.Vector3dVector(core_points_11_27[:,:3])\n",
    "# o3d.io.write_point_cloud(f\"data/hammer/output/triangulate-cluster/point_cloud_11_27_cluster.ply\", pcd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_points_cloud = np.concatenate((core_points_1_11, core_points_11_27), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open3d as o3d\n",
    "# pcd = o3d.geometry.PointCloud()\n",
    "# pcd.points = o3d.utility.Vector3dVector(global_points_cloud[:,:3])\n",
    "# o3d.io.write_point_cloud(f\"data/hammer/output/triangulate-cluster/point_cloud_global_1_2.ply\", pcd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Points Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "file_path = f\"data/cottage/output/triangulate-cluster/point_cloud_1_1_cDefault.ply\"\n",
    "point_cloud = o3d.io.read_point_cloud(file_path)\n",
    "o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open3d as o3d\n",
    "# file_path = f\"data/hammer/output/triangulate-cluster/point_cloud_11_27_cluster.ply\"\n",
    "# point_cloud = o3d.io.read_point_cloud(file_path)\n",
    "# o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open3d as o3d\n",
    "# file_path = f\"data/hammer/output/triangulate-cluster/point_cloud_global_1_27.ply\"\n",
    "# point_cloud = o3d.io.read_point_cloud(file_path)\n",
    "# o3d.visualization.draw_geometries([point_cloud])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
